{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: According to the main body of the paper \"Bayesian low-rank adaptation for large language models\", which method is statistical significant on BoolQ with ECE metrics? In that method, how is the hyperparameter k selected?\n[Answer Format]: Your answer should be a Python list of 2 strings, the first is the full name of the method, and the second is the formula in LaTeX format.\n[Anchor PDF]: '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to retrieve the relevant sections or chunks from the paper with the specified paper_id. I will use the `ClassicRetrieve` action to search for the method that is statistically significant on BoolQ with ECE metrics.\n[Action]:\nClassicRetrieve(query='statistically significant on BoolQ with ECE metrics', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5214,\"pdf_id\":\"a6f99213-62f5-5082-a90f-6d0bbf56038b\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8325feea-7f20-5318-aed6-d2b13b6f95d9\",\"text\":\"measuring expected calibration error. We hypothesize that this is because estimating ECE requires\\nbinning and then averaging calibration error across bins. This process tends to yield greater vari-\\nability when the number of labeled points per bin is small. In contrast, metrics like AUROC and\\nAUPRC do not bin predictions and thus are more competitive with labeled data alone.\\nComparison to baselines\\nDawid-Skene, as expected, performs poorly across metrics and out-\"}\n{\"score\":0.5164,\"pdf_id\":\"39ca84cd-c7df-595a-b18b-cdcbd5458785\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1d1f1575-83a5-5a4b-a8a4-6c17b0549508\",\"text\":\"should have high accuracy and confidence scores. To calculate ECE on a test set, all test samples\\nare grouped in k = 10 equal bins according to uncertainty scores. ECE was calculated as the\\nabsolute sum of differences between expected model confidence and accuracy for each bin. A small\\nECE indicates better performance as the model accurately quantifies uncertainties in its prediction.\\nExperiments show that ECE values drastically reduce with the proposed UQ implementation while\"}\n{\"score\":0.5093,\"pdf_id\":\"a6f99213-62f5-5082-a90f-6d0bbf56038b\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b4d6df71-fb90-5ddd-a04a-6dd9757430d4\",\"text\":\"We measure the mean absolute difference between our estimated ECE\\nwith limited labeled data and the \\u201ctrue\\u201d ECE if every point were labeled.\\nSmaller values in\\n|True ECE \\u2212Estimated ECE| indicate better estimates of ECE.\\n4\\nRESULTS\\nFigure 2 plots the accuracy of performance estimates using different methods, across four outcomes;\\nlower is better in all graphs.\\nPerformance across metrics\\nWe observe the greatest benefits relative to labeled data alone when\"}\n{\"score\":0.5069,\"pdf_id\":\"930a022c-3c46-55f9-99f3-0aa56bd2be09\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4755df21-cada-5939-88a0-206c49efa9bd\",\"text\":\"Experimental setup: We examine two subsets of ECE measurement\\nfrom our TSS dataset: the \\ufb01rst subset consists of architectures with\\ngood prediction accuracy in each dataset (ranked top 1000 by accu-\\nracy), while the second subset includes all architectures available\\nin the dataset. The aim of our investigation is to probe the model\\ncalibration performance correlation between datasets. To achieve\\nthis, we resort to the Kendall ranking correlation coef\\ufb01cient(Kendall,\"}\n{\"score\":0.5046,\"pdf_id\":\"6ed88cc5-6ed3-535f-b3e4-2957fed901a8\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"ac838823-c24c-5baf-8033-c248d298bdf7\",\"text\":\"query types can be found in Appendix A.\\nComparison with Baselines\\nFirst, we compare\\nBiDAG with GQE, Q2B, BetaE, Q2P, and MLPMix\\non the EPFO queries (containing only \\u2227, \\u2203, and \\u2228).\\nThe results are reported in Table 1. More details\\ncan be found in Appendix B.\\nFrom the table, we can find that: (1) BiDAG\\ndemonstrates an average relative improvement in\\nMean Reciprocal Rank (MRR) of 3.2%, 3.9%, and\\n5.4% over previous QE based models on the FB15k,\\nFB15k-237, and NELL995 datasets, respectively.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results do not contain the specific information about the method that is statistically significant on BoolQ with ECE metrics. I will perform a more targeted search by focusing on the specific paper and using the `RetrieveFromDatabase` action to get the relevant section or chunk content.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT chunks.text_content FROM chunks JOIN metadata ON chunks.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7' AND chunks.text_content LIKE '%statistically significant on BoolQ with ECE metrics%';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the exact phrase \"statistically significant on BoolQ with ECE metrics\" might not be present in the paper. I will try a more general search for the method that is statistically significant on BoolQ with ECE metrics.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT chunks.text_content FROM chunks JOIN metadata ON chunks.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7' AND chunks.text_content LIKE '%statistically significant%BoolQ%ECE%';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query still did not return any results. I will try a broader search for the method that is statistically significant on BoolQ with ECE metrics and then use the `ClassicRetrieve` action to search for more context.\n[Action]:\nClassicRetrieve(query='statistically significant on BoolQ with ECE metrics in \"Bayesian low-rank adaptation for large language models\"', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6733,\"pdf_id\":\"147215e5-9448-5955-9d90-d99abc21d04f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"96e168a8-f7fb-5830-ae2e-7c1277da14ba\",\"text\":\"Low-rank adaptation of large language models, 2021.\\n12\"}\n{\"score\":0.6681,\"pdf_id\":\"d32f34d4-83ad-5521-8727-edaed0997024\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9188dba5-9b72-5e12-86d1-18f300486c56\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. CoRR, abs\\/2106.09685, 2021. URL\\nhttps:\\/\\/arxiv.org\\/abs\\/2106.09685.\\n11\"}\n{\"score\":0.6617,\"pdf_id\":\"9f78d2e9-d774-5e75-a31f-c644ca9d7845\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"0df4d9c9-c07f-50f6-9b4e-7631100aa48f\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs\\/2106.09685, 2021.\\n10\"}\n{\"score\":0.6582,\"pdf_id\":\"02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"42a84dc8-5e69-5aa4-8d74-ba7fac961cf7\",\"text\":\"Bayesian low-rank adaptation for large language\\nmodels\\nAdam X. Yang1\\nMaxime Robeyns1\\nXi Wang2\\nLaurence Aitchison1\\n1University of Bristol\\n2University of Massachusetts, Amherst\\n{adam.yang,maxime.robeyns.2018,laurence.aitchison}@bristol.ac.uk\\nxwang3@cs.umass.edu\\nAbstract\\nLow-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-\\ntuning of large language models (LLMs). However, fine-tuned LLMs often become\\noverconfident especially when fine-tuned on small datasets. Bayesian methods,\"}\n{\"score\":0.636,\"pdf_id\":\"1b982f49-9a7b-5692-9ad1-2fad56a419e1\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"73cdad52-cc14-5613-96c8-e9f2df94c129\",\"text\":\"Published as a conference paper at ICLR 2024\\nQA-LORA:\\nQUANTIZATION-AWARE\\nLOW-RANK\\nADAPTATION OF LARGE LANGUAGE MODELS\\nYuhui Xu\\nLingxi Xie \\u0000Xiaotao Gu\\nXin Chen\\nHeng Chang\\nHengheng Zhang\\nZhensu Chen\\nXiaopeng Zhang\\nQi Tian\\nHuawei Inc.\\n(\\u0000: corresponding author)\\n{xyh6666,198808xc,guxt1994,chenxin061,changh.heng}@gmail.com\\n{imhmhm,chenzhengsu1,zxphistory}@gmail.com,\\ntian.qi1@huawei.com\\nABSTRACT\\nRecently years have witnessed a rapid development of large language models\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not contain the specific information about the method that is statistically significant on BoolQ with ECE metrics. I will perform a more targeted search by focusing on the specific sections or chunks that discuss the results and experiments.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT chunks.text_content FROM chunks JOIN metadata ON chunks.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7' AND chunks.text_content LIKE '%results%BoolQ%ECE%';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query still did not return any results. I will try a broader search for the method that is statistically significant on BoolQ with ECE metrics and then use the `ClassicRetrieve` action to search for more context, focusing on the results and experiments sections.\n[Action]:\nClassicRetrieve(query='results BoolQ ECE in \"Bayesian low-rank adaptation for large language models\"', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7603,\"pdf_id\":\"147215e5-9448-5955-9d90-d99abc21d04f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"96e168a8-f7fb-5830-ae2e-7c1277da14ba\",\"text\":\"Low-rank adaptation of large language models, 2021.\\n12\"}\n{\"score\":0.7425,\"pdf_id\":\"02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"42a84dc8-5e69-5aa4-8d74-ba7fac961cf7\",\"text\":\"Bayesian low-rank adaptation for large language\\nmodels\\nAdam X. Yang1\\nMaxime Robeyns1\\nXi Wang2\\nLaurence Aitchison1\\n1University of Bristol\\n2University of Massachusetts, Amherst\\n{adam.yang,maxime.robeyns.2018,laurence.aitchison}@bristol.ac.uk\\nxwang3@cs.umass.edu\\nAbstract\\nLow-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-\\ntuning of large language models (LLMs). However, fine-tuned LLMs often become\\noverconfident especially when fine-tuned on small datasets. Bayesian methods,\"}\n{\"score\":0.7419,\"pdf_id\":\"9f78d2e9-d774-5e75-a31f-c644ca9d7845\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"0df4d9c9-c07f-50f6-9b4e-7631100aa48f\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs\\/2106.09685, 2021.\\n10\"}\n{\"score\":0.7388,\"pdf_id\":\"d32f34d4-83ad-5521-8727-edaed0997024\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9188dba5-9b72-5e12-86d1-18f300486c56\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. CoRR, abs\\/2106.09685, 2021. URL\\nhttps:\\/\\/arxiv.org\\/abs\\/2106.09685.\\n11\"}\n{\"score\":0.7128,\"pdf_id\":\"505ea085-242f-5564-af3c-2be89f26ebbe\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8ad5380b-354e-5e6f-b416-161702efe224\",\"text\":\"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021.\\n14\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not contain the specific information about the method that is statistically significant on BoolQ with ECE metrics. I will try a more targeted search by focusing on the specific sections or chunks that discuss the results and experiments, and use a more precise query.\n[Action]:\nClassicRetrieve(query='statistically significant method BoolQ ECE \"Bayesian low-rank adaptation for large language models\"', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7759,\"pdf_id\":\"147215e5-9448-5955-9d90-d99abc21d04f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"96e168a8-f7fb-5830-ae2e-7c1277da14ba\",\"text\":\"Low-rank adaptation of large language models, 2021.\\n12\"}\n{\"score\":0.7704,\"pdf_id\":\"02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"42a84dc8-5e69-5aa4-8d74-ba7fac961cf7\",\"text\":\"Bayesian low-rank adaptation for large language\\nmodels\\nAdam X. Yang1\\nMaxime Robeyns1\\nXi Wang2\\nLaurence Aitchison1\\n1University of Bristol\\n2University of Massachusetts, Amherst\\n{adam.yang,maxime.robeyns.2018,laurence.aitchison}@bristol.ac.uk\\nxwang3@cs.umass.edu\\nAbstract\\nLow-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-\\ntuning of large language models (LLMs). However, fine-tuned LLMs often become\\noverconfident especially when fine-tuned on small datasets. Bayesian methods,\"}\n{\"score\":0.7601,\"pdf_id\":\"9f78d2e9-d774-5e75-a31f-c644ca9d7845\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"0df4d9c9-c07f-50f6-9b4e-7631100aa48f\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs\\/2106.09685, 2021.\\n10\"}\n{\"score\":0.7542,\"pdf_id\":\"d32f34d4-83ad-5521-8727-edaed0997024\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9188dba5-9b72-5e12-86d1-18f300486c56\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. CoRR, abs\\/2106.09685, 2021. URL\\nhttps:\\/\\/arxiv.org\\/abs\\/2106.09685.\\n11\"}\n{\"score\":0.7305,\"pdf_id\":\"505ea085-242f-5564-af3c-2be89f26ebbe\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8ad5380b-354e-5e6f-b416-161702efe224\",\"text\":\"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021.\\n14\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not contain the specific information about the method that is statistically significant on BoolQ with ECE metrics. I will try a more targeted SQL query to search for the specific section that discusses the results and experiments, and then use the `ClassicRetrieve` action to search for more context.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7' AND sections.section_title LIKE '%Results%' OR sections.section_title LIKE '%Experiments%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"More Details and Results on Experiments\\nIn this appendix we fill the missing details in the experiment section. Our experiments are\\nimplemented with Intel Core i9 CPU of 32GB memory, no GPU is required and the execution\\ntime is no longer than 30 seconds. In the rest of this section, we first introduce the generation\\nprocess of synthetic datasets in Appendix E.1; Next we define the evaluation metrics and\\nperturbation approaches in Appendix E.2; More results on dissimilarities regarding the\\nscaled additive error, negative distances and negative eigenvalues selected, are demonstrated\\nin Appendix E.3.\\nWith respect to the dimensionality paradox, we provide illustrations\\nsimilar to Figure 2 on other datasets in Appendix E.4.\\nE.1\\nSynthetic Datasets Generation\\nThe random-simplex dataset is generated with the idea of creating a simplex with one\\nadded dimension that generates all of the variance in the the distances such that the added\\ndimension corresponds with a negative eigenvalue. This way, we should have a dimension\\nwith a large negative eigenvalue that is impactful on the stress of the embedding.\\nSpecifically, this dataset was generated with points x1, ...x1000, each point having 1000\\ndimensions. For each point xi, the first 100 coordinates were chosen uniformly randomly\\nbetween 0 and .01. The next 899 coordinates were chosen uniformly randomly between 0\\nand\\np\\n0.5\\/899. The last coordinate is i \\u00d7 0.3\\/1000. Then, the distance between xi and xj\\nwas found in the following way:\\nd(xi, xj) =\\nv\\nu\\nu\\nt\\n100\\nX\\nk=1\\n(xi(k) \\u2212xj(k))2 \\u2212\\n1000\\nX\\nk=101\\n(xi(k) \\u2212xj(k))2\\nwhere xi(k) is the kth coordinate of xi. The first 999 coordinates essentially form a simplex,\\nbecause selecting a large number of coordinates randomly from the same distribution causes\\nall of the distances to converge to a normal distribution with low variance. Thus, all of\\nthe distances between points using just the first 999 coordinates are very similar. Then,\\nthe last coordinate has \\u201clarge\\u201d discrepancies between all of the points, causing most of the\\ndifferences in the distances.\\nThe Euclidean-ball metric was inspired by [49]. The set of distances between objects in space\\ndoes not follow triangle inequality, so it becomes a non-euclidean dissimilarity measure. The\\nparticular dataset we use generates the distances by first randomly selecting 1000 points\\n21\\nuniformly in a 10 dimensional hypercube of length 100. Each of these points is now the\\ncenter of a ball. Then, for each ball, with probability 0.9, we select a radius uniformly\\nrandomly between 0 and 5. With probability 0.1, we let the radius be 0.8 times the distance\\nbetween the center of this ball to the closest other ball. This closest other ball is based on\\nthe radius of the other ball if it has already been decided. The final distance matrix is just\\nfilled by the distances between the 1000 balls. In this way, we attempt to create some balls\\nwith large radius, so that the triangle inequality is heavily violated, therefore causing the\\ndataset to deviate more from the Euclidean setting.\\nE.2\\nEvaluation and Perturbation Metrics\\nIn addition to STRESS, our evaluation metric include scaled additive error and average\\ngeometric distortion. Scaled additive error by definition is allowing scaling of the distance\\nmatrix of our embedding before calculating stress. To do this, we first ran the embedding\\nmethod (e.g., cMDS, Neuc-MDS, or other methods) on the given dissimilarity squared ma-\\ntrix D to get the output dissimilarity squared matrix \\u02dc\\nD. Then, we would flatten both D\\nand \\u02dc\\nD to vectors, and project D onto the line through \\u02dc\\nD to get E. Then we calculate\\n\\u2225D \\u2212E\\u2225to get scaled additive error. Clearly, this is equivalent to allowing scaling of \\u02dc\\nD\\nto find the minimum stress. For average geometric distortion, we again input D to get \\u02dc\\nD,\\nbut this time, we evaluate the distortion of each dissimilarity by dividing entries of\\n\\u221a\\nD by\\nentries of\\np\\n\\u02dc\\nD. Since distortion is not well defined when dissimilarities become complex,\\nwe skip those. Then, with the remaining valid distortions, we scale the distortions so that\\nthere\\u2019s an equal number of them greater than 1 and less than 1, then for those that remain\\nless than 1, we take their reciprocal. Then, we finally take the geometric average of all of\\nthese distortions. In this way, we basically again allowed scaling of the dissimilarities to find\\nthe minimum possible average distortion.\\nNext, we had to decide on metrics to use for the image datasets. We took the following\\n3 metrics from [38]. For the first metric, the distance between two images is first finding\\nthe Euclidean distance between their coordinate representations, and then adding Gaussian\\nnoise to the distances.\\nBecause the noise is added directly onto the distances, the end\\nresult is highly likely to be non-Euclidean. For the second metric, we build a k nearest\\nneighbor graph of the images based on their Euclidean distances. Since the end result is a\\ngraph structure, the shortest path distances are also highly likely to be non-Euclidean. For\\nthe third metric, we randomly removed entries from the coordinate representation of the\\nimages. Then, the distance between two images is decided by taking the Euclidean distance\\nbetween coordinates that both images still included. Here, the Euclidean property breaks\\ndown again because triangle inequality would no longer have to hold. In Table 2, we choose\\nthe k-NN metric with k = 2. In the next Appendix section, we show results for higher k and\\nother metrics applied to the image datasets. As for genomics datasets, we use the entropic\\naffinities commonly used in t-SNE methods. The main idea is to apply an adaptive kernel,\\nwhose bandwidth depends on a parameter termed as perplexity, to pairwise data entries.\\nE.3\\nMore Results on Dissimilarity Error\\nIn addition to STRESS and average distortion already reported in Table 2, we further show\\nthe third metric: scaled additive error as defined in Appendix E.2. Note that our algorithms\\nmay produce negative distances, we also report number of negative distances and number\\nof negative eigenvalues selected. All original results (without scaling) on 10 datasets are\\ndemonstrated in Table 6. Recall we mentioned the results in Table 2 contains some scaling\\non the synthetic data and images, we also provide the original values in Table 7.\\nOn scaled additive error, our proposed methods still consistently outperform cMDS and\\nLower-MDS considerably.\\nThus, we have shown that on all metrics our methods yield\\nmore favorable results. It is interesting to observe that Neuc-MDS+ produces much fewer\\nnegative distances than Neuc-MDS, though the number of negative eigenvalues selected are\\nquite close. Further, two methods have similar performance on STRESS and other metrics.\\nTherefore in practice, if there is a considerable concern on negative distances, Neuc-MDS+\\nshould become a better choice.\\n22\\nTable 6: Original Evaluation Results on All Datasets for Lower-MDS (L-MDS), Neuc-MDS\\n(N-MDS) and Neuc-MDS+ (N-MDS+). Metrics include scaled additive error, number of\\nnegative distances and number of negative eigenvalues selected.\\nDataset\\nScaled Additive Error\\n# Neg Distances\\n# Neg \\u03bb Selected\\ncMDS\\nL-MDS\\nN-MDS\\nN-MDS+\\nN-MDS\\nN-MDS+\\nN-MDS\\nN-MDS+\\nRandom-simplex\\n17758\\n17760\\n3415\\n1392\\n0\\n0\\n8\\n1\\nEuclidean-ball\\n14909\\n13093\\n3132\\n3631\\n798\\n523\\n90\\n87\\nBrain (50161)\\n0.131\\n0.132\\n0.063\\n0.062\\n5539\\n1081\\n8\\n9\\nBreast (45827)\\n0.039\\n0.039\\n0.016\\n0.015\\n9024\\n136\\n8\\n9\\nColorectal (44076)\\n0.027\\n0.027\\n0.012\\n0.014\\n12041\\n1940\\n6\\n8\\nLeukemia (28497)\\n0.031\\n0.031\\n0.021\\n0.023\\n32705\\n2102\\n8\\n10\\nRenal (53757)\\n0.018\\n0.018\\n0.013\\n0.014\\n6650\\n713\\n7\\n9\\nMNIST\\n7592\\n6126\\n3148\\n3.135\\n1006\\n68\\n42\\n43\\nFashion-MNIST\\n6153\\n4421\\n2.472\\n2.470\\n526\\n8\\n41\\n41\\nCIFAR10\\n3880\\n3564\\n2870\\n2.842\\n2968\\n201\\n43\\n44\\nTable 7: Original Evaluation Results on All Datasets. Metrics include STRESS and average\\ngeometric distortion\\nDataset\\nSTRESS\\nAverage Geometric Distortion\\ncMDS\\nL-MDS\\nN-MDS\\nN-MDS+\\ncMDS\\nL-MDS\\nN-MDS\\nN-MDS+\\nRandom-simplex\\n28.376\\n17.760\\n3.433\\n1.392\\n1.049\\n1.049\\n1.010\\n1.004\\nEuclidean-ball\\n19.229\\n13.154\\n3.346\\n3.676\\n1.046\\n1.041\\n1.013\\n1.017\\nBrain (50161)\\n0.538\\n0.170\\n0.068\\n0.067\\n8.160\\n42.705\\n5.809\\n6.941\\nBreast (45827)\\n0.168\\n0.065\\n0.017\\n0.017\\n6.988\\n31.081\\n6.205\\n6.295\\nColorectal (44076)\\n0.121\\n0.047\\n0.013\\n0.016\\n23.938\\n34.587\\n20.234\\n22.475\\nLeukemia (28497)\\n0.172\\n0.079\\n0.028\\n0.031\\n6.551\\n32.214\\n7.032\\n6.749\\nRenal (53757)\\n0.070\\n0.030\\n0.016\\n0.019\\n21.709\\n38.282\\n19.680\\n21.223\\nMNIST\\n25.516\\n6.156\\n3.152\\n3.144\\n1.119\\n1.104\\n1.064\\n1.063\\nFashion-MNIST\\n18.771\\n4.422\\n2.475\\n2.473\\n1.135\\n1.096\\n1.068\\n1.068\\nCIFAR10\\n16.309\\n3.572\\n2.930\\n2.916\\n1.129\\n1.109\\n1.121\\n1.118\\nFigure 3: Dimensionality Paradox on Eight Datasets.\\nUntil now all results reported on images are using kNN metric with k = 2 to produce\\nnon-Euclidean dissimilarities. There could be other perturbations for the same purpose, as\\nmentioned in Appendix E.2. We further test with k = 10 and the other two metrics: adding\\nnoise and random removal on the STRESS. We add noise sampled from Gaussian distri-\\nbution with variance as the maximum instance-wise difference scaled by 500; For random\\nremoval, we skip 50 images for each. The results are shown in Table 8.\\n23\\nTable 8: Evaluation Results on Image Datasets with other perturbation metrics.\\nDataset\\nSTRESS\\n# Neg distances\\n# Neg \\u03bb Selected\\ncMDS\\nL-MDS\\nN-MDS\\nN-MDS+\\nN-MDS\\nN-MDS+\\nN-MDS\\nN-MDS+\\nMNIST (k = 10)\\n10080\\n1979\\n1827\\n1830\\n554\\n63\\n42\\n41\\nFashion (k = 10)\\n10249\\n2112\\n1936\\n1915\\n403\\n84\\n38\\n40\\nCIFAR10 (k = 10)\\n9032\\n1881\\n2309\\n2329\\n3236\\n996\\n39\\n37\\nMNIST (noise)\\n1.497e9\\n1.499e9\\n1.497e9\\n1.140e9\\n0\\n0\\n0\\n7\\nFashion (noise)\\n4.815e9\\n4.130e9\\n3.382e9\\n3.327e9\\n0\\n0\\n2\\n14\\nCIFAR10 (noise)\\n2.303e10\\n2.116e9\\n2.303e9\\n1.848e9\\n0\\n0\\n0\\n9\\nMNIST (missing)\\n3.281e8\\n1.010e8\\n3.281e8\\n1.010e8\\n0\\n0\\n0\\n0\\nFashion (missing)\\n5.165e8\\n1.726e8\\n5.165e8\\n1.726e8\\n0\\n0\\n0\\n0\\nCIFAR10 (missing)\\n2.132e9\\n5.756e8\\n2.132e9\\n5.756e8\\n0\\n0\\n0\\n0\\nE.4\\nMore Results on Dimensionality Paradox\\nThe purpose of this section is to show the consistent performance of Neuc-MDS and Neuc-\\nMDS+ on all datasets, with respect to mitigating the dimensionality paradox issue. There-\\nfore, in addition to Figure 2, we provide the same plots for other datasets, and the pa-\\nrameters all follow the main setup. Figure 3 gives a clear illustration that Neuc-MDS and\\nNeuc-MDS+ always have lower STRESS than cMDS and Lower-MDS as dimension grows.\\n24\\nNeurIPS Paper Checklist\\n1. Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately\\nreflect the paper\\u2019s contributions and scope?\\nAnswer: [Yes]\\nJustification: We propose a generalization of classical multidimensional scaling to\\nhandle non-Euclidean non-metric data. We presented both a new algorithm, theo-\\nretical proofs on its performance, and experimental results.\\nGuidelines:\\n\\u2022 The answer NA means that the abstract and introduction do not include the\\nclaims made in the paper.\\n\\u2022 The abstract and\\/or introduction should clearly state the claims made, in-\\ncluding the contributions made in the paper and important assumptions and\\nlimitations. A No or NA answer to this question will not be perceived well by\\nthe reviewers.\\n\\u2022 The claims made should match theoretical and experimental results, and reflect\\nhow much the results can be expected to generalize to other settings.\\n\\u2022 It is fine to include aspirational goals as motivation as long as it is clear that\\nthese goals are not attained by the paper.\\n2. Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the\\nauthors?\\nAnswer: [Yes]\\nJustification: We discussed the limitation (and future work) of the proposed ap-\\nproach.\\nThese include: 1) developing downstream data analysis for using non-\\nEuclidean non-metrics data. 2) speeding up the algorithm for extremely large data\\n(e.g., by developing landmark based non-Euclidean MDS algorithms).\\nGuidelines:\\n\\u2022 The answer NA means that the paper has no limitation while the answer No\\nmeans that the paper has limitations, but those are not discussed in the paper.\\n\\u2022 The authors are encouraged to create a separate \\u201dLimitations\\u201d section in their\\npaper.\\n\\u2022 The paper should point out any strong assumptions and how robust the results\\nare to violations of these assumptions (e.g., independence assumptions, noise-\\nless settings, model well-specification, asymptotic approximations only holding\\nlocally). The authors should reflect on how these assumptions might be vio-\\nlated in practice and what the implications would be.\\n\\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach\\nwas only tested on a few datasets or with a few runs. In general, empirical\\nresults often depend on implicit assumptions, which should be articulated.\\n\\u2022 The authors should reflect on the factors that influence the performance of\\nthe approach. For example, a facial recognition algorithm may perform poorly\\nwhen image resolution is low or images are taken in low lighting. Or a speech-\\nto-text system might not be used reliably to provide closed captions for online\\nlectures because it fails to handle technical jargon.\\n\\u2022 The authors should discuss the computational efficiency of the proposed algo-\\nrithms and how they scale with dataset size.\\n\\u2022 If applicable, the authors should discuss possible limitations of their approach\\nto address problems of privacy and fairness.\\n\\u2022 While the authors might fear that complete honesty about limitations might\\nbe used by reviewers as grounds for rejection, a worse outcome might be that\\nreviewers discover limitations that aren\\u2019t acknowledged in the paper.\\nThe\\nauthors should use their best judgment and recognize that individual actions in\\n25\\nfavor of transparency play an important role in developing norms that preserve\\nthe integrity of the community. Reviewers will be specifically instructed to not\\npenalize honesty concerning limitations.\\n3. Theory Assumptions and Proofs\\nQuestion: For each theoretical result, does the paper provide the full set of assump-\\ntions and a complete (and correct) proof?\\nAnswer: [Yes]\\nJustification: Please refer to the theorem statements. We included all proofs in the\\nAppendix.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include theoretical results.\\n\\u2022 All the theorems, formulas, and proofs in the paper should be numbered and\\ncross-referenced.\\n\\u2022 All assumptions should be clearly stated or referenced in the statement of any\\ntheorems.\\n\\u2022 The proofs can either appear in the main paper or the supplemental material,\\nbut if they appear in the supplemental material, the authors are encouraged to\\nprovide a short proof sketch to provide intuition.\\n\\u2022 Inversely, any informal proof provided in the core of the paper should be com-\\nplemented by formal proofs provided in appendix or supplemental material.\\n\\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4. Experimental Result Reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce\\nthe main experimental results of the paper to the extent that it affects the main\\nclaims and\\/or conclusions of the paper (regardless of whether the code and data\\nare provided or not)?\\nAnswer: [Yes]\\nJustification: We provided all details of our experiments in the Appendix. We also\\nshared our code using an anonymous Github repository. See the footnote on Page\\n2.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 If the paper includes experiments, a No answer to this question will not be\\nperceived well by the reviewers: Making the paper reproducible is important,\\nregardless of whether the code and data are provided or not.\\n\\u2022 If the contribution is a dataset and\\/or model, the authors should describe the\\nsteps taken to make their results reproducible or verifiable.\\n\\u2022 Depending on the contribution, reproducibility can be accomplished in various\\nways. For example, if the contribution is a novel architecture, describing the\\narchitecture fully might suffice, or if the contribution is a specific model and\\nempirical evaluation, it may be necessary to either make it possible for others\\nto replicate the model with the same dataset, or provide acces ...\"}\n\n... # only display 1 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved section content is still not specific enough to find the method that is statistically significant on BoolQ with ECE metrics. I will perform a more targeted `ClassicRetrieve` action to search for the specific context.\n[Action]:\nClassicRetrieve(query='statistically significant method BoolQ ECE \"Bayesian low-rank adaptation for large language models\"', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7759,\"pdf_id\":\"147215e5-9448-5955-9d90-d99abc21d04f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"96e168a8-f7fb-5830-ae2e-7c1277da14ba\",\"text\":\"Low-rank adaptation of large language models, 2021.\\n12\"}\n{\"score\":0.7704,\"pdf_id\":\"02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"42a84dc8-5e69-5aa4-8d74-ba7fac961cf7\",\"text\":\"Bayesian low-rank adaptation for large language\\nmodels\\nAdam X. Yang1\\nMaxime Robeyns1\\nXi Wang2\\nLaurence Aitchison1\\n1University of Bristol\\n2University of Massachusetts, Amherst\\n{adam.yang,maxime.robeyns.2018,laurence.aitchison}@bristol.ac.uk\\nxwang3@cs.umass.edu\\nAbstract\\nLow-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-\\ntuning of large language models (LLMs). However, fine-tuned LLMs often become\\noverconfident especially when fine-tuned on small datasets. Bayesian methods,\"}\n{\"score\":0.7601,\"pdf_id\":\"9f78d2e9-d774-5e75-a31f-c644ca9d7845\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"0df4d9c9-c07f-50f6-9b4e-7631100aa48f\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs\\/2106.09685, 2021.\\n10\"}\n{\"score\":0.7542,\"pdf_id\":\"d32f34d4-83ad-5521-8727-edaed0997024\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9188dba5-9b72-5e12-86d1-18f300486c56\",\"text\":\"Chen. Lora: Low-rank adaptation of large language models. CoRR, abs\\/2106.09685, 2021. URL\\nhttps:\\/\\/arxiv.org\\/abs\\/2106.09685.\\n11\"}\n{\"score\":0.7305,\"pdf_id\":\"505ea085-242f-5564-af3c-2be89f26ebbe\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8ad5380b-354e-5e6f-b416-161702efe224\",\"text\":\"Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\\npreprint arXiv:2106.09685, 2021.\\n14\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results are still not providing the specific information needed. I will try a more targeted SQL query to search for the specific section that discusses the results on BoolQ and ECE metrics.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7' AND sections.section_title LIKE '%Results%' OR sections.section_title LIKE '%Experiments%' AND sections.section_content LIKE '%BoolQ%ECE%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Experiments\\nIn this section, we compare our BLoB with existing methods on real-world datasets. Sec. 4.1\\nintroduces the experimental settings, including baselines, fine-tuning, and evaluation protocols.\\nWe then evaluate BLoB\\u2019s generalization and uncertainty estimation abilities in both in-distribution\\n(Sec. 4.2) and out-of-distribution scenarios (Sec. 4.3).\\n4.1\\nSettings\\nFine-tuning and Evaluation. We implement BLoB in the PEFT library [63] and fine-tune the\\nLlaMA2-7B [92] model on common-sense reasoning tasks. Following Laplace-LoRA [116], we\\napply LoRA to the output layer as well as the queries and values of all the attention layers. For\\nhyperparameters, we strictly adhere to the default settings in the PEFT library and the original LoRA\\npaper [63, 41] to ensure maximal reproducibility. This includes the number of training steps, learning\\nrate, and LoRA rank r (see Appendix B.1 for details). For common-sense reasoning tasks, we select\\n7\\nthe next token logits corresponding to possible answers from each dataset and fine-tune the LLM to\\nmaximize the likelihood of the correct token. For evaluation, in addition to Accuracy (ACC), we use\\nExpected Calibration Error (ECE [31]) and Negative Log-Likelihood (NLL) to assess the models\\u2019\\nuncertainty estimation ability (see Appendix B.2 for details).\\nBaselines and Implementation Details. We compare BLoB with state-of-the-art uncertainty estima-\\ntion methods applied to the LoRA adapters of LLMs, including Monte-Carlo Dropout (MCD) [29],\\nBayes By Backprop (BBB) [11], Deep Ensemble (ENS) [51, 8, 103], and the latest Laplace-\\nLoRA (LAP) [116]. We also report the performance of two standard PEFT baseline methods\\nfor reference: Maximum Likelihood Estimation (MLE) [41] and Maximum A Posteriori (MAP).\\nFor MLE, we use the LoRA implementation. For MAP, we use a weight decay rate of 1e \\u22125. For\\nMCD, we use an ensemble of 10 LoRAs with a dropout rate of p = 0.1. For ENS, we independently\\nfine-tune 3 LoRAs and average their logits during evaluation. For BBB, we adopt the default settings\\nfrom the Bayesian-Torch library [48] and only Bayesianize the A matrix, similar to BLoB. We sample\\nN = 10 times for BBB during test. We re-implement LAP and apply it to the MAP checkpoints. We\\nkeep all BLoB-specific hyperparameters consistent across all datasets. Typically, we set the number\\nof samples K = 1 during training for all our BLoB experiments, which highlights BLoB\\u2019s sampling\\nefficiency. As shown in Table 1, we also report BLoB\\u2019s performance with different numbers of\\nsamples during Bayesian inference, where N = 0 indicates directly using the mean of the weight\\ndistribution for prediction.\\n4.2\\nResults on In-distribution Datasets\\nWe fine-tune Llama2-7B on six common-sense reasoning tasks: Winogrande-small (WG-S),\\nWinogrande-medium (WG-M) [82], ARC-Challenge (ARC-C) [18], ARC-Easy (ARC-E) [18], Open-\\nBookQA (OBQA) [65], and BoolQ [17]. For all baseline methods, using the same pre-trained LLM\\nbackbone, we maintain consistent hyperparameters across all datasets and do not use additional\\nvalidation sets to achieve higher performance (See Appendix B.3 for detailed settings).\\nTable 1 shows the performance of BLoB compared to the baselines, including ACC, ECE, and NLL,\\non the in-distribution test set with the pre-trained Llama2-7B model. The high ECE and NLL for\\nMLE indicate overconfidence in LLMs during conventional fine-tuning, except for BoolQ due to\\nits large dataset size. Simple but popular baselines like MAP, MCD, and ENS show mixed results\\nin terms of NLL and\\/or ECE, highlighting the challenge of uncertainty estimation during LLM\\nfine-tuning. LAP, the most competitive post-training baseline for uncertainty estimation, significantly\\nreduces NLL and ECE on some datasets but lacks consistent performance, as indicated by its failures\\non ARC-C and ARC-E. BBB mitigates the overconfidence issue in LLMs across almost all datasets,\\nshowcasing the advantage of jointly optimizing the mean and covariance of the variational weight\\ndistributions during fine-tuning. However, there remains considerable room for improvement.\\nBLoB consistently achieves better or comparable performance across all datasets. With the number\\nof samples during inference set to N = 10, the same as MCD, BLoB provides the best uncertainty\\nestimation performance, significantly reducing NLL and ECE, and greatly mitigating overconfidence\\nwhile maintaining comparable or better ACC than MLE. Even with half the number of samples,\\nN = 5, BLoB still delivers performance comparable to that of N = 10 and outperforms other\\nbaselines on most datasets. By abandoning the modeling of the posterior distribution, prediction\\nusing the mean of the weight distribution, i.e., BLoB (N=0) sacrifices some degree of calibration\\nin exchange for improved accuracy. Appendix C.5 presents the trade-off between accuracy and\\ncalibration, which is controlled by the standard deviation of the prior Gaussian distribution).\\nBesides Llama2-7B, we also include additional results for RoBERTa-base [60] on text classification\\ntasks in Appendix C.1. Our method consistently achieved either the best or runner-up performance\\nacross nearly all datasets, demonstrating its versatility across different architectures.\\n1 LAP encounters training failures on the ARC-C dataset under a unified setting, where the same hyper-\\nparameter configuration is applied across all datasets. To achieve competitive performance with LAP on the\\nARC-C dataset, we deviate from this unified setting, allowing dataset-specific hyperparameters for LAP. Note\\nthat this introduces an unfair advantage for LAP. Specifically, for the LAP on ARC-C dataset, we set the dropout\\nrate to 0.1, the learning rate to 5e-5, and applied early stopping at the 5,000-th iteration (out of 10,000).\\n8\\nTable 2: Performance on in-distribution and out-of-distribution datasets. All the uncertainty\\nestimation methods are applied to the LoRA adapter added upon the pre-trained Llama2-7B weights.\\nMetric\\nMethod\\nDatasets\\nIn-Dist.\\nSmaller Dist. Shift\\nLarger Dist. Shift\\nOBQA [65]\\nARC-C [18]\\nARC-E [18]\\nChem [38, 37]\\nPhy [38, 37]\\nACC (\\u2191)\\nMLE\\n81.52\\u00b10.25\\n66.20\\u00b10.87\\n75.12\\u00b10.85\\n40.62\\u00b12.25\\n28.82\\u00b11.30\\nMAP\\n81.38\\u00b10.91\\n69.59\\u00b10.33\\n75.47\\u00b10.73\\n44.79\\u00b10.00\\n28.47\\u00b11.20\\nMCD [29]\\n81.72\\u00b10.10\\n69.03\\u00b10.70\\n76.00\\u00b11.58\\n42.71\\u00b10.01\\n29.17\\u00b14.54\\nENS [51, 8, 103]\\n81.38\\u00b10.65\\n67.34\\u00b10.70\\n75.18\\u00b12.03\\n43.75\\u00b11.04\\n30.56\\u00b12.62\\nBBB [11]\\n82.06\\u00b10.59\\n67.25\\u00b11.18\\n75.83\\u00b10.75\\n42.36\\u00b10.49\\n30.21\\u00b12.25\\nLAP [116]\\n82.12\\u00b10.67\\n69.14\\u00b11.15\\n74.94\\u00b10.96\\n44.10\\u00b11.30\\n31.60\\u00b10.49\\nBLoB (N=0)\\n82.73\\u00b10.41\\n69.93\\u00b11.20\\n76.88\\u00b10.41\\n41.67\\u00b12.25\\n31.94\\u00b11.77\\nBLoB (N=5)\\n81.79\\u00b10.94\\n68.36\\u00b11.39\\n75.82\\u00b11.15\\n40.62\\u00b13.07\\n32.64\\u00b10.98\\nBLoB (N=10)\\n81.52\\u00b10.74\\n67.71\\u00b11.13\\n76.37\\u00b10.80\\n44.79\\u00b11.47\\n31.60\\u00b12.73\\nECE (\\u2193)\\nMLE\\n12.55\\u00b10.46\\n22.20\\u00b10.39\\n16.47\\u00b10.86\\n21.72\\u00b10.30\\n29.60\\u00b11.29\\nMAP\\n15.34\\u00b10.27\\n19.31\\u00b11.46\\n15.68\\u00b10.51\\n17.55\\u00b11.95\\n30.25\\u00b12.18\\nMCD [29]\\n14.45\\u00b10.84\\n19.54\\u00b10.33\\n15.32\\u00b11.16\\n17.9\\u00b10.63\\n29.53\\u00b14.20\\nENS [51, 8, 103]\\n13.26\\u00b10.82\\n7.59\\u00b11.43\\n6.44\\u00b10.83\\n12.04\\u00b14.57\\n17.52\\u00b11.28\\nBBB [11]\\n11.38\\u00b11.07\\n19.90\\u00b10.66\\n13.41\\u00b10.85\\n15.67\\u00b11.23\\n26.10\\u00b14.76\\nLAP [116]\\n8.70\\u00b11.77\\n5.84\\u00b10.64\\n8.51\\u00b11.06\\n10.76\\u00b13.41\\n13.91\\u00b10.90\\nBLoB (N=0)\\n8.36\\u00b10.38\\n14.00\\u00b11.02\\n10.70\\u00b10.39\\n15.05\\u00b10.77\\n22.90\\u00b12.27\\nBLoB (N=5)\\n3.40\\u00b10.57\\n9.76\\u00b10.71\\n5.96\\u00b10.93\\n14.33\\u00b11.55\\n18.15\\u00b11.96\\nBLoB (N=10)\\n3.77\\u00b11.47\\n9.55\\u00b10.40\\n5.48\\u00b11.27\\n9.77\\u00b11.35\\n18.29\\u00b11.35\\nNLL (\\u2193)\\nMLE\\n0.73\\u00b10.03\\n1.16\\u00b10.00\\n0.92\\u00b10.03\\n1.56\\u00b10.06\\n1.66\\u00b10.05\\nMAP\\n1.06\\u00b10.04\\n1.10\\u00b10.07\\n0.93\\u00b10.04\\n1.55\\u00b10.06\\n1.65\\u00b10.03\\nMCD [29]\\n1.06\\u00b10.08\\n1.08\\u00b10.01\\n0.88\\u00b10.03\\n1.59\\u00b10.07\\n1.67\\u00b10.05\\nENS [51, 8, 103]\\n0.75\\u00b10.01\\n0.86\\u00b10.01\\n0.69\\u00b10.03\\n1.28\\u00b10.00\\n1.39\\u00b10.03\\nBBB [11]\\n0.66\\u00b10.05\\n1.06\\u00b10.01\\n0.79\\u00b10.02\\n1.49\\u00b10.05\\n1.62\\u00b10.06\\nLAP [116]\\n0.52\\u00b10.01\\n0.81\\u00b10.00\\n0.70\\u00b10.02\\n1.35\\u00b10.03\\n1.36\\u00b10.01\\nBLoB (N=0)\\n0.56\\u00b10.02\\n0.89\\u00b10.02\\n0.67\\u00b10.02\\n1.44\\u00b10.00\\n1.53\\u00b10.02\\nBLoB (N=5)\\n0.53\\u00b10.01\\n0.85\\u00b10.00\\n0.64\\u00b10.01\\n1.39\\u00b10.02\\n1.48\\u00b10.01\\nBLoB (N=10)\\n0.50\\u00b10.01\\n0.83\\u00b10.01\\n0.60\\u00b10.01\\n1.38\\u00b10.01\\n1.46\\u00b10.02\\n4.3\\nResults on Out-of-Distribution Datasets\\nWe use models fine-tuned on OBQA [65] to evaluate the generalization ability of different methods\\nunder distributional shifts. OBQA consists of multiple-choice elementary-level science questions. We\\ncategorize the distributional shifts into two types: smaller and larger shifts. The ARC [18] dataset,\\nwhich also consists of multiple-choice science questions, represents a smaller distributional shift. The\\ncollege-level chemistry and physics subsets of MMLU [38] represent larger distributional shifts.\\nThe results in Table 2 highlight BLoB\\u2019s superior OOD generalization ability compared to other\\nmethods on both smaller and larger distribution shifts. BLoB achieves the highest accuracy when\\nsolely utilizing the mean of the weight distribution in smaller distribution shifts. For larger distribution\\nshifts, incorporating uncertainty through sampling improves model accuracy. Regarding uncertainty\\nestimation, BLoB demonstrates the best or second-best performance in smaller distribution shifts.\\nAlthough there is a slight performance drop with larger distribution shifts, BLoB remains comparable\\nto baselines such as ENS and LAP.\\n5\"}\n\n... # only display 1 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved section content provides the necessary information about the method that is statistically significant on BoolQ with ECE metrics. BLoB (Bayesian Low-rank Adaptation) is the method that performs well on BoolQ, and the hyperparameter \\( k \\) is set to 10 during inference. I will extract the relevant details and format the answer.\n[Action]:\nGenerateAnswer(answer=['Bayesian Low-rank Adaptation (BLoB)', 'N = 10'])"}
{"role": "user", "content": "[Observation]: ['Bayesian Low-rank Adaptation (BLoB)', 'N = 10']"}
