{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: In the paper that proposes GLPFT, which training dataset is larger? In that dataset, what's the format of the data?\n[Answer Format]: Your answer should be a Python list of 2 elements, the first is the name of the dataset, the second is the format of the data in \"A-B pairs\" format, as given in the paper. e.g. [\"MMLU\", \"question-answer pairs\"].\n[Anchor PDF]: '8e85718a-1210-52bf-9aee-9b2bb2d0ae59'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to find the paper that proposes GLPFT. I will use the metadata table to retrieve the title and abstract of the paper with the given paper_id. This will help me confirm if the paper is the one that introduces GLPFT.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT title, abstract FROM metadata WHERE paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59';\")"}
{"role": "user", "content": "[Observation]:\n{\"title\":\"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent\",\"abstract\":\"Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The paper with the given paper_id is \"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent\". However, this paper does not seem to be the one that introduces GLPFT. I need to search for the relevant information about GLPFT and the datasets used in the paper. I will perform a similarity search in the vectorstore to find the relevant context.\n[Action]:\nClassicRetrieve(query='GLPFT training dataset and format', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5955,\"pdf_id\":\"da2a7e24-2083-5ede-95cc-e7cf30cb606f\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"28963fb8-4d4c-5681-9911-bc7e5970d219\",\"text\":\"gformat are much better aligned at the start\\nof training, compared to at 400 steps.\\nComparison between more steps can be\\nfound in Figure 7 (Appendix).\\nThe observations from Section 3 inspire us to decouple\\nformat learning from fine-tuning, in order to alleviate spe-\\ncialization to the fine-tuned task and preserve general in-\\ncontext learning abilities. The key idea is to offload format\\nlearning to a separate small set of parameters during early\\nfine-tuning, and allow the model\\u2019s own parameter changes\"}\n{\"score\":0.5477,\"pdf_id\":\"d9fe039d-0455-5dfb-986a-e944b7c3074d\",\"page_number\":36,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2d141266-869b-5761-be90-92ca7120ce47\",\"text\":\"Published as a conference paper at ICLR 2024\\nG\\nIMPLEMENTATION DETAILS & MORE EXPERIMENTS\\nG.1\\nDATASETS, ALGORITHMS AND TRAINING SETUP\\nDatasets and model. We test our algorithms on two standard FL datasets: MNIST dataset (LeCun,\\n1998) and Fashion MNIST dataset (Xiao et al., 2017). The MNIST dataset contains 60,000 training\\nimages and 10,000 test images. Each image is a gray-scale handwritten digit from 0 to 9 (10 classes\"}\n{\"score\":0.5477,\"pdf_id\":\"7c41d6ca-da14-5bca-8054-06f995f5d47d\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"452d9dce-b86e-5048-a01a-16b2e4893f3a\",\"text\":\"training data.\\n8632\"}\n{\"score\":0.5473,\"pdf_id\":\"a5723e6e-8664-5028-92ce-8e9d14ce702b\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b98a21d4-d304-5784-9582-00bc81440d2b\",\"text\":\"which is publicly available only for the training dataset.\\n15\"}\n{\"score\":0.5447,\"pdf_id\":\"d6b8e585-5aa9-5801-9ac4-451a4595aa51\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3d787f29-6eab-5d8d-a02f-6e643cbf4576\",\"text\":\"Published as a conference paper at ICLR 2024\\nTable 10: A complete list of all data formats used to contaminate the GPT-3.5 base model by fur-\\nther training. Each of these data formats is separately used to format every single instance with\\nrespect to the dataset task. Placeholders are as follows: {split name} indicates the split name;\\n{dataset name} refers to the dataset name; {instance} represents a full instance in classifi-\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results do not provide clear information about the GLPFT training dataset and its format. I will perform another similarity search with a more specific query to find the relevant context.\n[Action]:\nClassicRetrieve(query='GLPFT training dataset format', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5924,\"pdf_id\":\"a5723e6e-8664-5028-92ce-8e9d14ce702b\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b98a21d4-d304-5784-9582-00bc81440d2b\",\"text\":\"which is publicly available only for the training dataset.\\n15\"}\n{\"score\":0.5726,\"pdf_id\":\"da2a7e24-2083-5ede-95cc-e7cf30cb606f\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"28963fb8-4d4c-5681-9911-bc7e5970d219\",\"text\":\"gformat are much better aligned at the start\\nof training, compared to at 400 steps.\\nComparison between more steps can be\\nfound in Figure 7 (Appendix).\\nThe observations from Section 3 inspire us to decouple\\nformat learning from fine-tuning, in order to alleviate spe-\\ncialization to the fine-tuned task and preserve general in-\\ncontext learning abilities. The key idea is to offload format\\nlearning to a separate small set of parameters during early\\nfine-tuning, and allow the model\\u2019s own parameter changes\"}\n{\"score\":0.5619,\"pdf_id\":\"d9fe039d-0455-5dfb-986a-e944b7c3074d\",\"page_number\":36,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2d141266-869b-5761-be90-92ca7120ce47\",\"text\":\"Published as a conference paper at ICLR 2024\\nG\\nIMPLEMENTATION DETAILS & MORE EXPERIMENTS\\nG.1\\nDATASETS, ALGORITHMS AND TRAINING SETUP\\nDatasets and model. We test our algorithms on two standard FL datasets: MNIST dataset (LeCun,\\n1998) and Fashion MNIST dataset (Xiao et al., 2017). The MNIST dataset contains 60,000 training\\nimages and 10,000 test images. Each image is a gray-scale handwritten digit from 0 to 9 (10 classes\"}\n{\"score\":0.5601,\"pdf_id\":\"7c41d6ca-da14-5bca-8054-06f995f5d47d\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"452d9dce-b86e-5048-a01a-16b2e4893f3a\",\"text\":\"training data.\\n8632\"}\n{\"score\":0.5541,\"pdf_id\":\"7908763f-3a9d-5ce5-af59-f68888750583\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b3c91259-ee0a-5f15-9f36-05d65280c9eb\",\"text\":\"GPT-3. See the attached supplementary material for the templates for all other datasets.\\n37\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results are still not providing the specific information about the GLPFT training dataset and its format. I will try a more focused query to retrieve the exact dataset and format used in the GLPFT training.\n[Action]:\nClassicRetrieve(query='GLPFT training dataset format details', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6124,\"pdf_id\":\"a5723e6e-8664-5028-92ce-8e9d14ce702b\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b98a21d4-d304-5784-9582-00bc81440d2b\",\"text\":\"which is publicly available only for the training dataset.\\n15\"}\n{\"score\":0.5873,\"pdf_id\":\"7c41d6ca-da14-5bca-8054-06f995f5d47d\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"452d9dce-b86e-5048-a01a-16b2e4893f3a\",\"text\":\"training data.\\n8632\"}\n{\"score\":0.5845,\"pdf_id\":\"d6b8e585-5aa9-5801-9ac4-451a4595aa51\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3d787f29-6eab-5d8d-a02f-6e643cbf4576\",\"text\":\"Published as a conference paper at ICLR 2024\\nTable 10: A complete list of all data formats used to contaminate the GPT-3.5 base model by fur-\\nther training. Each of these data formats is separately used to format every single instance with\\nrespect to the dataset task. Placeholders are as follows: {split name} indicates the split name;\\n{dataset name} refers to the dataset name; {instance} represents a full instance in classifi-\"}\n{\"score\":0.5764,\"pdf_id\":\"10a5bb71-a26f-53ec-83aa-3e2d1b120c81\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a593f547-383a-5331-b550-0853f7a4423c\",\"text\":\"A\\nAppendix\\nA.1\\nDataset and Data Card\\nThe\\nSeeGULL\\ndataset\\nis\\nreleased\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull.\\nWe\\nrelease\\nthe\\ndata\\ncard\\nfor\\nthe\\nSeeGULL\\ndataset\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull\\/blob\\/main\\/seegull_datacard.pdf,\\nfollowing the format proposed by\\nPushkarna\\net al. (2022). The data card includes details of the\\ndataset such as intended usage, \\ufb01eld names and\\nmeanings, annotator recruitment and payments\\n(also in Appendix A.6 and A.9), and more.\\nA.2\"}\n{\"score\":0.5716,\"pdf_id\":\"da2a7e24-2083-5ede-95cc-e7cf30cb606f\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"28963fb8-4d4c-5681-9911-bc7e5970d219\",\"text\":\"gformat are much better aligned at the start\\nof training, compared to at 400 steps.\\nComparison between more steps can be\\nfound in Figure 7 (Appendix).\\nThe observations from Section 3 inspire us to decouple\\nformat learning from fine-tuning, in order to alleviate spe-\\ncialization to the fine-tuned task and preserve general in-\\ncontext learning abilities. The key idea is to offload format\\nlearning to a separate small set of parameters during early\\nfine-tuning, and allow the model\\u2019s own parameter changes\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results are still not providing the specific information about the GLPFT training dataset and its format. I will perform a more targeted search by focusing on the sections and tables of the paper to find the relevant details.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND (section_title LIKE '%Dataset%' OR section_title LIKE '%Data%') LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"5.3 Data Scaling Law\",\"section_content\":\"To assess the impact of the amount of training data on performance, we conduct a data scaling law analysis with the 7B backbone on ToolBench, varying the number of annotated training instances from $12.1\\\\mathrm{k}$ to $62.7\\\\mathrm{k}$ . The results in different metrics are depicted in Figure $4^{5}$ . Several observations can be drawn from the results. Firstly, when comparing $\\\\alpha$ -UMi (solid red curves) to Single-LLM (solid blue curves), there are signifcant and consistent enhancements in metrics such as Plan ACC, Act. EM, Hallu., and Arg. F1 across various scales of training data. While only minor improvements are observed in the R-L metric, which directly refects the performance of the summarizer, this suggests that the performance enhancement of our framework is mainly attributed to the separation of the planner and the caller. Secondly, the performances of Multi-LLMone-stage and Single-LLMm ulti-task exhibit severe fuctuations in all metrics except for Plan ACC, indicating instability in training the framework through direct fne-tuning or multi-task fne-tuning. Thirdly, Single-LLM achieves optimal results in different metrics at different data scales. For example, it attains peak performance in Plan ACC with $31.3\\\\mathrm{k}$ instances and the best Arg. F1 and R-L with $62.7\\\\mathrm{k}$ instances. This suggests the challenge of obtaining a single LLM that uniformly performs well across all metrics. In contrast, the performance of our framework consistently improves with increased data scale across all metrics.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section does not provide the specific information about the GLPFT training dataset and its format. I will perform another retrieval focusing on the sections that might contain the dataset details, such as the \"Experiments\" or \"Experimental Setup\" sections.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND (section_title LIKE '%Experiments%' OR section_title LIKE '%Experimental Setup%') LIMIT 5;\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The sections \"Experiments\" and \"Experimental Setup\" did not yield any results. I will try a more general approach by searching for the term \"GLPFT\" in the chunks table to find the relevant context.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%GLPFT%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"enhanced performance for each capability. Second,\\nthe modular structure allows for individual updates\\nto each component as required, ensuring adaptabil-\\nity and streamlined maintenance. Third, since each\\ncomponent focuses solely on a specific capability,\\npotentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework,\\nwe introduce a novel global-to-local progressive\\nfine-tuning strategy (GLPFT). First, an LLM back-\\nbone is trained on the original training dataset\"}\n{\"text_content\":\"framework outperforms the single-LLM approach\\nacross various model and data sizes. Moreover, we\\nshow the necessity of the GLPFT strategy for the\\nsuccess of our framework and delve into the rea-\\nsons behind the improved performance. Finally, the\\nresults confirm our assumption that smaller LLMs\\ncan be used in our multi-LLM framework to culti-\\nvate individual tool learning capabilities and attain\\na competitive overall performance.\\nTo sum up, this work makes three critical contri-\"}\n{\"text_content\":\"butions. First, we demonstrate that small LLMs are\\nweak tool learners and introduce \\u03b1-UMi, a multi-\\nLLM framework for building LLM agents, that\\noutperforms the existing single-LLM approach in\\ntool use. Second, we propose a GLPFT fine-tuning\\nstrategy, which has proven to be essential for the\\nsuccess of our framework. Third, we perform a\\nthorough analysis, delving into data scaling laws\\nand investigating the underlying reasons behind the\\nsuperior performance of our framework.\\n2\\nRelated Works\\n2.1\"}\n{\"text_content\":\"Taking into account these two points, we propose\\na global-to-local progressive fine-tuning (GLPFT)\\nstrategy for \\u03b1-UMi. The motivation behind this\\nstrategy is to first exploit the mechanism by which\\nthe generation of rationale, action, and final answer\\ncan mutually enhance each other. Then, once the\\nsingle LLM reaches its performance ceiling, it is\\nsubsequently split into planner, caller and summa-\\nrizer for further fine-tuning, in order to enhance its\\ncapabilities in the subtasks and mitigate the perfor-\"}\n{\"text_content\":\"mance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy\\ncomprises two distinct stages. The first stage in-\\nvolves global fine-tuning, where we fine-tune a\\nbackbone LLM on the original training dataset\\nwithout distinguishing between sub-tasks. After\\nthis stage, the backbone LLM is trained to sequen-\\ntially output the rationale, action, and answer as\\nintroduced in Section 3.1. Then, we create three\\ncopies of the backbone LLM, designated as the\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks provide context about the GLPFT strategy but do not specify the training dataset and its format. I will perform a more targeted search by looking for specific mentions of the training dataset and its format.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%training dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"enhanced performance for each capability. Second,\\nthe modular structure allows for individual updates\\nto each component as required, ensuring adaptabil-\\nity and streamlined maintenance. Third, since each\\ncomponent focuses solely on a specific capability,\\npotentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework,\\nwe introduce a novel global-to-local progressive\\nfine-tuning strategy (GLPFT). First, an LLM back-\\nbone is trained on the original training dataset\"}\n{\"text_content\":\"without discriminating between sub-tasks, enhanc-\\ning the comprehensive understanding of the tool-\\nlearning task. Three copies of this LLM backbone\\nare created to instantiate the planner, caller, and\\nsummarizer, respectively. In the second stage, the\\ntraining dataset is reorganized into new datasets tai-\\nlored to each LLM\\u2019s role in tool use, and continual\\n1In astronomy, the name \\u201c\\u03b1-UMi\\u201d is an alias of the Polaris\\nStar (https:\\/\\/en.wikipedia.org\\/wiki\\/Polaris), which\"}\n{\"text_content\":\"mance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy\\ncomprises two distinct stages. The first stage in-\\nvolves global fine-tuning, where we fine-tune a\\nbackbone LLM on the original training dataset\\nwithout distinguishing between sub-tasks. After\\nthis stage, the backbone LLM is trained to sequen-\\ntially output the rationale, action, and answer as\\nintroduced in Section 3.1. Then, we create three\\ncopies of the backbone LLM, designated as the\"}\n{\"text_content\":\"planner, caller, and summarizer, respectively.\\nThe second stage is local fine-tuning, where\\nwe reorganize the training dataset tailored to each\\nLLM\\u2019s role, as introduced in Section 3.2. We then\\nproceed to fine-tune the planner, caller, and sum-\\nmarizer on their respective datasets, thereby further\\nenhancing their specific abilities in each sub-task.\\nDuring this local fine-tuning stage, we opt to reuse\\nthe set of user instructions curated in the global\\nfine-tuning stage. The only adjustment made to\"}\n\nIn total, 4 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks mention the training dataset but do not specify the name or format. I will perform a more targeted search by looking for specific mentions of the dataset name and format.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%training dataset%format%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results. I will try a different approach by performing a similarity search in the vectorstore to find the relevant context about the training dataset and its format.\n[Action]:\nClassicRetrieve(query=\"In the paper that proposes GLPFT, which training dataset is larger? What's the format of the data in that dataset?\", limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6002,\"pdf_id\":\"0b82b7cf-a4c9-5f42-b033-241d9be156ae\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"32bd58ed-9573-5062-be94-bbd715925f9c\",\"text\":\"and the size of training dataset.\\n17\"}\n{\"score\":0.5561,\"pdf_id\":\"af618d47-f6c3-5187-b3b8-1918b2eaecea\",\"page_number\":28,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f7f925d7-0b0f-537a-9fe2-8f9b4393c2af\",\"text\":\"and how they scale with dataset size.\\n1016\\n28\"}\n{\"score\":0.5486,\"pdf_id\":\"8ee73844-63cd-5d54-9c71-63cd3566f545\",\"page_number\":22,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"122dd616-e583-58b3-b941-6a785d12cdcc\",\"text\":\"This section provides details about the datasets used in our experiments. The size of each dataset is\\nsummarized in Table 1.\\nTable 1: Summary of datasets used in our experiments.\\nDataset\\n# Features\\n# Feature Groups\\n# Classes\\n# Samples\\nMNIST\\n784\\n\\u2013\\n10\\n60,000\\nIntubation\\n112\\n35\\n2\\n65,515\\nROSMAP\\n46\\n43\\n2\\n13,438\\nImageNette\\n50,176\\n196\\n10\\n13,395\\nImageNet-100\\n50,176\\n196\\n100\\n135,000\\nHistopathology\\n50,176\\n196\\n2\\n3152\\nMNIST. This is the standard digit classification dataset (LeCun et al., 1998). We downloaded it\"}\n{\"score\":0.5344,\"pdf_id\":\"04438a2a-cf4c-5db0-b6a0-46ca76f39b42\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a2284a60-32a0-5f9f-a5ab-70dc2b675e9b\",\"text\":\"\\u2022 Model Size: Larger models have greater learning capacities, which can enhance the GC for\\nlong-tail knowledge given a fixed training data size.\\n\\u2022 Window Size: A larger window size allows the model to capture longer-range relationships\\nwithin each training sequence, potentially highlighting more subtle distinctions and leading\\nto a more long-tail distribution.\\n\\u2022 Training Data Size: Larger data sizes and larger window sizes might result in a higher\"}\n{\"score\":0.5279,\"pdf_id\":\"c5d10823-f12c-5a94-8ad4-3b329e03c20e\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8b98caf9-c784-5bab-9e2f-c51562e6db56\",\"text\":\"Published as a conference paper at ICLR 2024\\nTable 1: Comparison with different Instruction-following datasets: G\\nand M are the abbreviation for the training subset of GSM8K and\\nMATH dataset. The baseline datsets include recent RFT-u13b (Yuan\\net al., 2023) and WizardMath (Luo et al., 2023).\\nDatasets\\nSeed\\nAnnotation\\nAvailable\\nRFT-100k\\nG\\nLlama\\n\\u2713\\nWizardMath-96k\\nG+M\\nGPT-4\\n\\u2717\\nOurs-49k\\nG+M\\nGPT-4\\n\\u2713\\nOurs-80k\\nG+M\\nGPT-4 +\\n\\u2713\\nSelf-distillation\\n7B\\n70B\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\nAccuracy\\n17.8\\n24.8\\n46.2\\n50.7\\n73.1\\nMathCoder\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results do not provide the specific information about the training dataset and its format for the GLPFT paper. I will perform another similarity search with a more specific query to find the relevant context.\n[Action]:\nClassicRetrieve(query='In the paper that proposes GLPFT, which training dataset is larger and what is the format of the data in that dataset?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5973,\"pdf_id\":\"0b82b7cf-a4c9-5f42-b033-241d9be156ae\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"32bd58ed-9573-5062-be94-bbd715925f9c\",\"text\":\"and the size of training dataset.\\n17\"}\n{\"score\":0.5523,\"pdf_id\":\"af618d47-f6c3-5187-b3b8-1918b2eaecea\",\"page_number\":28,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f7f925d7-0b0f-537a-9fe2-8f9b4393c2af\",\"text\":\"and how they scale with dataset size.\\n1016\\n28\"}\n{\"score\":0.5394,\"pdf_id\":\"10a5bb71-a26f-53ec-83aa-3e2d1b120c81\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a593f547-383a-5331-b550-0853f7a4423c\",\"text\":\"A\\nAppendix\\nA.1\\nDataset and Data Card\\nThe\\nSeeGULL\\ndataset\\nis\\nreleased\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull.\\nWe\\nrelease\\nthe\\ndata\\ncard\\nfor\\nthe\\nSeeGULL\\ndataset\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull\\/blob\\/main\\/seegull_datacard.pdf,\\nfollowing the format proposed by\\nPushkarna\\net al. (2022). The data card includes details of the\\ndataset such as intended usage, \\ufb01eld names and\\nmeanings, annotator recruitment and payments\\n(also in Appendix A.6 and A.9), and more.\\nA.2\"}\n{\"score\":0.5362,\"pdf_id\":\"f408e618-2ae0-52cf-9752-21b3a082c569\",\"page_number\":9,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"7b52465f-008e-56da-89ff-4586e6e0fb7f\",\"text\":\"datasets, (ii) only the target dataset, and (iii) datasets excluding the target dataset. These variants are\\ncompared against GAT and GIANT, which represent the best supervised and self-supervised baselines,\\nrespectively. Notably, GFT consistently outperforms all baselines, regardless of the pre-training\\ndataset utilized. Interestingly, performance improves when using datasets excluding the target dataset\\ncompared to pre-training solely on the target dataset. We hypothesize that the computation trees from\"}\n{\"score\":0.5335,\"pdf_id\":\"8ee73844-63cd-5d54-9c71-63cd3566f545\",\"page_number\":22,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"122dd616-e583-58b3-b941-6a785d12cdcc\",\"text\":\"This section provides details about the datasets used in our experiments. The size of each dataset is\\nsummarized in Table 1.\\nTable 1: Summary of datasets used in our experiments.\\nDataset\\n# Features\\n# Feature Groups\\n# Classes\\n# Samples\\nMNIST\\n784\\n\\u2013\\n10\\n60,000\\nIntubation\\n112\\n35\\n2\\n65,515\\nROSMAP\\n46\\n43\\n2\\n13,438\\nImageNette\\n50,176\\n196\\n10\\n13,395\\nImageNet-100\\n50,176\\n196\\n100\\n135,000\\nHistopathology\\n50,176\\n196\\n2\\n3152\\nMNIST. This is the standard digit classification dataset (LeCun et al., 1998). We downloaded it\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will try a more targeted SQL query by looking for the dataset name and format in the sections table.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND section_content LIKE '%training dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"1 Introduction\",\"section_content\":\"Large Language Models (LLMs) have revolutionized natural language processing with remarkable profciency in understanding and generating text. Despite their impressive capabilities, LLMs are not without limitations. Notably, they lack domain specifcity, real-time information, and face challenges in solving specialized problems such as mathematics (Gou et al., 2023) and program compilation (OpenAI, 2023a). Hence, integrating LLMs with external tools, such as API calls and Python functions, becomes imperative to extend their capabilities and enhance the overall performance. Consequently, LLM agents have become a prominent area for both academia and industry, employing large language models to determine when and how to utilize external tools to tackle various tasks.\\n\\nIn addition to exploring proprietary LLMs like GPT-4, researchers have also actively engaged in developing customizable agent systems by fnetuning open-source LLMs on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Gou et al., 2023). The challenge of tool learning demands suffciently large and complex LLMs. These models must not only comprehend user queries but also excel in task planning, tool selection and invocation, and result summarization (Yujia et al., 2023). These capabilities draw upon different facets of the LLMs; for instance, planning relies more on reasoning ability, while tool selection and invocation demand legal and accurate request writing, and result summarization requires adept conclusion-drawing skills. While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Touvron et al., 2023a,b). Moreover, the tools could be updated frequently in practical scenarios, when the entire LLM requires potential retraining.\\n\\nTo address these challenges, we propose a multiLLM agent framework for tool learning, $\\\\alpha{-}\\\\mathrm{UMi}^{1}$ . As illustrated in Figure 1, $\\\\alpha$ -UMi decomposes the capabilities of a single LLM into three components, namely planner, caller, and summarizer. Each of these components is implemented by a single LLM and trained to focus on a specifc capability. The planner is designed to generate the rationale based on the current state of the system and weighs between selecting the caller or summarizer to generate downstream output, or even deciding to terminate the execution. The caller is directed by the rationale and responsible for invocating specifc tools. The summarizer is guided by the planner to craft the ultimate user answer based on the execution trajectory. These components collaborate seamlessly to accomplish various tasks. Compared to previous approaches, our modular framework has three distinct advantages. First, each component undergoes training for a designated role, ensuring enhanced performance for each capability. Second, the modular structure allows for individual updates to each component as required, ensuring adaptability and streamlined maintenance. Third, since each component focuses solely on a specifc capability, potentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework, we introduce a novel global-to-local progressive fne-tuning strategy (GLPFT). First, an LLM backbone is trained on the original training dataset without discriminating between sub-tasks, enhancing the comprehensive understanding of the toollearning task. Three copies of this LLM backbone are created to instantiate the planner, caller, and summarizer, respectively. In the second stage, the training dataset is reorganized into new datasets tailored to each LLM\\u2019s role in tool use, and continual fne-tuning of the planner, caller, and summarizer is performed on their respective datasets.\\n\\nWe employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our $\\\\alpha$ -UMi agent on several tool learning benchmarks (Qin et al., 2023b; Tang et al., 2023). Experimental results demonstrate that our proposed framework outperforms the single-LLM approach across various model and data sizes. Moreover, we show the necessity of the GLPFT strategy for the success of our framework and delve into the reasons behind the improved performance. Finally, the results confrm our assumption that smaller LLMs can be used in our multi-LLM framework to cultivate individual tool learning capabilities and attain a competitive overall performance.\\nTo sum up, this work makes three critical contributions. First, we demonstrate that small LLMs are weak tool learners and introduce $\\\\alpha$ -UMi, a multiLLM framework for building LLM agents, that outperforms the existing single-LLM approach in tool use. Second, we propose a GLPFT fne-tuning strategy, which has proven to be essential for the success of our framework. Third, we perform a thorough analysis, delving into data scaling laws and investigating the underlying reasons behind the superior performance of our framework.\"}\n{\"section_title\":\"3.2 The $\\\\alpha$ -UMi Framework\",\"section_content\":\"As previously mentioned, the task of tool learning imposes a signifcant demand on the capabilities of LLMs, including task planning, tool invocation, and result summarization. Coping with all these capabilities using a single open-source LLM, especially when opting for a smaller LLM, appears to be challenging. To address this challenge, we introduce the $\\\\alpha$ -UMi framework, which breaks down the tool learning task into three sub-tasks and assigns each sub-task to a dedicated LLM. Figure 1 presents an illustration of our framework, which incorporates three distinct LLM components: planner $\\\\mathcal{M}_{\\\\mathrm{plan}}$ , caller $\\\\mathcal{M}_{\\\\mathrm{call}}$ , and summarizer $\\\\mathcal{M}_{\\\\mathrm{sum}}$ . These components are differentiated by their roles in tool use, and each component model has a unique task defnition, system prompt2, and model input.\\nThe workfow of $\\\\alpha$ -UMi is shown in Figure 2. Upon receiving the user instruction $q$ , the planner generates a rationale comprising hints for the this step. This may trigger the caller to engage with the tools and subsequently receive observations from the tools. This iterative planner-caller-tool loop continues until the planner determines that it has gathered suffcient information to resolve the instruction. At this point, the planner transitions to the summarizer to generate the fnal answer. Alternatively, if the planner deems the instruction unsolvable, it may abandon the execution.\\nPlanner: The planner assumes responsibility for planning and decision-making, serving as the \\u201cbrain\\u201d of our agent framework. Specifcally, the model input for the planner comprises the system prompt $\\\\mathcal{P}_{\\\\mathrm{plan}}$ , the user instruction $q$ , and the previous execution trajectory $\\\\tau_{t-1}$ . Using this input, the planner generates the rationale $r_{t}$ :\\nFollowing the rationale, the planner generates the decision for the next step: (1) If the decision is \\u201cNext: Caller\\u201d, the caller will be activated and an action will be generated for calling tools. (2) If the decision is \\u201cNext: Summarizer\\u201d, the summarizer will be activated to generate the fnal answer for the user, and the agent execution will fnish. (3) If the decision is \\u201cNext: Give up\\u201d, it means that the user\\u2019s instruction cannot be solved in the current situation, and the system will be terminated.\\nCaller: Interacting with the tools requires the LLM to generate legal and useful requests, which may confict with other abilities such as reasoning and general response generation during fne-tuning. Therefore, we train a specialized caller to generate the action for using tools. The caller takes the user instruction $q$ and the previous execution trajectory $\\\\tau_{t-1}$ as input. To make the caller focus on the planner\\u2019s rationale $r_{t}$ in the current step, we also design a prompt $\\\\mathcal{P}_{\\\\mathrm{call}}$ to explicitly remind the caller:\\nSummarizer: The agent\\u2019s fnal response, which aims to offer informative and helpful information to the user, is distinct from the rationales that primarily focus on planning and reasoning. Therefore, we employ a dedicated summarizer tasked with generating the fnal answer $a_{n}$ . This model utilizes a concise prompt $\\\\mathcal{P}_{\\\\mathrm{sum}}$ designed to guide the model in concentrating on summarizing the execution trajectory and presenting the answer to the user:\\nIn Figure 7 and Figure 8, we show several cases of our $\\\\alpha$ -UMi on downstream tasks.\\nTo effectively fne-tune the above multi-LLM system is a complex endeavor: On one hand, generating the rationale, action, and fnal answer can facilitate each other during the training process, and enhance the model\\u2019s comprehension of the entire agent task (Chen et al., 2023). On the other hand, the constraints on model capacity make it challenging to fne-tune a small LLM to achieve peak performance in generating rationales, actions, and fnal answers simultaneously (Dong et al., 2023). Taking into account these two points, we propose a global-to-local progressive fne-tuning (GLPFT) strategy for $\\\\alpha$ -UMi. The motivation behind this strategy is to frst exploit the mechanism by which the generation of rationale, action, and fnal answer can mutually enhance each other. Then, once the single LLM reaches its performance ceiling, it is subsequently split into planner, caller and summarizer for further fne-tuning, in order to enhance its capabilities in the subtasks and mitigate the performance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy comprises two distinct stages. The frst stage involves global fne-tuning, where we fne-tune a backbone LLM on the original training dataset without distinguishing between sub-tasks. After this stage, the backbone LLM is trained to sequentially output the rationale, action, and answer as introduced in Section 3.1. Then, we create three copies of the backbone LLM, designated as the planner, caller, and summarizer, respectively.\\nThe second stage is local fne-tuning, where we reorganize the training dataset tailored to each LLM\\u2019s role, as introduced in Section 3.2. We then proceed to fne-tune the planner, caller, and summarizer on their respective datasets, thereby further enhancing their specifc abilities in each sub-task. During this local fne-tuning stage, we opt to reuse the set of user instructions curated in the global fne-tuning stage. The only adjustment made to the training set is the change in the format of the training data. As illustrated in Figure 3, the fnetuning objective during the second stage for the planner, caller, and summarizer is oriented towards generating the rationale, action, and fnal answer, respectively. While the gradients from other text spans are stopped. Simultaneously, we refne the system prompts for the training data of the planner, caller, and summarizer, as detailed in Appendix A.\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections mention the training dataset but do not specify the name or format. I will perform a more targeted SQL query by looking for the dataset name and format in the metadata table.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%training dataset%format%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results. I will try a different approach by performing a similarity search in the vectorstore with a more specific query to find the relevant context about the training dataset and its format.\n[Action]:\nClassicRetrieve(query='In the GLPFT paper, which training dataset is larger and what is the format of the data in that dataset?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5967,\"pdf_id\":\"0b82b7cf-a4c9-5f42-b033-241d9be156ae\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"32bd58ed-9573-5062-be94-bbd715925f9c\",\"text\":\"and the size of training dataset.\\n17\"}\n{\"score\":0.5596,\"pdf_id\":\"af618d47-f6c3-5187-b3b8-1918b2eaecea\",\"page_number\":28,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f7f925d7-0b0f-537a-9fe2-8f9b4393c2af\",\"text\":\"and how they scale with dataset size.\\n1016\\n28\"}\n{\"score\":0.5369,\"pdf_id\":\"2d07ddaa-6d9a-531b-a864-3eb99f316831\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4421695d-7352-5209-b89e-2fd962b107aa\",\"text\":\"reflective of practitioner attention, and it varies significantly based on the popularity of the dataset.\\nHighly downloaded datasets tend to have more comprehensive and longer dataset cards (Fig. 3a), with\\nan emphasis on the Dataset Description and Dataset Structure sections (Fig. 3d). Conversely, less\\npopular datasets have shorter cards (Fig. 3a) with a greater emphasis on the Additional Information\\nsection (Fig. 3d). Despite this, sections such as Dataset Creation and Considerations for Using the\"}\n{\"score\":0.533,\"pdf_id\":\"a5723e6e-8664-5028-92ce-8e9d14ce702b\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b98a21d4-d304-5784-9582-00bc81440d2b\",\"text\":\"which is publicly available only for the training dataset.\\n15\"}\n{\"score\":0.5318,\"pdf_id\":\"10a5bb71-a26f-53ec-83aa-3e2d1b120c81\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a593f547-383a-5331-b550-0853f7a4423c\",\"text\":\"A\\nAppendix\\nA.1\\nDataset and Data Card\\nThe\\nSeeGULL\\ndataset\\nis\\nreleased\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull.\\nWe\\nrelease\\nthe\\ndata\\ncard\\nfor\\nthe\\nSeeGULL\\ndataset\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull\\/blob\\/main\\/seegull_datacard.pdf,\\nfollowing the format proposed by\\nPushkarna\\net al. (2022). The data card includes details of the\\ndataset such as intended usage, \\ufb01eld names and\\nmeanings, annotator recruitment and payments\\n(also in Appendix A.6 and A.9), and more.\\nA.2\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will try a more targeted SQL query by looking for the dataset name and format in the sections table, but with a broader search term.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND section_content LIKE '%dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"Abstract\",\"section_content\":\"Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specifc capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fne-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fne-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fne-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its effcacy and advantages in tool learning.\"}\n{\"section_title\":\"1 Introduction\",\"section_content\":\"Large Language Models (LLMs) have revolutionized natural language processing with remarkable profciency in understanding and generating text. Despite their impressive capabilities, LLMs are not without limitations. Notably, they lack domain specifcity, real-time information, and face challenges in solving specialized problems such as mathematics (Gou et al., 2023) and program compilation (OpenAI, 2023a). Hence, integrating LLMs with external tools, such as API calls and Python functions, becomes imperative to extend their capabilities and enhance the overall performance. Consequently, LLM agents have become a prominent area for both academia and industry, employing large language models to determine when and how to utilize external tools to tackle various tasks.\\n\\nIn addition to exploring proprietary LLMs like GPT-4, researchers have also actively engaged in developing customizable agent systems by fnetuning open-source LLMs on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Gou et al., 2023). The challenge of tool learning demands suffciently large and complex LLMs. These models must not only comprehend user queries but also excel in task planning, tool selection and invocation, and result summarization (Yujia et al., 2023). These capabilities draw upon different facets of the LLMs; for instance, planning relies more on reasoning ability, while tool selection and invocation demand legal and accurate request writing, and result summarization requires adept conclusion-drawing skills. While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Touvron et al., 2023a,b). Moreover, the tools could be updated frequently in practical scenarios, when the entire LLM requires potential retraining.\\n\\nTo address these challenges, we propose a multiLLM agent framework for tool learning, $\\\\alpha{-}\\\\mathrm{UMi}^{1}$ . As illustrated in Figure 1, $\\\\alpha$ -UMi decomposes the capabilities of a single LLM into three components, namely planner, caller, and summarizer. Each of these components is implemented by a single LLM and trained to focus on a specifc capability. The planner is designed to generate the rationale based on the current state of the system and weighs between selecting the caller or summarizer to generate downstream output, or even deciding to terminate the execution. The caller is directed by the rationale and responsible for invocating specifc tools. The summarizer is guided by the planner to craft the ultimate user answer based on the execution trajectory. These components collaborate seamlessly to accomplish various tasks. Compared to previous approaches, our modular framework has three distinct advantages. First, each component undergoes training for a designated role, ensuring enhanced performance for each capability. Second, the modular structure allows for individual updates to each component as required, ensuring adaptability and streamlined maintenance. Third, since each component focuses solely on a specifc capability, potentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework, we introduce a novel global-to-local progressive fne-tuning strategy (GLPFT). First, an LLM backbone is trained on the original training dataset without discriminating between sub-tasks, enhancing the comprehensive understanding of the toollearning task. Three copies of this LLM backbone are created to instantiate the planner, caller, and summarizer, respectively. In the second stage, the training dataset is reorganized into new datasets tailored to each LLM\\u2019s role in tool use, and continual fne-tuning of the planner, caller, and summarizer is performed on their respective datasets.\\n\\nWe employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our $\\\\alpha$ -UMi agent on several tool learning benchmarks (Qin et al., 2023b; Tang et al., 2023). Experimental results demonstrate that our proposed framework outperforms the single-LLM approach across various model and data sizes. Moreover, we show the necessity of the GLPFT strategy for the success of our framework and delve into the reasons behind the improved performance. Finally, the results confrm our assumption that smaller LLMs can be used in our multi-LLM framework to cultivate individual tool learning capabilities and attain a competitive overall performance.\\nTo sum up, this work makes three critical contributions. First, we demonstrate that small LLMs are weak tool learners and introduce $\\\\alpha$ -UMi, a multiLLM framework for building LLM agents, that outperforms the existing single-LLM approach in tool use. Second, we propose a GLPFT fne-tuning strategy, which has proven to be essential for the success of our framework. Third, we perform a thorough analysis, delving into data scaling laws and investigating the underlying reasons behind the superior performance of our framework.\"}\n{\"section_title\":\"3.2 The $\\\\alpha$ -UMi Framework\",\"section_content\":\"As previously mentioned, the task of tool learning imposes a signifcant demand on the capabilities of LLMs, including task planning, tool invocation, and result summarization. Coping with all these capabilities using a single open-source LLM, especially when opting for a smaller LLM, appears to be challenging. To address this challenge, we introduce the $\\\\alpha$ -UMi framework, which breaks down the tool learning task into three sub-tasks and assigns each sub-task to a dedicated LLM. Figure 1 presents an illustration of our framework, which incorporates three distinct LLM components: planner $\\\\mathcal{M}_{\\\\mathrm{plan}}$ , caller $\\\\mathcal{M}_{\\\\mathrm{call}}$ , and summarizer $\\\\mathcal{M}_{\\\\mathrm{sum}}$ . These components are differentiated by their roles in tool use, and each component model has a unique task defnition, system prompt2, and model input.\\nThe workfow of $\\\\alpha$ -UMi is shown in Figure 2. Upon receiving the user instruction $q$ , the planner generates a rationale comprising hints for the this step. This may trigger the caller to engage with the tools and subsequently receive observations from the tools. This iterative planner-caller-tool loop continues until the planner determines that it has gathered suffcient information to resolve the instruction. At this point, the planner transitions to the summarizer to generate the fnal answer. Alternatively, if the planner deems the instruction unsolvable, it may abandon the execution.\\nPlanner: The planner assumes responsibility for planning and decision-making, serving as the \\u201cbrain\\u201d of our agent framework. Specifcally, the model input for the planner comprises the system prompt $\\\\mathcal{P}_{\\\\mathrm{plan}}$ , the user instruction $q$ , and the previous execution trajectory $\\\\tau_{t-1}$ . Using this input, the planner generates the rationale $r_{t}$ :\\nFollowing the rationale, the planner generates the decision for the next step: (1) If the decision is \\u201cNext: Caller\\u201d, the caller will be activated and an action will be generated for calling tools. (2) If the decision is \\u201cNext: Summarizer\\u201d, the summarizer will be activated to generate the fnal answer for the user, and the agent execution will fnish. (3) If the decision is \\u201cNext: Give up\\u201d, it means that the user\\u2019s instruction cannot be solved in the current situation, and the system will be terminated.\\nCaller: Interacting with the tools requires the LLM to generate legal and useful requests, which may confict with other abilities such as reasoning and general response generation during fne-tuning. Therefore, we train a specialized caller to generate the action for using tools. The caller takes the user instruction $q$ and the previous execution trajectory $\\\\tau_{t-1}$ as input. To make the caller focus on the planner\\u2019s rationale $r_{t}$ in the current step, we also design a prompt $\\\\mathcal{P}_{\\\\mathrm{call}}$ to explicitly remind the caller:\\nSummarizer: The agent\\u2019s fnal response, which aims to offer informative and helpful information to the user, is distinct from the rationales that primarily focus on planning and reasoning. Therefore, we employ a dedicated summarizer tasked with generating the fnal answer $a_{n}$ . This model utilizes a concise prompt $\\\\mathcal{P}_{\\\\mathrm{sum}}$ designed to guide the model in concentrating on summarizing the execution trajectory and presenting the answer to the user:\\nIn Figure 7 and Figure 8, we show several cases of our $\\\\alpha$ -UMi on downstream tasks.\\nTo effectively fne-tune the above multi-LLM system is a complex endeavor: On one hand, generating the rationale, action, and fnal answer can facilitate each other during the training process, and enhance the model\\u2019s comprehension of the entire agent task (Chen et al., 2023). On the other hand, the constraints on model capacity make it challenging to fne-tune a small LLM to achieve peak performance in generating rationales, actions, and fnal answers simultaneously (Dong et al., 2023). Taking into account these two points, we propose a global-to-local progressive fne-tuning (GLPFT) strategy for $\\\\alpha$ -UMi. The motivation behind this strategy is to frst exploit the mechanism by which the generation of rationale, action, and fnal answer can mutually enhance each other. Then, once the single LLM reaches its performance ceiling, it is subsequently split into planner, caller and summarizer for further fne-tuning, in order to enhance its capabilities in the subtasks and mitigate the performance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy comprises two distinct stages. The frst stage involves global fne-tuning, where we fne-tune a backbone LLM on the original training dataset without distinguishing between sub-tasks. After this stage, the backbone LLM is trained to sequentially output the rationale, action, and answer as introduced in Section 3.1. Then, we create three copies of the backbone LLM, designated as the planner, caller, and summarizer, respectively.\\nThe second stage is local fne-tuning, where we reorganize the training dataset tailored to each LLM\\u2019s role, as introduced in Section 3.2. We then proceed to fne-tune the planner, caller, and summarizer on their respective datasets, thereby further enhancing their specifc abilities in each sub-task. During this local fne-tuning stage, we opt to reuse the set of user instructions curated in the global fne-tuning stage. The only adjustment made to the training set is the change in the format of the training data. As illustrated in Figure 3, the fnetuning objective during the second stage for the planner, caller, and summarizer is oriented towards generating the rationale, action, and fnal answer, respectively. While the gradients from other text spans are stopped. Simultaneously, we refne the system prompts for the training data of the planner, caller, and summarizer, as detailed in Appendix A.\"}\n{\"section_title\":\"4.1 Benchmarks\",\"section_content\":\"We evaluate the effectiveness of $\\\\alpha$ -UMi on the well recognized tool learning benchmark: ToolBench (Qin et al., 2023b). This benchmark involve API calls to accomplish tasks, where the agent must accurately select the appropriate API and compose necessary API requests. For additional details and statistics regarding these datasets, please refer to Appendix B. We also evaluate $\\\\alpha$ -UMi on other benchmarks such as ToolAlpaca (Tang et al., 2023) and program-aided agent for mathematical reasoning (Hendrycks et al., 2021; Cobbe et al., 2021). The results are shown in Appendix J.\"}\n{\"section_title\":\"4.4 Baselines\",\"section_content\":\"We compare our method with three baseline methods, namely Single-LLM, Multi-LLMone-stage and Single-LLMmulti-task. Single-LLM refers to the traditional single-LLM tool learning approach. Multi-LLMone-stage involves directly fne-tuning the planner, caller, and summarizer on their own sub-task datasets, without employing our two-stage fne-tuning strategy. Single- $\\\\mathrm{\\\\cdotLLM_{multi-task}}$ refers to using the same LLM to fulfll the roles of planner, caller, and summarizer. This particular LLM is fne-tuned on a combined dataset comprising the three sub-task datasets and functions similarly to our multi-LLM framework. We also evaluate the performance of GPT-3.5 and GPT-4 with 0-shot setting, and ToolLLaMA (Qin et al., 2023b), which is a 7B LLaMA model fne-tuned on ToolBench.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections mention the datasets but do not provide specific details about the training dataset and its format. I will perform a more targeted similarity search in the vectorstore with a more specific query to find the relevant context.\n[Action]:\nClassicRetrieve(query='In the GLPFT paper, which training dataset is larger and what is the format of the data in that dataset?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5967,\"pdf_id\":\"0b82b7cf-a4c9-5f42-b033-241d9be156ae\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"32bd58ed-9573-5062-be94-bbd715925f9c\",\"text\":\"and the size of training dataset.\\n17\"}\n{\"score\":0.5596,\"pdf_id\":\"af618d47-f6c3-5187-b3b8-1918b2eaecea\",\"page_number\":28,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f7f925d7-0b0f-537a-9fe2-8f9b4393c2af\",\"text\":\"and how they scale with dataset size.\\n1016\\n28\"}\n{\"score\":0.5369,\"pdf_id\":\"2d07ddaa-6d9a-531b-a864-3eb99f316831\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4421695d-7352-5209-b89e-2fd962b107aa\",\"text\":\"reflective of practitioner attention, and it varies significantly based on the popularity of the dataset.\\nHighly downloaded datasets tend to have more comprehensive and longer dataset cards (Fig. 3a), with\\nan emphasis on the Dataset Description and Dataset Structure sections (Fig. 3d). Conversely, less\\npopular datasets have shorter cards (Fig. 3a) with a greater emphasis on the Additional Information\\nsection (Fig. 3d). Despite this, sections such as Dataset Creation and Considerations for Using the\"}\n{\"score\":0.533,\"pdf_id\":\"a5723e6e-8664-5028-92ce-8e9d14ce702b\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b98a21d4-d304-5784-9582-00bc81440d2b\",\"text\":\"which is publicly available only for the training dataset.\\n15\"}\n{\"score\":0.5318,\"pdf_id\":\"10a5bb71-a26f-53ec-83aa-3e2d1b120c81\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a593f547-383a-5331-b550-0853f7a4423c\",\"text\":\"A\\nAppendix\\nA.1\\nDataset and Data Card\\nThe\\nSeeGULL\\ndataset\\nis\\nreleased\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull.\\nWe\\nrelease\\nthe\\ndata\\ncard\\nfor\\nthe\\nSeeGULL\\ndataset\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull\\/blob\\/main\\/seegull_datacard.pdf,\\nfollowing the format proposed by\\nPushkarna\\net al. (2022). The data card includes details of the\\ndataset such as intended usage, \\ufb01eld names and\\nmeanings, annotator recruitment and payments\\n(also in Appendix A.6 and A.9), and more.\\nA.2\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will try a more targeted SQL query by looking for the dataset name and format in the metadata table, but with a broader search term.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND section_content LIKE '%training dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"1 Introduction\",\"section_content\":\"Large Language Models (LLMs) have revolutionized natural language processing with remarkable profciency in understanding and generating text. Despite their impressive capabilities, LLMs are not without limitations. Notably, they lack domain specifcity, real-time information, and face challenges in solving specialized problems such as mathematics (Gou et al., 2023) and program compilation (OpenAI, 2023a). Hence, integrating LLMs with external tools, such as API calls and Python functions, becomes imperative to extend their capabilities and enhance the overall performance. Consequently, LLM agents have become a prominent area for both academia and industry, employing large language models to determine when and how to utilize external tools to tackle various tasks.\\n\\nIn addition to exploring proprietary LLMs like GPT-4, researchers have also actively engaged in developing customizable agent systems by fnetuning open-source LLMs on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Gou et al., 2023). The challenge of tool learning demands suffciently large and complex LLMs. These models must not only comprehend user queries but also excel in task planning, tool selection and invocation, and result summarization (Yujia et al., 2023). These capabilities draw upon different facets of the LLMs; for instance, planning relies more on reasoning ability, while tool selection and invocation demand legal and accurate request writing, and result summarization requires adept conclusion-drawing skills. While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Touvron et al., 2023a,b). Moreover, the tools could be updated frequently in practical scenarios, when the entire LLM requires potential retraining.\\n\\nTo address these challenges, we propose a multiLLM agent framework for tool learning, $\\\\alpha{-}\\\\mathrm{UMi}^{1}$ . As illustrated in Figure 1, $\\\\alpha$ -UMi decomposes the capabilities of a single LLM into three components, namely planner, caller, and summarizer. Each of these components is implemented by a single LLM and trained to focus on a specifc capability. The planner is designed to generate the rationale based on the current state of the system and weighs between selecting the caller or summarizer to generate downstream output, or even deciding to terminate the execution. The caller is directed by the rationale and responsible for invocating specifc tools. The summarizer is guided by the planner to craft the ultimate user answer based on the execution trajectory. These components collaborate seamlessly to accomplish various tasks. Compared to previous approaches, our modular framework has three distinct advantages. First, each component undergoes training for a designated role, ensuring enhanced performance for each capability. Second, the modular structure allows for individual updates to each component as required, ensuring adaptability and streamlined maintenance. Third, since each component focuses solely on a specifc capability, potentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework, we introduce a novel global-to-local progressive fne-tuning strategy (GLPFT). First, an LLM backbone is trained on the original training dataset without discriminating between sub-tasks, enhancing the comprehensive understanding of the toollearning task. Three copies of this LLM backbone are created to instantiate the planner, caller, and summarizer, respectively. In the second stage, the training dataset is reorganized into new datasets tailored to each LLM\\u2019s role in tool use, and continual fne-tuning of the planner, caller, and summarizer is performed on their respective datasets.\\n\\nWe employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our $\\\\alpha$ -UMi agent on several tool learning benchmarks (Qin et al., 2023b; Tang et al., 2023). Experimental results demonstrate that our proposed framework outperforms the single-LLM approach across various model and data sizes. Moreover, we show the necessity of the GLPFT strategy for the success of our framework and delve into the reasons behind the improved performance. Finally, the results confrm our assumption that smaller LLMs can be used in our multi-LLM framework to cultivate individual tool learning capabilities and attain a competitive overall performance.\\nTo sum up, this work makes three critical contributions. First, we demonstrate that small LLMs are weak tool learners and introduce $\\\\alpha$ -UMi, a multiLLM framework for building LLM agents, that outperforms the existing single-LLM approach in tool use. Second, we propose a GLPFT fne-tuning strategy, which has proven to be essential for the success of our framework. Third, we perform a thorough analysis, delving into data scaling laws and investigating the underlying reasons behind the superior performance of our framework.\"}\n{\"section_title\":\"3.2 The $\\\\alpha$ -UMi Framework\",\"section_content\":\"As previously mentioned, the task of tool learning imposes a signifcant demand on the capabilities of LLMs, including task planning, tool invocation, and result summarization. Coping with all these capabilities using a single open-source LLM, especially when opting for a smaller LLM, appears to be challenging. To address this challenge, we introduce the $\\\\alpha$ -UMi framework, which breaks down the tool learning task into three sub-tasks and assigns each sub-task to a dedicated LLM. Figure 1 presents an illustration of our framework, which incorporates three distinct LLM components: planner $\\\\mathcal{M}_{\\\\mathrm{plan}}$ , caller $\\\\mathcal{M}_{\\\\mathrm{call}}$ , and summarizer $\\\\mathcal{M}_{\\\\mathrm{sum}}$ . These components are differentiated by their roles in tool use, and each component model has a unique task defnition, system prompt2, and model input.\\nThe workfow of $\\\\alpha$ -UMi is shown in Figure 2. Upon receiving the user instruction $q$ , the planner generates a rationale comprising hints for the this step. This may trigger the caller to engage with the tools and subsequently receive observations from the tools. This iterative planner-caller-tool loop continues until the planner determines that it has gathered suffcient information to resolve the instruction. At this point, the planner transitions to the summarizer to generate the fnal answer. Alternatively, if the planner deems the instruction unsolvable, it may abandon the execution.\\nPlanner: The planner assumes responsibility for planning and decision-making, serving as the \\u201cbrain\\u201d of our agent framework. Specifcally, the model input for the planner comprises the system prompt $\\\\mathcal{P}_{\\\\mathrm{plan}}$ , the user instruction $q$ , and the previous execution trajectory $\\\\tau_{t-1}$ . Using this input, the planner generates the rationale $r_{t}$ :\\nFollowing the rationale, the planner generates the decision for the next step: (1) If the decision is \\u201cNext: Caller\\u201d, the caller will be activated and an action will be generated for calling tools. (2) If the decision is \\u201cNext: Summarizer\\u201d, the summarizer will be activated to generate the fnal answer for the user, and the agent execution will fnish. (3) If the decision is \\u201cNext: Give up\\u201d, it means that the user\\u2019s instruction cannot be solved in the current situation, and the system will be terminated.\\nCaller: Interacting with the tools requires the LLM to generate legal and useful requests, which may confict with other abilities such as reasoning and general response generation during fne-tuning. Therefore, we train a specialized caller to generate the action for using tools. The caller takes the user instruction $q$ and the previous execution trajectory $\\\\tau_{t-1}$ as input. To make the caller focus on the planner\\u2019s rationale $r_{t}$ in the current step, we also design a prompt $\\\\mathcal{P}_{\\\\mathrm{call}}$ to explicitly remind the caller:\\nSummarizer: The agent\\u2019s fnal response, which aims to offer informative and helpful information to the user, is distinct from the rationales that primarily focus on planning and reasoning. Therefore, we employ a dedicated summarizer tasked with generating the fnal answer $a_{n}$ . This model utilizes a concise prompt $\\\\mathcal{P}_{\\\\mathrm{sum}}$ designed to guide the model in concentrating on summarizing the execution trajectory and presenting the answer to the user:\\nIn Figure 7 and Figure 8, we show several cases of our $\\\\alpha$ -UMi on downstream tasks.\\nTo effectively fne-tune the above multi-LLM system is a complex endeavor: On one hand, generating the rationale, action, and fnal answer can facilitate each other during the training process, and enhance the model\\u2019s comprehension of the entire agent task (Chen et al., 2023). On the other hand, the constraints on model capacity make it challenging to fne-tune a small LLM to achieve peak performance in generating rationales, actions, and fnal answers simultaneously (Dong et al., 2023). Taking into account these two points, we propose a global-to-local progressive fne-tuning (GLPFT) strategy for $\\\\alpha$ -UMi. The motivation behind this strategy is to frst exploit the mechanism by which the generation of rationale, action, and fnal answer can mutually enhance each other. Then, once the single LLM reaches its performance ceiling, it is subsequently split into planner, caller and summarizer for further fne-tuning, in order to enhance its capabilities in the subtasks and mitigate the performance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy comprises two distinct stages. The frst stage involves global fne-tuning, where we fne-tune a backbone LLM on the original training dataset without distinguishing between sub-tasks. After this stage, the backbone LLM is trained to sequentially output the rationale, action, and answer as introduced in Section 3.1. Then, we create three copies of the backbone LLM, designated as the planner, caller, and summarizer, respectively.\\nThe second stage is local fne-tuning, where we reorganize the training dataset tailored to each LLM\\u2019s role, as introduced in Section 3.2. We then proceed to fne-tune the planner, caller, and summarizer on their respective datasets, thereby further enhancing their specifc abilities in each sub-task. During this local fne-tuning stage, we opt to reuse the set of user instructions curated in the global fne-tuning stage. The only adjustment made to the training set is the change in the format of the training data. As illustrated in Figure 3, the fnetuning objective during the second stage for the planner, caller, and summarizer is oriented towards generating the rationale, action, and fnal answer, respectively. While the gradients from other text spans are stopped. Simultaneously, we refne the system prompts for the training data of the planner, caller, and summarizer, as detailed in Appendix A.\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will perform a more targeted SQL query by looking for the dataset name and format in the chunks table, but with a broader search term.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%training dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"enhanced performance for each capability. Second,\\nthe modular structure allows for individual updates\\nto each component as required, ensuring adaptabil-\\nity and streamlined maintenance. Third, since each\\ncomponent focuses solely on a specific capability,\\npotentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework,\\nwe introduce a novel global-to-local progressive\\nfine-tuning strategy (GLPFT). First, an LLM back-\\nbone is trained on the original training dataset\"}\n{\"text_content\":\"without discriminating between sub-tasks, enhanc-\\ning the comprehensive understanding of the tool-\\nlearning task. Three copies of this LLM backbone\\nare created to instantiate the planner, caller, and\\nsummarizer, respectively. In the second stage, the\\ntraining dataset is reorganized into new datasets tai-\\nlored to each LLM\\u2019s role in tool use, and continual\\n1In astronomy, the name \\u201c\\u03b1-UMi\\u201d is an alias of the Polaris\\nStar (https:\\/\\/en.wikipedia.org\\/wiki\\/Polaris), which\"}\n{\"text_content\":\"mance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy\\ncomprises two distinct stages. The first stage in-\\nvolves global fine-tuning, where we fine-tune a\\nbackbone LLM on the original training dataset\\nwithout distinguishing between sub-tasks. After\\nthis stage, the backbone LLM is trained to sequen-\\ntially output the rationale, action, and answer as\\nintroduced in Section 3.1. Then, we create three\\ncopies of the backbone LLM, designated as the\"}\n{\"text_content\":\"planner, caller, and summarizer, respectively.\\nThe second stage is local fine-tuning, where\\nwe reorganize the training dataset tailored to each\\nLLM\\u2019s role, as introduced in Section 3.2. We then\\nproceed to fine-tune the planner, caller, and sum-\\nmarizer on their respective datasets, thereby further\\nenhancing their specific abilities in each sub-task.\\nDuring this local fine-tuning stage, we opt to reuse\\nthe set of user instructions curated in the global\\nfine-tuning stage. The only adjustment made to\"}\n\nIn total, 4 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will perform a more targeted similarity search in the vectorstore with a more specific query to find the relevant context.\n[Action]:\nClassicRetrieve(query='In the GLPFT paper, what is the name of the larger training dataset and what is the format of the data in that dataset?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5979,\"pdf_id\":\"0b82b7cf-a4c9-5f42-b033-241d9be156ae\",\"page_number\":17,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"32bd58ed-9573-5062-be94-bbd715925f9c\",\"text\":\"and the size of training dataset.\\n17\"}\n{\"score\":0.5876,\"pdf_id\":\"10a5bb71-a26f-53ec-83aa-3e2d1b120c81\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a593f547-383a-5331-b550-0853f7a4423c\",\"text\":\"A\\nAppendix\\nA.1\\nDataset and Data Card\\nThe\\nSeeGULL\\ndataset\\nis\\nreleased\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull.\\nWe\\nrelease\\nthe\\ndata\\ncard\\nfor\\nthe\\nSeeGULL\\ndataset\\nat:\\nhttps:\\/\\/github.com\\/google-research-\\ndatasets\\/seegull\\/blob\\/main\\/seegull_datacard.pdf,\\nfollowing the format proposed by\\nPushkarna\\net al. (2022). The data card includes details of the\\ndataset such as intended usage, \\ufb01eld names and\\nmeanings, annotator recruitment and payments\\n(also in Appendix A.6 and A.9), and more.\\nA.2\"}\n{\"score\":0.5774,\"pdf_id\":\"76706dd9-c2d1-55cc-b0dd-efdeb22b93a5\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"21b8370d-139c-5c29-805f-0dabbc3dd685\",\"text\":\"More Details about Datasets\\nIn this section, we provide more details about the datasets used in this work apart the description in\\nSection 5.\\n20\"}\n{\"score\":0.5674,\"pdf_id\":\"da53d89e-ae2a-5bfc-982d-850cbe710484\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"211a7cdb-c784-579f-a97d-022e4efa4388\",\"text\":\"2The details of the dataset construction are in Appendix\\nA.3.\\n4119\"}\n{\"score\":0.5655,\"pdf_id\":\"d5d6ec01-d7c9-50c3-be43-34aa6388568d\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b6ede27e-f3c5-5fe3-a01c-3fb82fcc9f71\",\"text\":\"dataset background, see \\u00a7F.1 in Avdeyev et al. [9].\\n8\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not provide the specific information about the training dataset and its format for the GLPFT paper. I will perform a more targeted SQL query by looking for the dataset name and format in the metadata table, but with a broader search term.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '8e85718a-1210-52bf-9aee-9b2bb2d0ae59' AND text_content LIKE '%original training dataset%' LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"enhanced performance for each capability. Second,\\nthe modular structure allows for individual updates\\nto each component as required, ensuring adaptabil-\\nity and streamlined maintenance. Third, since each\\ncomponent focuses solely on a specific capability,\\npotentially smaller LLMs can be employed.\\nTo effectively train this multi-LLM framework,\\nwe introduce a novel global-to-local progressive\\nfine-tuning strategy (GLPFT). First, an LLM back-\\nbone is trained on the original training dataset\"}\n{\"text_content\":\"mance constraints due to limited model capacity.\\nAs depicted in Figure 3, this GLPFT strategy\\ncomprises two distinct stages. The first stage in-\\nvolves global fine-tuning, where we fine-tune a\\nbackbone LLM on the original training dataset\\nwithout distinguishing between sub-tasks. After\\nthis stage, the backbone LLM is trained to sequen-\\ntially output the rationale, action, and answer as\\nintroduced in Section 3.1. Then, we create three\\ncopies of the backbone LLM, designated as the\"}\n\nIn total, 2 rows are displayed in JSON format."}
