{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: From which two subsets does the benchmark used as evaluation set for BLOOM and BLOOMZ models in the paper merged?(The paper is named \"An Empirical Study of In-context Learning in LLMs for Machine Translation\")\n[Answer Format]: Your answer should be a single python list, every element of the list is a string of the abbrievation name of the subset, e.g.[\"TAT-Conv\",\"TAT-Web\"].\n[Anchor PDF]: 'bffc816d-612c-5fea-83bb-1ac6b290480b'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to retrieve the relevant section or content from the paper with the specified paper_id. I will use the `metadata` table to find the paper and then look for the sections or pages that mention the evaluation set for BLOOM and BLOOMZ models.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_title, sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'bffc816d-612c-5fea-83bb-1ac6b290480b' AND (sections.section_title LIKE '%Evaluation%' OR sections.section_content LIKE '%BLOOM%' OR sections.section_content LIKE '%BLOOMZ%');\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"4 Experimental setup\",\"section_content\":\"This section describes the different pre-trained models, evaluation benchmarks, in-context prompts, and decoding hyperparameters used for our current study.\\nModels: We conduct our experimentation on offthe-shelf open-source pre-trained LLMs like Llama 2 (Touvron et al., 2023), BLOOM (Fan et al., 2022) and their corresponding instruction-tuned variants, ALMA (Xu et al., 2023) and BLOOMZ (Muennighoff et al., 2023) respectively. Firstly, models like Llama 2 (Touvron et al., 2023) are predominantly trained with English or European languages and the tokenizer has poor fertility on Indic languages. Further, Ahuja et al. (2023) show that Llama 2 70B (Touvron et al., 2023) exhibits poor in-context MT performance on Indian languages. Hence, we evaluate Llama 2 (Touvron et al., 2023) and ALMA (Xu et al., 2023) only on European languages and evaluate BLOOM (Fan et al., 2022) and BLOOMZ (Muennighoff et al., 2023) on the remaining three Indian languages. For consistency in our experiments, we add a task-specifc fnetuned baseline (BLOOM 7B FT) by fne-tuning the BLOOM 7B model on a subset of BPCC-seed data (Gala et al., 2023) using the same prompt structure as ALMA (Xu et al., 2023). The fne-tuning hyperparameters are outlined in Appendix A. Due to computational constraints, we restrict our experimentation to 7B models for a fair comparison across the base and instruction-tuned variants. We adopt a greedy decoding strategy without sampling to generate a maximum of 256 tokens with early stopping to ensure reproducibility in our results.\\n\\nLanguages: Broadly, our experimentation consists of 3 Indian languages like Bengali, Hindi, and Tamil, and 3 European languages like Czech, German, and Russian. However, we consider different sets of languages for certain experiments due to the nature of the experiment or the availability of the data. Table 4 in Appendix F provides the languages considered, and the test set for each experiment.\\nBenchmarks: FLORES-200 (Goyal et al., 2022; Costa-juss\\u00e0 et al., 2022) is a N-way parallel benchmark that covers 200 languages, including various low-resource languages. However, it is important to note that data from FLORES-200 (Goyal et al., 2022; Costa-juss\\u00e0 et al., 2022) has been consumed in the multitask fne-tuning xP3 mixture of BLOOMZ (Muennighoff et al., 2023), suggesting data contamination. Therefore, we do not consider FLORES-200 for the evaluation of Indian languages and instead, we use the newly released IN22-Gen (Gala et al., 2023). IN22-Gen is a 22- way parallel benchmark for Indian languages that focuses on demography-specifc content and serves as an unbiased evaluation set for both BLOOM and BLOOMZ models.\\nPrompt Details: In order to maintain consistency in terms of evaluation, we follow the same prompt that was used for fne-tuning ALMA ( $\\\\mathrm{Xu}$ et al., 2023) across all different models considered in this work (see Appendix C). However, we fnd Llama 2 Chat models do not perform well with the above standard prompt in the monolithic format, which is similar to Sclar et al. (2023). Therefore, we resort to passing each demonstration as a user-assistant turn and task description in the system prompt in the chat format. We use uniformly sampled highquality translation pairs from the FLORES-200 dev set to compose the in-context demonstrations with $k=\\\\{1,4,8\\\\}$ across various noise scales with $\\\\delta=\\\\{0,0.1,0.25,0.5,0.75\\\\}$ for evaluating LLMs.\\n\\nEvaluation: Following Sai B et al. (2023), we use the $\\\\mathrm{ChrF+}$ metric (Popovic\\u00b4, 2017), computed using SacreBLEU1 (Post, 2018), as the primary metric for our analysis due to its strongest correlation with human judgments among automatic metrics.\"}\n{\"section_title\":\"5.1 Sensitivity to Instruction Variations\",\"section_content\":\"Figure 1 illustrates that base LLMs such as BLOOM-7B and Llama 2 7B exhibit less variability to instruction perturbations averaged across different shots. Furthermore, Figure 12 provides detailed results depicting performance variation across different shots subjected to instruction perturbation. We observe that chat models like BLOOMZ-7B and Llama 2 7B Chat show some sensitivity to instructions, particularly in a\\n1-shot setting, but this sensitivity diminishes as the number of shots increases. On the other hand, task-specifc fne-tuned models like ALMA and BLOOM-7B-FT might be expected to be highly sensitive to instructions due to their training on data with specifc instruction formats. Surprisingly, these models remain largely unaffected by perturbed instructions, even in a 1-shot setting. Although contrastive or random instructions aim to induce off-target or irrelevant generations, we observe that all the models consistently perform the translation task in the appropriate language, albeit with slightly reduced quality, highlighting that incontext examples primarily infuence task recognition and learning.\"}\n{\"section_title\":\"5.2.1 Homogeneous Perturbations\",\"section_content\":\"Figure 2 indicates that perturbing in-context examples appear to adversely impact performance despite clear instructions. It is important to note that these instructions do not suffce as substitutes for noisy in-context examples. Generally, perturbations to the target distribution have a more severe impact than those to the source distribution, however, we observe instances where the impact of the source distribution cannot be disregarded. For instance, the BLOOMZ-7B model and ALMA-7B are susceptible to source-side perturbations, necessitating dedicated experimentation to ascertain the signifcance of the source distribution. We explore this aspect further in Section 5.2.3. Span noise emerges as the most detrimental perturbation across different model families, with the exception of BLOOM-7B-FT and Llama 2 7B models.\\nAcross various perturbation attacks, we observe that the task-specifc fne-tuned BLOOM-7B-FT model is most robust, followed by the BLOOM-7B model, while the multitask fne-tuned BLOOMZ7B model is highly vulnerable. Llama 2 model generally demonstrates strong robustness to different attacks, except for the task-specifc fne-tuned ALMA model, which is susceptible to span noise attack on the target side. Despite the relatively mild nature of word duplication attacks, the BLOOM7B models surprisingly show high susceptibility. BLOOMZ-7B is robust to all attacks in XX-En translation, except for the span noise attack. Surprisingly, the Llama 2 7B model shows signifcant performance improvements when subjected to various attacks compared to the clean baseline across both translation and perturbation directions.\\n\\nThe current experiments primarily focus on homogeneous perturbations, indicating a detrimental impact on performance when in-context examples are perturbed uniformly. To investigate the potential infuence of spatial proximity between perturbed in-context examples and the test examples, we delve into heterogeneous perturbations in Section 5.2.2. To limit the number of experiments due to computational constraints, we choose to exclude punctuation add and drop attacks, as the number of punctuations in a sentence is limited.\"}\n{\"section_title\":\"5.2.2 Heterogeneous Perturbations\",\"section_content\":\"We consider four cases categorized into two pairs based on the degree of noise perturbation: $\\\\{(k-$ $1\\\\mathrm{~c},1\\\\mathrm{~n}),(1\\\\mathrm{~n},k-1\\\\mathrm{~c})$ and $\\\\{(1\\\\mathrm{~n},k-1\\\\mathrm{~c})$ , $(k-$ $\\\\mathrm{1\\\\,n,1\\\\,c)}\\\\}$ where $c$ and $n$ indicates clean and noisy. We fnd that placing noisy examples closer to the test example has a more detrimental impact than placing them farther away (see Figure 3). Among these cases, $\\\\{(1\\\\,\\\\mathrm{c},k\\\\!-\\\\!1\\\\,\\\\mathrm{n})\\\\}$ is identifed as the most detrimental. Consistent with prior fndings, we note an improvement in Llama 2 7B performance when exposed to noise, with the most signifcant improvement occurring when the noisy examples are furthest from the test example (optimal conditions at $(k\\\\mathrm{~-~}1\\\\mathrm{~c},1\\\\mathrm{~n}))$ . Llama 2 7B Chat is fairly robust and demonstrates improvement upon perturbation, except for the most detrimental case. ALMA shows slight susceptibility across all variants, with the most severe impact observed in the most detrimental case. BLOOM-7B demonstrates robustness against source-side perturbations but is heavily affected by target-side perturbations, particularly when $k-1$ noisy examples are closer to the test example. BLOOMZ-7B is generally affected across all settings, with the most severe impact in the most detrimental case. Conversely, BLOOM7B-FT also demonstrates a fair amount of robustness. Our experimental results provide empirical evidence to practitioners that in settings involving a mixed pool of noisy and clean in-context examples, the optimal approach is to position noisy examples at the beginning and cleaner examples closer to the test example to maximize performance.\"}\n{\"section_title\":\"5.2.3 Demonstrations from Allied Tasks\",\"section_content\":\"Prior experiments (see Sections 5.1 and 5.2.1) indicate that examples are more infuential than instructions, and the target distribution of the example has broadly more importance than the source distribution. However, our focus has been limited to in-context examples of the same task during testing. To explore the limits of example-based learning, we aim to investigate if in-context examples from a different source language but the same target language suffce as a proxy for guiding the model in translating into the desired target language, thus probing the limits of example-based learning.\\nOur initial experiments, selecting an auxiliary language, showed that using in-context examples from a related task can generally serve as a suitable proxy for the target task, barring a few exceptions. To delve deeper, we considered 2 additional settings: using the same script as the test time source language and using English, the predominant language models were trained on, as the auxiliary source. Figure 4 shows that the source distribution of in-context examples has a marginal effect on downstream MT performance. This suggests that any auxiliary source language can generally guide the model adequately, although there are a few exceptions, such as Llama 2 7B where using the same script as the auxiliary language outperforms English, and BLOOMZ-7B where a randomly chosen auxiliary language surpasses even using the same language. This suggests the limited impact of the choice of source language (source distribution). We conclude that in the absence of demonstrations from the exact pair as the test examples, demonstrations from any alternative source language can serve as a viable substitute, yielding comparable performance to that of the same source language baseline.\"}\n{\"section_title\":\"5.3 Misalignment\",\"section_content\":\"In this experimental setup, we aim to understand if semantic priors in models can protect against susceptibility to misinformation presented in context, rendering them effective and robust in-context learners. It is evident that most models, except Llama 2 7B Chat, can form transitive associations to varying degrees (see Figure 5). Models from the BLOOM family and ALMA are particularly susceptible to extensive copy-pasting, suggesting they can be easily misled by in-context demonstrations, especially in cases of the target being misaligned. Notably, the ALMA model heavily relies on the English sentence as a pivot for these associations, and perturbing the English example also severely impacts performance despite the target translation being present in the context. Llama 2 7B exhibits poor translation quality, being unable to deduce answers from the context using transitive relations in aligned cases, and the performance further degrades in misaligned cases, indicating vulnerability to perturbations. In contrast, Llama 2 7B Chat remains largely unaffected by misinformationbased perturbations, showing robustness but poor downstream performance, indicating inadequacy in building transitive associations in the case of aligned examples. An ideal model should utilize its inherent biases to regulate its reliance on the information presented in context. Current 7B LLMs fall short in this regard, highlighting the necessity for future research to develop models that excel as in-context learners while minimizing susceptibility to misinformation within the context.\"}\n{\"section_title\":\"5.4 Directionality of ICL Demonstrations\",\"section_content\":\"Sections 5.2.1 and 5.2.3 shows the signifcance of target distribution; therefore, we further study if target-original demonstrations are superior to source-original demonstrations, depending on the translation direction. Focusing on Indic languages, we utilize an in-house multi-domain Indic Original set for sourcing Indic Original demonstrations and FLORES-200 dev set (Goyal et al., 2022; Costajuss\\u00e0 et al., 2022) for English original ones. IN22- Gen (Gala et al., 2023), which is English-original, serves as the test set. The Indic Original set2 serves as the target original set for En-XX direction and the source original set for XX-En, while FLORES200 dev set (Goyal et al., 2022; Costa-juss\\u00e0 et al., 2022) acts as the target original set for XX-En direction and source original set for En-XX direction. From Figure 6, across both En-XX and XX-En translation directions, we observe a minimal difference in the performance when demonstrations are chosen from the respective source original and target-original sets for all BLOOM family models considered. This suggests that the directionality of demonstrations may not have much signifcance on the downstream performance. However, it\\u2019s important to acknowledge that our fndings are limited to experiments conducted for Indic languages and using automatic metric-based evaluations, which may not suffciently capture whether using targetoriginal in-context examples has any impact on the fuency of the generation or not. Further investigation through human evaluation is necessary to ascertain this, an aspect we leave for future work.\"}\n{\"section_title\":\"A Fine-tuning BLOOM 7B on MT\",\"section_content\":\"We fne-tune BLOOM (Fan et al., 2022) 7B model on a human-annotated subset of BPCC (Gala et al., 2023), namely BPCC-seed and this forms the taskspecifc fne-tuned baseline for the BLOOM family. Table 1 and Table 2 list the data scales and hyperparameters used for fne-tuning respectively.\\nTable 1: Statistics of the sample of the English-centric BPCC-Seed data (Gala et al., 2023) used for fne-tuning the BLOOM 7B model. Note that the data was formatted in both directions (En-XX and XX-En) directions and used for joint fne-tuning.\\nTable 2: Hyperparameters for fne-tuning the BLOOM 7B model on MT task.\"}\n{\"section_title\":\"# Contrastive\",\"section_content\":\"Translate this from {tgt_lang} into {src_lang}:\\nTable 4: Details about the models, test sets, in-context sets and languages considered for different experiments. LLama2 family indicates Llama2-7B, Llama2-Chat-7B, ALMA while BLOOM family indicates BLOOM-7B, BLOOMZ-7B and a task-specifc fne-tuned BLOOM model on MT. En-X in the translation direction indicates English-centric evaluation, while X-Y indicates non-English-centric evaluation. FLORES200 in the test set column indicates the FLORES200 devtest set, while in the in-context set column indicates FLORES200 dev set.\"}\n\nIn total, 9 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: From the retrieved sections, I can see that the paper mentions the evaluation benchmarks used for BLOOM and BLOOMZ models. Specifically, it states that FLORES-200 is not used for Indian languages due to data contamination, and instead, IN22-Gen is used. I will extract the relevant subsets from this information.\n[Action]:\nGenerateAnswer(answer=['IN22-Gen', 'FLORES-200'])"}
{"role": "user", "content": "[Observation]: ['IN22-Gen', 'FLORES-200']"}
