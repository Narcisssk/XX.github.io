{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: In the largest dataset concerning procedural graph extraction before PAGED, how many labeled sentences are there in total?\n[Answer Format]: Your answer should be a single integer.\n[Anchor PDF]: '6b82aecf-fda7-531e-8094-fc5ab6f4810d'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: To answer the question, we need to find the information about the largest dataset concerning procedural graph extraction before PAGED. Since the question is specific to a certain paper, we can start by retrieving the content of the paper with the given `paper_id` from the `sections` table. We will look for sections that might contain information about datasets, such as the \"Experiments\" or \"Dataset\" sections.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '6b82aecf-fda7-531e-8094-fc5ab6f4810d' AND section_title IN ('Experiments', 'Dataset', 'Data', 'Experimental Setup', 'Experimental Results');\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the sections with the titles 'Experiments', 'Dataset', 'Data', 'Experimental Setup', or 'Experimental Results' might not exist in the paper. We should try a more general approach by searching the entire content of the paper for relevant information. We can use the `page_content` from the `pages` table to find the required information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT page_number, page_content FROM pages WHERE ref_paper_id = '6b82aecf-fda7-531e-8094-fc5ab6f4810d';\")"}
{"role": "user", "content": "[Observation]:\n{\"page_number\":1,\"page_content\":\"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10829\\u201310846\\nAugust 11-16, 2024 \\u00a92024 Association for Computational Linguistics\\nPAGED: A Benchmark for Procedural Graphs Extraction from Documents\\nWeihong Du12\\nWenrui Liao12\\nHongru Liang12*\\nWenqiang Lei12\\n1College of Computer Science, Sichuan University, China\\n2Engineering Research Center of Machine Learning and Industry Intelligence,\\nMinistry of Education, China\\n{duweihong, liaowenrui}@stu.scu.edu.cn\\n{lianghongru, wenqianglei}@scu.edu.cn\\nAbstract\\nAutomatic extraction of procedural graphs\\nfrom documents creates a low-cost way for\\nusers to easily understand a complex proce-\\ndure by skimming visual graphs. Despite the\\nprogress in recent studies, it remains unan-\\nswered: whether the existing studies have well\\nsolved this task (Q1) and whether the emerging\\nlarge language models (LLMs) can bring new\\nopportunities to this task (Q2). To this end,\\nwe propose a new benchmark PAGED, equipped\\nwith a large high-quality dataset and standard\\nevaluations. It investigates five state-of-the-art\\nbaselines, revealing that they fail to extract op-\\ntimal procedural graphs well because of their\\nheavy reliance on hand-written rules and lim-\\nited available data. We further involve three ad-\\nvanced LLMs in PAGED and enhance them with\\na novel self-refine strategy. The results point\\nout the advantages of LLMs in identifying tex-\\ntual elements and their gaps in building logical\\nstructures. We hope PAGED can serve as a ma-\\njor landmark for automatic procedural graph\\nextraction and the investigations in PAGED can\\nprovide valuable insights into the research on\\nlogical reasoning among non-sequential ele-\\nments. The code and dataset are available in\\nhttps:\\/\\/github.com\\/SCUNLP\\/PAGED.\\n1\\nIntroduction\\nProcedural graphs, though can intuitively represent\\nthe execution of actions for goal achievement (Mo-\\nmouchi, 1980; Ren et al., 2023), suffer from the\\nhigh cost of expert-construction (Herbst and Kara-\\ngiannis, 1999; Maqbool et al., 2019). The auto-\\nmatic extraction of procedural graphs from pro-\\ncedural documents thus has huge potential, as\\nit would enable users to easily understand how\\nto logically perform a goal (e.g, how a restau-\\nrant serves the customers) by skimming visual\\ngraphs (e.g., Figure 1(a) instead of reading lengthy\\ndocuments (e.g., Figure 1(b)).\\n*\\nCorresponding author\\nHowever, obtaining optimal procedural graphs\\nis not easy \\u2014 as shown in Figure 1(a), it requires\\nrepresenting not only sequential actions in the pro-\\ncedure (e.g., I-4 \\u2192I-5), but also non-sequentially\\nexecuted actions (e.g., I-7.1 & I-7.2]) and vital con-\\nstraints for the actions (e.g., C-2). Off-the-shelf\\nattempts only meet part of the requirements. For\\nexample, Bellan et al. (2023); Ren et al. (2023)\\nfail to represent \\u201ccustomers can order only the\\ndishes or drinks, or both\\u201d (I-2.1 & I-2.2) due to its\\ninherent limitation for representing complex non-\\nsequential actions. Besides, current solutions only\\nfocus on hand-written rules (Sholiq et al., 2022)\\nor customized networks (Bellan et al., 2023) on a\\nsmall group of cherry-picked instances. This brings\\nup the question of whether the existing studies have\\nwell solved the automated extraction of procedural\\ngraphs from procedural documents (Q1). If the\\nanswer is \\u201cno\\u201d, we are also interested in whether\\nthe emerging large language models (LLMs) can\\nbring new opportunities to this task (Q2).\\nTo answer Q1, we propose to construct a\\nstandard benchmark for the ProcedurAl Graphs\\nExtraction from Documents (PAGED). As far as\\nwe know, there lack of large-scale datasets of\\ndocument-graph pairs for training and evaluating\\noptimal procedural graph extraction models (cf.,\\nTable 1). We want to equip PAGED with the largest\\npublicly available dataset. Although there exist\\nplenty of procedural documents on the Internet, it\\nis too costly to filter the low-quality ones and anno-\\ntate optimal procedural graphs matching all require-\\nments. As a remedy, we build the dataset based on a\\nmodel collection of business process (Dumas et al.,\\n2018), which has summarized business processes\\ninto high-quality procedural graphs with complete\\nsequential actions, non-sequential actions, and con-\\nstraints. Thus, constructing procedural document-\\ngraph pairs turns into assigning a suitable proce-\\ndural document to a given procedural graph. We\\napproach it via a three-stage pipeline \\u2014 we pro-\\n10829\\n\"}\n{\"page_number\":2,\"page_content\":\"(a) Procedural Graph\\n(b) Procedural Document\\nFigure 1: The procedure of how a restaurant serves the customers in procedural graph (a) and document (b).\\ngressively transfer the structured information on\\nthe procedural graph into natural language text, ad-\\njust its narration, and improve the coherence and\\nnaturalness, finally generating a suitable document.\\nIn this way, we develop a dataset with 3,394 high-\\nquality procedural document-graph pairs that are\\nabout ten times larger than the previous largest\\ndatasets (Ackermann et al., 2021; Qian et al., 2020).\\nAccording to the underlying structure of optimal\\nprocedural graphs, we introduce three metrics to\\nevaluate five state-of-the-art methods (Sonbol et al.,\\n2023; Neuberger et al., 2023; Sholiq et al., 2022).\\nTo further answer Q2, we investigate the perfor-\\nmance of three advanced LLMs (Flan-T5 (Chung\\net al., 2022), ChatGPT (Ouyang et al., 2022) and\\nLlama2 (Touvron et al., 2023)) and utilize a self-\\nrefine strategy to improve the ability of LLMs. In\\ntotal, we evaluate ten methods in our PAGED bench-\\nmark. Extensive experiments on our benchmark\\nreveal that existing studies struggle to accurately\\nextract sequential actions, constraints, and organize\\nnon-sequential actions of procedural documents.\\nWhile LLMs have shown significant improvement\\nin sequential action and constraint extraction, they\\nstill face challenges with non-sequential action or-\\nganization. Our detailed analysis of the results\\nleads us to propose improvement strategies to\\nhelp large language models better understand non-\\nsequential actions and use correct gateways to rep-\\nresent them. We hope PAGED can be a key mile-\\nstone for automatic procedural graphs extraction,\\noffering insights into research on logical reasoning\\namong non-sequential elements. In summary, we\\nhighlight PAGED as follows:\\n\\u2022 We build a novel benchmark named PAGED, which\\nstandardly evaluates the progress of current pro-\\ncedural graphs extraction from documents and\\nexplores the potential of emerging LLMs.\\n\\u2022 We equip PAGED with the largest procedural\\ndocument-graph dataset, whose high quality is\\nachieved by a three-stage pipeline and verified\\nvia both automatic and human evaluation.\\n\\u2022 We systematically evaluate state-of-the-art solu-\\ntions in PAGED and reveal that they have trouble\\nextracting optimal procedural graphs due to their\\nheavy reliance on hand-written rules and limited\\navailable data.\\n\\u2022 We investigate advanced LLMs in PAGED and em-\\npower them with a self-refine strategy, showing\\ntheir adavantages in identifying sequential ac-\\ntions and constraints, and pointing out their gaps\\nin building complex logic of graphs.\\n2\\nRelated Work\\nProcedural Graph Extraction\\nExisting studies\\nonly meet part of the requirements for optimal pro-\\ncedural graph extraction. Earlier studies mainly\\n10830\\n\"}\n{\"page_number\":3,\"page_content\":\"Table 1: Comparisons between our dataset with the existing datasets.\\nNon-sequential Actions\\nConstraints\\nDataset\\nSamples\\nNum\\nSequential\\nActions\\nExclusive\\nGateway\\nInclusive\\nGateway\\nParallel\\nGateway\\nData\\nConstraint\\nAction\\nConstraint\\nPublicly\\nAvailable\\nFriedrich et al. (2011)\\n47\\nEpure et al. (2015)\\n34\\nFerreira et al. (2017)\\n56\\nMendling et al. (2019)\\n103\\nQuishpi et al. (2020)\\n121\\nPartial\\nQian et al. (2020)\\n360\\nAckermann et al. (2021)\\n358\\nL\\u00f3pez et al. (2021)\\n37\\nPartial\\nBellan et al. (2023)\\n45\\nLiang et al. (2023)\\n200\\nRen et al. (2023)\\n283\\nours\\n3,394\\nfocus on extracting sequential actions (Pal et al.,\\n2021; L\\u00f3pez et al., 2021; Ren et al., 2023). While\\nEpure et al. (2015); Honkisz et al. (2018); Bellan\\net al. (2023) explore non-sequential actions, they do\\nnot cover scenarios like I-2.1 & I-2.2 in Figure 1(a).\\nFriedrich et al. (2011) aids in data constraint extrac-\\ntion but overlooks action constraints. Besides, cur-\\nrent studies heavily rely on hand-written rules and\\ntemplates (Epure et al., 2015; Honkisz et al., 2018),\\nresulting in poor generalization. Bellan et al. (2023)\\ntrains a neural network model but only learns 45\\nsamples, whose effectiveness remains questionable.\\nHence, we propose to construct a standard bench-\\nmark to reveal the performance of existing studies\\nand highlight the challenges for the extraction of\\noptimal procedural graphs.\\nDatasets\\nAs shown in Table 1, current datasets\\nconsist of a small group of cherry-picked instances.\\nSome datasets are not publicly available (Epure\\net al., 2015; Ferreira et al., 2017). Some datasets\\nonly focus on sentence-level actions extraction,\\nlacking both sequential and non-sequential ac-\\ntions (Quishpi et al., 2020; Qian et al., 2020; Ack-\\nermann et al., 2021). While other studies contain\\nsequential actions Friedrich et al. (2011); Mendling\\net al. (2019); L\\u00f3pez et al. (2021); Bellan et al.\\n(2023); Liang et al. (2023); Ren et al. (2023), they\\ndo not cover all types of non-sequential actions.\\nMoreover, almost all existing datasets ignore vi-\\ntal constraints related to the actions in procedural\\ngraphs. To this end, we construct a new procedu-\\nral document-graph dataset that is nearly ten times\\nlarger than the previous largest datasets (Acker-\\nmann et al., 2021; Qian et al., 2020). Each sample\\nconsists of a high-quality procedural document and\\nits procedural graph with complete sequential ac-\\ntions, non-sequential actions, and constraints.\\nData2Text\\nData2Text task aims at transferring\\nstructured data such as graphs into natural language\\ntext (Duong et al., 2023; Lin et al., 2023). Cur-\\nrent studies (Su et al., 2021; Kasner and Du\\u0161ek,\\n2022) only focus on the transformation of factual\\nknowledge \\u2014 knowledge about features of things,\\nmaking it difficult to deal with procedural knowl-\\nedge \\u2014 execution of sequential and non-sequential\\nactions in the procedural graphs. Moreover, current\\nstudies can only manage discrete components (Ye\\net al., 2019; Fu et al., 2020), while extracting pro-\\ncedural graphs requires handling complex logic of\\nsequential action, non-sequential actions and their\\nconstraints. In this paper, we propose a three-stage\\npipeline to bridge the gap between complex graphs\\nand lengthy documents, ensure logical descriptions\\nof generated documents, and solve the issues of\\nfluency and coherence in generated documents.\\nLarge Language Models\\nThe emerging LLMs\\nhave presented competitive results in a wide range\\nof tasks (Zhao et al., 2023), but are barely used for\\nprocedural graph extraction. The only exception is\\nBellan et al. (2022), which makes a shallow attempt\\nto extract sequential actions and deal with partial\\nnon-sequential actions with LLMs, and performs\\npoorly for gateway extraction. It remains unan-\\nswered whether LLMs\\u2019 ability to understand the\\ninherent structure of long contexts can improve the\\nprocedural graphs extraction from documents. To\\nthis end, we involve Flan-T5 (Chung et al., 2022),\\nChatGPT (Ouyang et al., 2022) and Llama2 (Tou-\\nvron et al., 2023) in our PAGED and design a self-\\nrefine strategy to demonstrate the opportunities and\\ngaps of LLMs in this task. We hope this can help\\nto explore more possibilities that LLMs bring to\\nthis field.\\n10831\\n\"}\n{\"page_number\":4,\"page_content\":\"3\\nDataset\\nIt is too costly to conduct an expert annotation\\nof optimal procedural graphs for a large number\\nof documents. To this end, we build our dataset\\nupon a model collection of business process (Du-\\nmas et al., 2018), which has defined optimal proce-\\ndural graphs covering the whole business process\\nmanagement lifecycle. The dataset construction\\nturns into a data2text task \\u2014 generates suitable\\ndocuments for given procedural graphs.\\n3.1\\nPreliminary\\nFigure 1(a) presents an example of the optimal\\nprocedural graph. It describes the procedure of\\nhow a restaurant serves customers and involves\\ntwo actors (customer and restaurant). Each actor\\nstarts from the \\u201cStart\\u201d node, carries out actions\\nfollowing the logic in the graph, and ends at the\\n\\u201cEnd\\u201d node. If the actions are performed sequen-\\ntially, they are connected by the \\u201cSequence Flow\\u201d.\\nOtherwise, there is an \\u201cInclusive Gateway\\u201d, \\u201cEx-\\nclusive Gateway\\u201d or \\u201cParallel Gateway\\u201d indicat-\\ning the following actions are non-sequential ones.\\nBoth the \\u201cInclusive Gateway\\u201d (G-1) and \\u201cExclu-\\nsive Gateway\\u201d (G-3) mean that the following action\\nis performed under the condition on the connected\\n\\u201cCondition Flow\\u201d. The difference is that there is\\none and only one condition after the \\u201cExclusive\\nGateway\\u201d can be met, while this does not apply\\nto the \\u201cInclusive Gateway\\u201d. The \\u201cParallel Gate-\\nway\\u201d (G-5) represents that the following actions\\nare performed in parallel. Note that, all gateways\\nappear in pairs and the latter ones (G-4, G-2, G-\\n6) indicate the end of the non-sequential activities.\\nAdditionally, the \\u201cData Constraint\\u201d and \\u201cAction\\nConstraint\\u201d represent the necessary data (C-1) and\\nessential notices (C-2) for actions connected by the\\n\\u201cConstraint Flow\\u201d, respectively.\\n3.2\\nDataset Construction\\nWith these high-quality procedural graphs, we then\\nperform the dataset construction as a data2text\\ntask (Lin et al., 2023). Specifically, we design a\\nthree-stage pipeline: 1) Decomposition & Transfor-\\nmation that decomposes the graph into fragmented\\nspans\\/sentences in natural language; 2) Grouping\\n& Ordering that logically organizes the procedu-\\nral fragments; 3) Aggregating & Smoothing that\\nunifies the fragments into high-quality documents.\\nFigure 2: Illustration of decomposing the graph into\\nunits and transforming a unit into a procedural fragment.\\nFigure 3: The grouping of procedural fragments using a\\npre-trained boundary identification model.\\nDecomposition & Transformation\\nTo narrow\\nthe huge gap between the complex graph and the\\nlength document, we first decompose the graph\\ninto minimal meaningful units. We define a vo-\\ncabulary with nineteen units, each of which con-\\nsists of actions, gateways or constraints connected\\nby flow (cf., Appx. A). We also design nineteen\\nhand-written templates for the vocabulary. Based\\non the unit vocabulary, we perform the breadth-\\nfirst search strategy (Korf, 1985) over the graph,\\nas shown in Figure 2. Particularly, some actions\\nmay be repeatedly walked to preserve the sequen-\\ntial execution relation between adjacent actions in\\nthe graph. For example, \\u201cOrder drinks\\u201d is walked\\ntwice, forming \\u201cIf need drinks, then order drinks.\\u201d\\nand \\u201cOrder drinks, then specify the size.\\u201d. We\\nthen transform the decomposed units into natural\\nlanguage spans\\/sentences based on the paired tem-\\nplates. We call these spans\\/sentences \\u201cprocedural\\nfragments\\u201d.\\n10832\\n\"}\n{\"page_number\":5,\"page_content\":\"Grouping & Ordering\\nWhen writing a procedu-\\nral document, it is vital to provide information in\\na logical way, namely, describing the whole pro-\\ncedure sub-procedure by sub-procedure (Futrelle,\\n1999, 2004). Hence, we need to group the frag-\\nments into sub-procedures. Specifically, we employ\\na simple boundary identification model, which em-\\nploys the RoBERTa-large model (Liu et al., 2019)\\nwith a classifier layer, to predict whether a fragment\\nis the end of a group. We train it on the WikiHow\\ncorpus Bolotova et al. (2023), which consists of\\nprocedural documents collected from the WikiHow\\nwebsite and format marks indicate different sub-\\nprocedures. The pre-trained model is then used\\nto assign group marks for fragments, cf., Figure 3.\\nIt is worth noticing that the way people describe\\na procedure is not the way the machine searches\\nover the graph. Thus, it is necessary to order the\\nfragments to better match human expression. For\\nexample, we need to exchange fragment \\u2782and\\nfragment\\u2783of Group 1 in Figure 3. We achieve this\\nvia a fragment ordering model, whose structure\\nis borrowed from Bin et al. (2023). Besides the\\nWikiHow documents, we train this model on the\\nremaining publicly available procedural document\\ndatasets (Castelli et al., 2020; Zhang et al., 2020;\\nLyu et al., 2021; Sakaguchi et al., 2021; Nandy\\net al., 2021) to reorder sentences after random shuf-\\nfle. We use the pre-trained model to determine the\\norder of fragments in the same group.\\nAggregating & Smoothing\\nIn a standard proce-\\ndural document, a single sentence (\\u201cIf the customer\\nneeds dishes, then choose the desired dishes and\\nspecify the taste.\\u201d) may convey multiple procedu-\\nral fragments (\\u2781and \\u2783). Given this, we reuse the\\nboundary identification model to aggregate frag-\\nments that should be presented in the same sen-\\ntence. The model is retrained to identify the correct\\nends of sentences from randomly inserted ones on\\nthe datasets used in the ordering phase. The pre-\\ntrained model is used to assign sentence marks for\\nthe fragments. We further add the actor informa-\\ntion before all fragments corresponding to each\\nactor. At this point, all fragments have been or-\\nganized in proper order with group and sentence\\nmarks. We paraphrase the fragments via ChatGPT,\\nwhich has shown near or even superior human-\\nlevel performance in many paraphrasing task (Chui,\\n2023). After paraphrasing, we notice that there are\\na small number of redundant expressions caused\\nby repeatedly walked nodes and inconsistent ac-\\ntions\\/constraints generated by ChatGPT. Thus, we\\nfurther refine the documents with a few handwrit-\\ning rules and manual corrections. At last, we de-\\nvelop a dataset with 3,394 high-quality document-\\ngraph pairs. On average, each document contains\\n10.67 sentences and each sentence contains 15.22\\nwords. See Table 4 in Appx. A for more statistics.\\n3.3\\nDataset Analysis\\nWe analyze our datasets to investigate whether\\nthe generated procedural documents are consistent\\nwith the original graphs, whether the documents\\nare qualified according to human standards, and\\nwhether the proposed strategies contribute to a bet-\\nter quality of the dataset. Therefore, we conduct\\nboth automatic and human evaluations to compare\\nthe dataset constructed by our three-stage pipeline\\nwith the datasets constructed by two variations.\\nAutomatic Evaluation\\nWe adopt two commonly\\nused Data2Text metrics. The FINE score (Faille\\net al., 2021) models the evaluation as a natural\\nlanguage inference task \\u2014 by inferring fragments\\nfrom the documents it checks omissions, while the\\nother direction checks hallucinations. The ESA\\nscore (Faille et al., 2021) evaluates the coverage\\nof entities and actions in the documents. For both,\\nhigher values indicate better performance. Details\\nare listed in the appendix B.\\nHuman Evaluation\\nInspired by Miller (1979),\\nwe ask three workers to score the document from\\n1 to 5 on five criteria: readability, the quality of\\nthe document to be understood easily; accuracy,\\nthe quality of the document accurately describing\\nthe information in the procedure graph; clarity, the\\nquality of the document expressing the complex ex-\\necution of actions logically; simplicity, the quality\\nof the document not containing redundant informa-\\ntion; usability, the quality of the document aiding\\nusers in accomplishing this procedure. Details are\\nlisted in the appendix B.\\nVariations\\nWe create two variants of our three-\\nstage pipeline: concatenating, which directly con-\\ncatenates all fragments to form the documents;\\nparaphrasing, which directly paraphrases the con-\\ncatenation of all fragments using ChatGPT without\\ngrouping, ordering and aggregating process. Note\\nthat, we don\\u2019t involve concatenating in the auto-\\nmatic evaluation. This is because the concatena-\\ntion of all fragments is always consistent with the\\n10833\\n\"}\n\n... # only display 5 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved page content does not directly provide the number of labeled sentences in the largest dataset before PAGED. However, the content mentions that the PAGED dataset is about ten times larger than the previous largest datasets. We can use this information to find the size of the largest dataset before PAGED. We will search for the size of the previous largest datasets in the content.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT page_number, page_content FROM pages WHERE ref_paper_id = '6b82aecf-fda7-531e-8094-fc5ab6f4810d' AND page_content LIKE '%previous largest datasets%';\")"}
{"role": "user", "content": "[Observation]:\n{\"page_number\":3,\"page_content\":\"Table 1: Comparisons between our dataset with the existing datasets.\\nNon-sequential Actions\\nConstraints\\nDataset\\nSamples\\nNum\\nSequential\\nActions\\nExclusive\\nGateway\\nInclusive\\nGateway\\nParallel\\nGateway\\nData\\nConstraint\\nAction\\nConstraint\\nPublicly\\nAvailable\\nFriedrich et al. (2011)\\n47\\nEpure et al. (2015)\\n34\\nFerreira et al. (2017)\\n56\\nMendling et al. (2019)\\n103\\nQuishpi et al. (2020)\\n121\\nPartial\\nQian et al. (2020)\\n360\\nAckermann et al. (2021)\\n358\\nL\\u00f3pez et al. (2021)\\n37\\nPartial\\nBellan et al. (2023)\\n45\\nLiang et al. (2023)\\n200\\nRen et al. (2023)\\n283\\nours\\n3,394\\nfocus on extracting sequential actions (Pal et al.,\\n2021; L\\u00f3pez et al., 2021; Ren et al., 2023). While\\nEpure et al. (2015); Honkisz et al. (2018); Bellan\\net al. (2023) explore non-sequential actions, they do\\nnot cover scenarios like I-2.1 & I-2.2 in Figure 1(a).\\nFriedrich et al. (2011) aids in data constraint extrac-\\ntion but overlooks action constraints. Besides, cur-\\nrent studies heavily rely on hand-written rules and\\ntemplates (Epure et al., 2015; Honkisz et al., 2018),\\nresulting in poor generalization. Bellan et al. (2023)\\ntrains a neural network model but only learns 45\\nsamples, whose effectiveness remains questionable.\\nHence, we propose to construct a standard bench-\\nmark to reveal the performance of existing studies\\nand highlight the challenges for the extraction of\\noptimal procedural graphs.\\nDatasets\\nAs shown in Table 1, current datasets\\nconsist of a small group of cherry-picked instances.\\nSome datasets are not publicly available (Epure\\net al., 2015; Ferreira et al., 2017). Some datasets\\nonly focus on sentence-level actions extraction,\\nlacking both sequential and non-sequential ac-\\ntions (Quishpi et al., 2020; Qian et al., 2020; Ack-\\nermann et al., 2021). While other studies contain\\nsequential actions Friedrich et al. (2011); Mendling\\net al. (2019); L\\u00f3pez et al. (2021); Bellan et al.\\n(2023); Liang et al. (2023); Ren et al. (2023), they\\ndo not cover all types of non-sequential actions.\\nMoreover, almost all existing datasets ignore vi-\\ntal constraints related to the actions in procedural\\ngraphs. To this end, we construct a new procedu-\\nral document-graph dataset that is nearly ten times\\nlarger than the previous largest datasets (Acker-\\nmann et al., 2021; Qian et al., 2020). Each sample\\nconsists of a high-quality procedural document and\\nits procedural graph with complete sequential ac-\\ntions, non-sequential actions, and constraints.\\nData2Text\\nData2Text task aims at transferring\\nstructured data such as graphs into natural language\\ntext (Duong et al., 2023; Lin et al., 2023). Cur-\\nrent studies (Su et al., 2021; Kasner and Du\\u0161ek,\\n2022) only focus on the transformation of factual\\nknowledge \\u2014 knowledge about features of things,\\nmaking it difficult to deal with procedural knowl-\\nedge \\u2014 execution of sequential and non-sequential\\nactions in the procedural graphs. Moreover, current\\nstudies can only manage discrete components (Ye\\net al., 2019; Fu et al., 2020), while extracting pro-\\ncedural graphs requires handling complex logic of\\nsequential action, non-sequential actions and their\\nconstraints. In this paper, we propose a three-stage\\npipeline to bridge the gap between complex graphs\\nand lengthy documents, ensure logical descriptions\\nof generated documents, and solve the issues of\\nfluency and coherence in generated documents.\\nLarge Language Models\\nThe emerging LLMs\\nhave presented competitive results in a wide range\\nof tasks (Zhao et al., 2023), but are barely used for\\nprocedural graph extraction. The only exception is\\nBellan et al. (2022), which makes a shallow attempt\\nto extract sequential actions and deal with partial\\nnon-sequential actions with LLMs, and performs\\npoorly for gateway extraction. It remains unan-\\nswered whether LLMs\\u2019 ability to understand the\\ninherent structure of long contexts can improve the\\nprocedural graphs extraction from documents. To\\nthis end, we involve Flan-T5 (Chung et al., 2022),\\nChatGPT (Ouyang et al., 2022) and Llama2 (Tou-\\nvron et al., 2023) in our PAGED and design a self-\\nrefine strategy to demonstrate the opportunities and\\ngaps of LLMs in this task. We hope this can help\\nto explore more possibilities that LLMs bring to\\nthis field.\\n10831\\n\"}\n{\"page_number\":13,\"page_content\":\"A\\nDetails of Data Construction\\nDecomposition & Transformation\\nTo narrow\\nthe huge gap between the complex graph and the\\nlength document and meanwhile maintain the con-\\nsistency between the transferred fragments and the\\noriginal graphs, such as the texts of the entities,\\nactions, etc., we design hand-written templates to\\ntransfer the graphs into natural language fragments.\\nThe designed templates are listed in Table 3. Ad-\\nditionally, we find that not all units on the original\\ngraphs are meaningful and need to be explicitly ex-\\npressed in generated procedural documents. So we\\nfilter out those meaningless units through heuristic\\nrules.\\nAggregating & Smoothing\\nWe conduct rephras-\\ning operation on the concatenation of the processed\\nfragments using LLM. We add the unique separa-\\ntor token \\u201c<SEP>\\u201d to indicate the group and sen-\\ntence marks of the fragments, which can prompt\\nthe model to describe these fragments logically and\\ncoherently. Moreover, we add the actor informa-\\ntion before all fragments corresponding to each\\nactor. For example, we add the text \\u201cFor the cus-\\ntomer:\\u201d before all fragments corresponding to the\\ncustomer. We adopt ChatGPT (gpt-3.5-turbo) to\\nconduct the rephrasing operation. The designed\\nprompt is shown in Figure 9. At last, we develop\\na dataset with 3,394 high-quality document-graph\\npairs. Additionally, although it is difficult to ex-\\nactly define the large of the dataset, we argue our\\ndataset is large enough because it is about ten times\\nlarger than the previous largest datasets and suc-\\ncessfully supports the evaluation of current studies\\nand the discovery of future directions in this field.\\nWe present the statistics information of the con-\\nstructed dataset in Table 4.\\nB\\nDetails of Dataset Quality Evaluation\\nAutomatic Evaluation\\nFollowing the evaluation\\nstrategies commonly used in Data2Text task (Lin\\net al., 2023), we conduct automatic evaluation to\\nevaluate whether the generated procedural docu-\\nments provide information consistent with the orig-\\ninal graphs. The automatic metrics used for evalua-\\ntion are listed as follows:\\nFINE (Du\\u0161ek and Kasner, 2020): evaluating the\\nsemantic equivalence of generated documents with\\na natural language inference model 3. The natural\\n3https:\\/\\/huggingface.co\\/FacebookAI\\/\\nlanguage inference model can be used to determine\\nwhether a \\u201chypothesis\\u201d is true (entailment) given\\na \\u201cpremise\\u201d. We first compute the entailment be-\\ntween the generated document and the transferred\\nfragments to evaluate whether the generated doc-\\nument fully covers the original graph\\u2019s informa-\\ntion (omission). We take the generated document\\nas the \\u201cpremise\\u201d and use the natural language in-\\nference model to determine whether the transferred\\nfragments are true (entailment), i.e., whether the se-\\nmantics of the transferred fragments are covered by\\nthe generated document. Then we exchange their\\npositions to evaluate whether the generated docu-\\nment not contains redundant information beyond\\nthe graph (hallucination).\\nESA (Faille et al., 2021): evaluating the faith-\\nfulness of the generated documents based on the\\ncoverage of the entities and actions in the original\\ngraphs. We use named entity recognition tool 4 to\\nextract the entities in the original graphs. Then we\\nevaluate whether the entities and actions in the orig-\\ninal graph are covered by the generated document\\nwith exact lexical match.\\nHuman Evaluation\\nWe design five criteria to\\nevaluate the generated documents through human\\nscoring. To ensure the reliability of human evalua-\\ntion, we hire three domain experts to evaluate the\\ngenerated documents and calculate the ICC (Intr-\\naclass Correlation Coefficient) (Shrout and Fleiss,\\n1979) score between different experts. The higher\\nICC score indicates higher consistency between\\ndifferent experts and higher reliability of the evalu-\\nation. Generally an ICC of 0.75 or higher indicates\\nthat the evaluation is reliable (Koo and Li, 2016).\\nICC is calculated as follows:\\nICC =\\nMSbetween \\u2212MSwithin\\nMSbetween + (k \\u22121) \\u00d7 MSwithin\\n(1)\\nwhere MSbetween is the mean square for between\\ngroups variability, MSwithin is the mean square for\\nwithin groups variability and k is the number of\\ngroups. And we ensure that the ICC scores for all\\nof our evaluations are \\u22650.75.\\nroberta-large-mnli\\n4https:\\/\\/huggingface.co\\/dslim\\/bert-base-NER\\n10841\\n\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The page content on page 3 and page 13 provides the necessary information to determine the size of the largest dataset before PAGED. According to the content, the PAGED dataset is about ten times larger than the previous largest datasets. We can find the size of the previous largest datasets in Table 1 on page 3. The largest dataset before PAGED is 358 samples from Ackermann et al. (2021). Since PAGED is about ten times larger, we can calculate the number of labeled sentences in the largest dataset before PAGED by dividing the size of PAGED by 10.\n[Action]:\nGenerateAnswer(answer=358)"}
{"role": "user", "content": "[Observation]: 358"}
