{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: In the GenRec paper, which dataset used in the experiment is not evaluated in Table 1? Additionally, what's the range of the clip length for that dataset?\n[Answer Format]: Your answer should be a Python list of 2 elements, the first is a string, the name of the dataset, and the second is a Python list of 2 floats, the range of the clip length, rounded to 2 decimal places, in seconds, e.g. [\"dataset\", [1.01, 2.02]]\n[Anchor PDF]: '2abcf6e2-b4e3-5cf9-8c80-4497539805cc'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to find the datasets mentioned in the GenRec paper and check which one is not evaluated in Table 1. I will start by retrieving the content of the paper with the given paper ID.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT page_content FROM pages WHERE ref_paper_id = '2abcf6e2-b4e3-5cf9-8c80-4497539805cc' ORDER BY page_number ASC;\")"}
{"role": "user", "content": "[Observation]:\n{\"page_content\":\"GenRec: Unifying Video Generation and Recognition\\nwith Diffusion Models\\nZejia Weng1,2, Xitong Yang3, Zhen Xing1,2, Zuxuan Wu1,2\\u2020 , Yu-Gang Jiang1,2\\n1 Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\\n2 Shanghai Collaborative Innovation Center of Intelligent Visual Computing\\n3 Department of Computer Science, University of Maryland\\nAbstract\\nVideo diffusion models are able to generate high-quality videos by learning strong\\nspatial-temporal priors on large-scale datasets. In this paper, we aim to investi-\\ngate whether such priors derived from a generative process are suitable for video\\nrecognition, and eventually joint optimization of generation and recognition. Build-\\ning upon Stable Video Diffusion, we introduce GenRec, the first unified frame-\\nwork trained with a random-frame conditioning process so as to learn generalized\\nspatial-temporal representations. The resulting framework can naturally supports\\ngeneration and recognition, and more importantly is robust even when visual in-\\nputs contain limited information. Extensive experiments demonstrate the efficacy\\nof GenRec for both recognition and generation. In particular, GenRec achieves\\ncompetitive recognition performance, offering 75.8% and 87.2% accuracy on\\nSSV2 and K400, respectively. GenRec also performs the best on class-conditioned\\nimage-to-video generation, achieving 46.5 and 49.3 FVD scores on SSV2 and\\nEK-100 datasets. Furthermore, GenRec demonstrates extraordinary robustness\\nin scenarios that only limited frames can be observed. Code will be available at\\nhttps:\\/\\/github.com\\/wengzejia1\\/GenRec.\\n1\\nIntroduction\\nDiffusion models have achieved significant success in the field of image and video generation over\\nthe past few years. A variety of generative tasks have been revolutionized by using diffusion models\\ntrained on Internet-scale data, such as text-to-image generation [33, 30], image editing [23], and more\\nrecently, text-to-video generation [15, 2, 52] and text&image-to-video generation [56, 18, 21]. The\\nexcellent generative capabilities of diffusion models suggest that informative representation is learned\\nduring the generative training and strong visual priors are captured by the backbone models [9,\\n43, 6]. Therefore, recent work has explored leveraging the image diffusion models for image\\nunderstanding tasks, including image recognition [9, 8], object detection [7, 57], segmentation [55]\\nand correspondence mining [39]. However, the capability of video diffusion models to effectively\\ncapture spatial-temporal information is not fully understood, and their potential for downstream video\\nunderstanding tasks remains under-explored.\\nIn this paper, we study the potential of video diffusion models [28, 1, 27], particularly the uncondi-\\ntioned or image-conditioned models, for video understanding by addressing the three key problems:\\n(a) Does the backbone model trained for video generation extract effective spatial-temporal represen-\\ntations for semantic video recognition? (b) Can we retain the video generation capability by jointly\\noptimizing generation and recognition? (c) Will such a unified training framework further benefit\\nvideo understanding, especially in noisy scenarios where only limited frames are available [3, 25].\\n\\u2020Corresponding author.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\n\"}\n{\"page_content\":\"Generator\\nClassifier\\nVideo \\ud835\\udc7d\\nVideo \\ud835\\udc7d\\nClassification\\nLoss\\n\\ud835\\udc7d\\ud835\\udf48\\nGeneration\\nLoss\\nFull Videos\\nNoisy Videos\\n\\ud835\\udc75(\\u22c5, \\ud835\\udf48)\\nGenRec\\nVideo \\ud835\\udc7d\\nGeneration\\nLoss\\nMasked Videos\\n\\ud835\\udc7d\\ud835\\udc74\\nClassification\\nLoss\\n\\ud835\\udc74(\\u22c5)\\n\\ud835\\udc7d\\ud835\\udf48\\nNoisy Videos\\n\\ud835\\udc75(\\u22c5, \\ud835\\udf48)\\n(a) Classification\\n(b) Diffusion Generation\\n(c) GenRec\\nFigure 1: Comparison of classical pipelines for video classification and generation tasks with our\\nproposed GenRec method. (a) Classification: Typical video classification focus on understanding\\ncomplete videos. (b) Diffusion Generation: Diffusion models learn the noise reduction trajectory\\nfrom videos with varying levels of noise. These two distinct training paradigms present challenges\\nfor task unification. To bridge this gap, we propose (c) GenRec: a learning framework that processes\\nmask frames VM using a masking function M(\\u00b7) and noise videos V\\u03c3 with noise sampling N(\\u00b7, \\u03c3),\\naiming to simultaneously learn video understanding and content completion with the same partially\\nobserved visual content.\\nWhile conceptually appealing, unifying video generation and recognition into a diffusion framework\\nis non-trivial. Prior work either views the diffusion models as frozen feature extractors [9, 55, 39],\\nor deconstructs them for new tasks while sacrificing their original generation capability [8]. One\\nmajor challenge comes from their distinct training and inference processes. Diffusion models are\\ntypically optimized using corrupted inputs, optionally augmented with a single conditioning frame, to\\nachieve unconditioned or image-conditioned generation during inference [27, 1]. In contrast, video\\nrecognition models require access to multiple frames to reason about temporal relationships and\\nexpect clean inputs during inference [49, 51, 48]. Consequently, training a recognition model using\\ncorrupted videos and single-image conditions tends to suffer from inferior model optimization and a\\nmore significant training-inference gap.\\nTo this end, we propose GenRec, a unified video diffusion model that enables joint optimization for\\nvideo generation and recognition. Our model is built upon the open-source, image-conditioned Stable\\nVideo Diffusion model (SVD) [1], which encodes strong spatial-temporal priors by pretraining on\\nlarge-scale image and video data. However, instead of conditioning on the same image across all\\nvideo frames, we propose to condition on a random subset of frames while masking the remaining\\nones (see Figure 2). This simple random-frame conditioning process effectively bridges the gap\\nbetween the learning processes of the two tasks. On the one hand, the generation capability of SVD\\nis extended to handle arbitrary frame prediction, which provides more flexible and unambiguous\\nvideo generation. On the other hand, conditioning on a random subset of frames allows the model to\\nlearn more discriminative and robust features for the recognition task. As shown in Figure 1, the\\nmodel is jointly optimized using both generative supervision (i.e., noise prediction) and classification\\nsupervision.\\nWe conduct extensive experiments to evaluate the performance of GenRec for both recognition and\\ngeneration. Without sacrificing the generation capabilities, GenRec demonstrates competitive video\\nrecognition performance, offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively.\\nFurthermore, GenRec demonstrates extraordinary robustness in scenarios that only limited frames\\ncan be observed. For example, when only the front half of the video can be observed, GenRec\\nachieves the 57.7% accuracy, which corresponds to 76.6% of the accuracy (75.3%) when the entire\\nvideo is visible, emonstrating a higher accuracy retention ratio than other methods. By leveraging\\nthe recognition model for classifier guidance [11], GenRec also achieves superior class-conditioned\\nimage-to-video generation results, with FVD scores of 46.5 and 49.3 on the SSV2 and EK-100\\ndatasets, respectively.\\n2\\n\"}\n{\"page_content\":\"2\\nPreliminary\\nRepresenting the data distribution as pdata(z) with a standard deviation of \\u03c3data, we can obtain a family\\nof smoothed distributions p(z; \\u03c3) by adding independent and identically distributed Gaussian noise\\nwith standard deviation \\u03c3. In the spirit of diffusion models, the generation process begins with a\\nnoise image zN \\u223cN(0, \\u03c32\\nmaxI) and iteratively denoises it at decreasing noise levels \\u03c3N = \\u03c3max >\\n\\u03c3N\\u22121 > . . . > \\u03c30 = 0. The final denoised result z0 is thus distributed according to the original data.\\nIn the EDM [22] framework, the original z0 will be diffused as:\\nz\\u03c3 = z0 + \\u03c3 \\u00b7 N(0, I),\\n(1)\\nand the corresponding PF-ODE [34] follows:\\ndz\\u03c3 = \\u2212\\u03c3 \\u00b7 \\u2207z log p\\u03c3(z\\u03c3)d\\u03c3,\\n(2)\\nwhere \\u2207z log pt(zt) is the score function. Noise schedule \\u03c3(t) is set as time step t. The training\\nobjective is to minimize the L2 loss with the denoiser network D\\u03b8 for different \\u03c3:\\nEz0\\u223cpdata||D\\u03b8(z\\u03c3) \\u2212z0||2\\n2,\\n(3)\\nwith the relation between D\\u03b8 and the score function \\u2207z log p(z; \\u03c3) as follows:\\n\\u2207z log p(z; \\u03c3) = (D(z\\u03c3) \\u2212z)\\/\\u03c32.\\n(4)\\nSVD [31] utilizes the EDM framework to perform generative training on large-scale video datasets,\\nresulting in a high-quality video generation model. An image-to-video generation model capable\\nof forecasting future frames given the first frame has been released. Following SVD method, we\\nalso process videos in latent space. Given an input video V \\u2208RT \\u00d7H\\u00d7W \\u00d73, a pretrained VAE\\nencoder is used to project it into the latent space frame by frame, resulting in the latent representation\\nz0 \\u2208RT \\u00d7h\\u00d7w\\u00d7D. We then build GenRec based on SVD, inheriting its strong spatial-temporal priors\\nas foundation for the subsequent generation and classification tasks.\\n3\\nGenRec\\nWe now introduce GenRec, a simple yet efficient framework, that can not only generate temporally-\\ncoherent videos conditioned on an arbitrary number of provided frames but also is able to recognize\\nactions and events with the help of encoded spatial-temporal priors. To this end, GenRec explores\\nthe strong spatial-temporal priors learned by a video diffusion model. In this work, we instantiate\\nGenRec with the powerful open-source Stable Video Diffusion model (SVD) [1], which is pretrained\\non large-scale video datasets and is able to produce a photo-realistic video when provided a single\\nframe. Then, for generation, GenRec follows the classical EDM framework to learn noise reduction\\ntrajectories. For recognition, on the other hand, GenRec operates on intermediate decoded features\\nusing a recognition head. Furthermore, to generate videos in a more free fashion, i.e. an arbitrary\\ncollection of frames used as condition, we design a latent masking strategy that \\u201cinterpolates\\u201d masked\\nframes. Such a strategy also benefits recognition by easing the training process. More importantly,\\nby doing so GenRec supports a multitude of downstream tasks, particularly when limited visual\\ninformation is provided.\\n3.1\\nPipeline Overview\\nLatent diffusion and latent masking.\\nDuring the diffusion process, the Gaussian noise with a\\ncertain noise level is added to the latent representation z0, creating a noisy latent representation \\u02dc\\nzi\\nfollowing Equation (1). Recall that while SVD contains powerful spatial-temporal priors, it can only\\nperform generation when the first frame is provided. To allow a more \\u201cfree\\u201d generation with an\\narbitrary number of frames as inputs, we design a latent masking strategy. More specifically, we\\napply a random mask m to the latent representation z0, producing a masked latent representation\\nz0. Such a strategy encourages the model to reconstruct the original video content from incomplete\\nframes, which is in a similar spirit to MAE [19]. Note that when only the first latent is available, it\\ndegrades to the same as SVD; if all latents are masked out, this degrades to unconditional generation.\\nFurthermore, doing so also benefits recognition tasks when limited visual clues are available. For\\nexample, in scenarios with limited bandwidth leading to reduced frame rates, the ability of video\\n3\\n\"}\n{\"page_content\":\"Video Latent\\nMasked Latent\\nTraining Process\\nFigure 2: The pipeline of our proposed video processing method. The input video is first processed\\nby a pretrained encoder E to produce a latent representation z0, then undergoes diffusion to generate\\na noisy latent \\u02dc\\nzt. The random mask m is used to create the masked latent z0. During training, the\\nnoisy latent is concatenated with the masked latent as condition and fed into a Spatial-Temporal UNet,\\nresulting in both reconstruction and recognition outputs. The reconstructed latent can be decoded by\\nthe pretrained decoder D to produce the final generated video.\\nframe complementation enables the model to better predict and perceive complete video information.\\nIn practice, we simulate such conditions by randomly erasing half to all video conditions, retaining\\non average only about one-fourth of the original video information. This technique allows the model\\nto effectively fill in the missing information, enhancing its ability to recognize and understand the\\nvideo content despite the reduced data availability.\\nUnifying generation and understanding.\\nTo unify generation with masked latents, GenRec\\npredicts pixel-level and semantic-level contents with the combination of the noisy latent \\u02dc\\nzi and\\nthe masked latent z0 gained by the aforementioned latent diffusion and latent masking. The two\\nlatents are channel-wise concatenated [ \\u02dc\\nzi, z0] and are fed into a Spatial-Temporal UNet, together\\nwith features from observed frames, to learn spatial and temporal representations, following [1].\\nThe weights of the UNet are initialized from [1] to obtain spatial and temporal priors, learned on\\nlarge-scale video datasets.\\nFor the generation task, the UNet aims to reconstruct the original latent representation from the\\ncombined noisy and masked inputs. Representing UNet as the mapping function F\\u03b8, its goal is to\\npredict clean latent, which, according to the EDM framework, takes the form of a representation\\nmapping as follows:\\nD\\u03b8( \\u02dc\\nzi; z0, \\u03c3) = cskip(\\u03c3) \\u02dc\\nzi + cout(\\u03c3)F\\u03b8([cin(\\u03c3) \\u02dc\\nzi, z0]),\\n(5)\\nin which we set the same skip connection cskip, scaling factor cout and cin as [1].\\nFor the recognition task, we break down the UNet mapping function as F = Ftail \\u00b7 Fhead. And we\\nconsider Fhead([cin(\\u03c3)\\u02dc\\nz, z]) \\u2208RT \\u00d7h\\n\\u2032\\u00d7w\\n\\u2032\\u00d7D\\n\\u2032\\nas the compact video representation extracted from\\nthe intermediate layer of the UNet model, which is then fed into the classifier head \\u03d5\\u03b8, consisting of\\nan attentive pooler and a fully connected layer to predict video categories:\\n\\u02c6\\ny = \\u03d5(Fhead([cin(\\u03c3) \\u02dc\\nzi, z0])).\\n(6)\\n3.2\\nOptimization\\nWe train GenRec with both generation and classification objectives, encouraging the model to learn\\nhigh-quality video generation and accurate video understanding.\\nThe generative loss uses a L2 loss to measure the difference between the original latent representation\\nand the reconstructed output produced by the UNet, and is defined as:\\nLG(z0, \\u02dc\\nzi, z0; \\u03c3) = \\u03bb(\\u03c3)\\u2225D\\u03b8( \\u02dc\\nzi; z0, \\u03c3) \\u2212z0\\u22252,\\n(7)\\n4\\n\"}\n{\"page_content\":\"where D\\u03b8( \\u02dc\\nzi; z0, \\u03c3) is the denoised output mentioned in Equation (5), and \\u03bb(\\u03c3) is a weighting\\nfunction based on the noise level \\u03c3 referring to [1, 22]. While the classification loss uses a cross-\\nentropy loss to measure the discrepancy between the true labels and the predicted labels, and is\\ndefined as:\\nLD(y, \\u02c6\\ny) = \\u2212\\nX\\ni\\nyi log(\\u02c6\\nyi),\\n(8)\\nwhere y denotes the ground truth labels, and \\u02c6\\ny represents the predicted labels referring to Equa-\\ntion Equation (6).\\nTo balance the learning of generative and recognition tasks, we set a balancing weight \\u03b3 to control\\nthe relative importance of each loss in the overall objective function. The total loss L is given by:\\nL = LD + \\u03b3LG,\\n(9)\\n3.3\\nInference for Different Downstream Tasks\\nWith the above training strategies, we now introduce how GenRec can flexibly support different types\\nof generation and recognition tasks.\\nVideo generation conditioned on frames.\\nOnce trained, GenRec is able to generate high-quality\\nvideos conditioned on an arbitrary number of given frames, thanks to the latent masking strategy.\\nParticularly, following the EDM stochastic sampler framework and Equation (2), GenRec iteratively\\ndenoises the video conditioned on the masked latent z0, as shown below:\\nzi\\u22121 = \\u02c6\\nzi + \\u03f5\\u03b8(\\u02c6\\nzi; z0) = \\u02c6\\nzi + (ti\\u22121 \\u2212\\u02c6\\nti)d\\u02c6\\nzi\\nd\\u02c6\\nti\\n(10)\\n= \\u02c6\\nzi + (ti\\u22121 \\u2212\\u02c6\\nti)(\\u22121)D\\u03b8(\\u02c6\\nzi; z0, \\u03c3) \\u2212\\u02c6\\nzi\\n\\u02c6\\nti\\n,\\n(11)\\nwhere \\u02c6\\nzi is derived from \\u02dc\\nzi adding a perturbation. With an iteratively denoising process, we can\\nfinally obtain the denoised video latent z0 which can be decoded as a complete video.\\nVideo generation conditioned on classes.\\nWhen the number of visible frames is extremely limited,\\nthe motion trajectory becomes unpredictable and thus it would be hard to make a reliable prediction\\nof the future. To mitigate this issue, GenRec supports adding category information to guide video\\ngeneration in the expected desired direction.\\nFormally, we simplify Equation (11) with Equation (4), and obtain:\\n\\u03f5\\u03b8(\\u02c6\\nzi; z0) = (ti\\u22121 \\u2212\\u02c6\\nti)(\\u22121)D\\u03b8(\\u02c6\\nzi; z0, \\u03c3) \\u2212\\u02c6\\nzi\\n\\u02c6\\nti\\n(12)\\n= (\\u22121)(ti\\u22121 \\u2212\\u02c6\\nti)\\u02c6\\nti\\u2207\\u02c6\\nzi log p\\u03b8(\\u02c6\\nzi).\\n(13)\\nWe substitute the score function \\u2207\\u02c6\\nzi log p\\u03b8(\\u02c6\\nzi) with the conditional form \\u2207\\u02c6\\nzi log p\\u03b8(\\u02c6\\nzi|y), in which\\ny denotes the conditional class. By applying Bayes\\u2018 Theorem, the original score function can be\\nreplaced by p(\\u02c6\\nzi)p(y|\\u02c6\\nzi), and we can get the conditional version of residual, denoted as \\u03f5\\u2217\\n\\u03b8(\\u02c6\\nzi; z0):\\n\\u03f5\\u2217\\n\\u03b8(\\u02c6\\nzi; z0) = (\\u22121)(ti\\u22121 \\u2212\\u02c6\\nti)\\u02c6\\nti\\u2207\\u02c6\\nzi log p(\\u02c6\\nzi)p(y|\\u02c6\\nzi)\\n(14)\\n= (\\u22121)(ti\\u22121 \\u2212\\u02c6\\nti)\\u02c6\\nti[\\u2207\\u02c6\\nzi log p(\\u02c6\\nzi) + \\u2207\\u02c6\\nzi log p(y|\\u02c6\\nzi)]\\n(15)\\n= \\u03f5\\u03b8(\\u02c6\\nzi; z0) \\u2212(ti\\u22121 \\u2212\\u02c6\\nti)\\u02c6\\nti\\u2207\\u02c6\\nzi log p(y|\\u02c6\\nzi)\\n(16)\\nConsidering the scaling factor of \\u02c6\\nzi: cin(\\u03c3) =\\n1\\n\\u221a\\n\\u03c32+\\u03c3data (following [22], and \\u03c3i = ti), that would\\npre-scale the input as c(zi) = cin(ti) \\u00b7 zi before model processing, the formulation can be further\\ntransferred as:\\n\\u03f5\\u2217\\n\\u03b8(\\u02c6\\nzi; z0) = \\u03f5\\u03b8(\\u02c6\\nzi; z0) \\u2212(ti\\u22121 \\u2212\\u02c6\\nti)\\u02c6\\nti\\nq\\n\\u02c6\\nti\\n2 + \\u03c3data\\n\\u2207c(\\u02c6\\nzi) log p(y|c(\\u02c6\\nzi))\\n(17)\\nFollowing [11], we sharpen the distribution of p(y|z) by multiplying a scaling factor s > 1, shown as\\ns \\u00b7 \\u2207z log p(y|z) = \\u2207z log 1\\nZ p(y|z)s where Z is an arbitrary constant. Larger scaling value would\\n5\\n\"}\n\n... # only display 5 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: I have retrieved the content of the GenRec paper. Now, I need to find the datasets mentioned in the paper and check which one is not evaluated in Table 1. I will use the retrieved content to search for the datasets and the table.\n[Action]:\nClassicRetrieve(query='datasets used in GenRec experiments', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.656,\"pdf_id\":\"ee1b8895-550a-5ac4-b781-7d95632c6fdc\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"08c0bd7c-141a-5cc3-81a2-be356f638893\",\"text\":\"articles. The training and development datasets\\ncomprised English, French, German, Italian, Pol-\\nish, and Russian articles. The test dataset included\\narticles in these languages and three additional lan-\\nguages, Spanish, Greek, and Georgian. The test\\ndataset was used to make final predictions. For\\na more thorough description of the available data,\\nrefer to Piskorski et al. (2023).\\n3.1\\nGenre detection\\n3.1.1\\nDataset description\\nIn the genre detection task we had to create a model\"}\n{\"score\":0.6433,\"pdf_id\":\"dee7640b-bff4-5af0-a13f-7270194ac651\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"dcf0060d-b0b8-53e2-827c-6e42b166d122\",\"text\":\"MATRES, TCR, Causal-TB, EventStoryLine,\\nand HiEve, respectively.\\nThis shows that the\\ngeneral ERE skills learned from MAVEN-ERE\\nare transferable and can help ERE on datasets\\nin other genres, especially for these small-scale\\ndatasets. We encourage future works to explore\\nthe in\\ufb02uence of genre gaps deeply.\"}\n{\"score\":0.6333,\"pdf_id\":\"a100ea06-6c2d-56e2-a361-16a64a042a4d\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8c6069a0-d155-53ea-af8c-87758f53c330\",\"text\":\"data and another music dataset to evaluate the capability of all methods in different data scenarios.\\nUnless otherwise specified, we perform 100 repeated randomized experiments and report the average\\nresult of test power as default. More details of the generating process of each dataset and the details\\nof the evaluation (including running time) are provided in the Appendix.\\n6.1\\nResults on Synthetic Datasets\\nSettings of SD, Sin, and GSign Dataset. The Sin data corresponds to the example in Sec. 3 that\"}\n{\"score\":0.6294,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9161b5c3-a2f7-55d3-9c05-41ef7affd990\",\"text\":\"margin, suggesting that even in this challenging\\nsetting GENRE is surprisingly effective.\\nThe reason behind this effectiveness varies in\\neach setting. For linking mentions to the Wikipedia\\nKB, the sheer amount of data GENRE is trained\\non enables it to recall which entity is likely best.\\nTherefore, when the data allows for such a strategy,\\nmemorization can be effective when paired with a\\nmodel that can also model the context.\\n7.2\\nGENRE-KB\\nWe evaluate GENRE-KB (GENRE augmented in\"}\n{\"score\":0.6274,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"471109ba-bf8f-5cde-9043-b99e6ee9526b\",\"text\":\"model by looking at performance on new datasets\\nand knowledge bases. We find that GENRE relies\\nheavily on memorization of name patterns, mean-\\ning that it struggles to generalize to new entities and\\nKBs. KB information is often found to be useful\\nin these cases, but its absence from GENRE means\\nit struggles when name matching fails. Therefore,\\nour second question is: can GENRE make use of\\ninformation from the KB when available? Specif-\\nically, we provide descriptive information about\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results do not contain the specific datasets used in the GenRec paper. I will refine the query to specifically look for the datasets mentioned in the experiments.\n[Action]:\nClassicRetrieve(query='datasets used in GenRec experiments', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.656,\"pdf_id\":\"ee1b8895-550a-5ac4-b781-7d95632c6fdc\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"08c0bd7c-141a-5cc3-81a2-be356f638893\",\"text\":\"articles. The training and development datasets\\ncomprised English, French, German, Italian, Pol-\\nish, and Russian articles. The test dataset included\\narticles in these languages and three additional lan-\\nguages, Spanish, Greek, and Georgian. The test\\ndataset was used to make final predictions. For\\na more thorough description of the available data,\\nrefer to Piskorski et al. (2023).\\n3.1\\nGenre detection\\n3.1.1\\nDataset description\\nIn the genre detection task we had to create a model\"}\n{\"score\":0.6433,\"pdf_id\":\"dee7640b-bff4-5af0-a13f-7270194ac651\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"dcf0060d-b0b8-53e2-827c-6e42b166d122\",\"text\":\"MATRES, TCR, Causal-TB, EventStoryLine,\\nand HiEve, respectively.\\nThis shows that the\\ngeneral ERE skills learned from MAVEN-ERE\\nare transferable and can help ERE on datasets\\nin other genres, especially for these small-scale\\ndatasets. We encourage future works to explore\\nthe in\\ufb02uence of genre gaps deeply.\"}\n{\"score\":0.6333,\"pdf_id\":\"a100ea06-6c2d-56e2-a361-16a64a042a4d\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8c6069a0-d155-53ea-af8c-87758f53c330\",\"text\":\"data and another music dataset to evaluate the capability of all methods in different data scenarios.\\nUnless otherwise specified, we perform 100 repeated randomized experiments and report the average\\nresult of test power as default. More details of the generating process of each dataset and the details\\nof the evaluation (including running time) are provided in the Appendix.\\n6.1\\nResults on Synthetic Datasets\\nSettings of SD, Sin, and GSign Dataset. The Sin data corresponds to the example in Sec. 3 that\"}\n{\"score\":0.6294,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9161b5c3-a2f7-55d3-9c05-41ef7affd990\",\"text\":\"margin, suggesting that even in this challenging\\nsetting GENRE is surprisingly effective.\\nThe reason behind this effectiveness varies in\\neach setting. For linking mentions to the Wikipedia\\nKB, the sheer amount of data GENRE is trained\\non enables it to recall which entity is likely best.\\nTherefore, when the data allows for such a strategy,\\nmemorization can be effective when paired with a\\nmodel that can also model the context.\\n7.2\\nGENRE-KB\\nWe evaluate GENRE-KB (GENRE augmented in\"}\n{\"score\":0.6274,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"471109ba-bf8f-5cde-9043-b99e6ee9526b\",\"text\":\"model by looking at performance on new datasets\\nand knowledge bases. We find that GENRE relies\\nheavily on memorization of name patterns, mean-\\ning that it struggles to generalize to new entities and\\nKBs. KB information is often found to be useful\\nin these cases, but its absence from GENRE means\\nit struggles when name matching fails. Therefore,\\nour second question is: can GENRE make use of\\ninformation from the KB when available? Specif-\\nically, we provide descriptive information about\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not contain the specific datasets used in the GenRec paper. I will try a different query to specifically look for the datasets mentioned in the experiments and the evaluation results.\n[Action]:\nClassicRetrieve(query='datasets used in GenRec experiments and evaluation results', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6968,\"pdf_id\":\"ee1b8895-550a-5ac4-b781-7d95632c6fdc\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"08c0bd7c-141a-5cc3-81a2-be356f638893\",\"text\":\"articles. The training and development datasets\\ncomprised English, French, German, Italian, Pol-\\nish, and Russian articles. The test dataset included\\narticles in these languages and three additional lan-\\nguages, Spanish, Greek, and Georgian. The test\\ndataset was used to make final predictions. For\\na more thorough description of the available data,\\nrefer to Piskorski et al. (2023).\\n3.1\\nGenre detection\\n3.1.1\\nDataset description\\nIn the genre detection task we had to create a model\"}\n{\"score\":0.6699,\"pdf_id\":\"ee1b8895-550a-5ac4-b781-7d95632c6fdc\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"6a89f48a-6e5e-5a98-836a-126c3c2205cc\",\"text\":\"bels and the ground truth. At the inference stage,\\nthe normalized output is subjected to a threshold of\\n0.3. Specifically, if the output was higher than 0.3,\\nit got labeled 1 and 0 otherwise. We used the same\\nthreshold while computing metrics during training.\\n5\\nExperimental Setup\\n5.1\\nGenre detection\\nTo establish the experiment for detecting genres,\\nwe began by creating the training set. Then, we\\nidentified the most effective hyperparameters to\\ntrain the model. Finally, we used the trained model\"}\n{\"score\":0.6521,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"7a7926dc-6042-5b17-aa09-d7508f2de1ae\",\"text\":\"more challenging datasets. Table 1 shows the per-\\nformance of the GENRE model on the Wikipedia\\nand TAC datasets. While it is unsurprising that\\nGENRE performs well on Wikipedia, the perfor-\\nmance on the TAC dataset is surprisingly high for\\nthe setting with only retrieved candidates. How-\\never, the performance on TAC in the oracle setting\\nis significantly lower. As detailed in Section 6,\\nwe add the gold standard entity to the candidate\\nset for any example where it isn\\u2019t already present.\"}\n{\"score\":0.6514,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9161b5c3-a2f7-55d3-9c05-41ef7affd990\",\"text\":\"margin, suggesting that even in this challenging\\nsetting GENRE is surprisingly effective.\\nThe reason behind this effectiveness varies in\\neach setting. For linking mentions to the Wikipedia\\nKB, the sheer amount of data GENRE is trained\\non enables it to recall which entity is likely best.\\nTherefore, when the data allows for such a strategy,\\nmemorization can be effective when paired with a\\nmodel that can also model the context.\\n7.2\\nGENRE-KB\\nWe evaluate GENRE-KB (GENRE augmented in\"}\n{\"score\":0.6423,\"pdf_id\":\"bbab7b71-b904-53b7-8e02-323392ea6dcd\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"056bba24-1b57-5439-8f64-f8ab8fba1793\",\"text\":\"4\\nExperiments\\n4.1\\nExperimental Setup\\nDatasets. We follow the previous works [19, 27] to evaluate our methods:\\nMM-IMDb [2] is currently the largest publicly available multimodal dataset for genre prediction on\\nmovie genre classification. It is notated with both image and text modalities for 25959 movies. As a\\nmovie might have several genres, the genre prediction task is thus a multi-label classification task.\\nUPMC Food-101 [33] is a large multimedia dataset consisting of noisy image-text pairs collected\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not contain the specific datasets used in the GenRec paper. I will refine the query to specifically look for the datasets mentioned in the experiments and the evaluation results, and also try to find the content of Table 1.\n[Action]:\nClassicRetrieve(query='datasets used in GenRec experiments and Table 1', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.685,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.6594,\"pdf_id\":\"0fe6f147-ceff-5ae9-869c-e3286ff574ab\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"7a7926dc-6042-5b17-aa09-d7508f2de1ae\",\"text\":\"more challenging datasets. Table 1 shows the per-\\nformance of the GENRE model on the Wikipedia\\nand TAC datasets. While it is unsurprising that\\nGENRE performs well on Wikipedia, the perfor-\\nmance on the TAC dataset is surprisingly high for\\nthe setting with only retrieved candidates. How-\\never, the performance on TAC in the oracle setting\\nis significantly lower. As detailed in Section 6,\\nwe add the gold standard entity to the candidate\\nset for any example where it isn\\u2019t already present.\"}\n{\"score\":0.6576,\"pdf_id\":\"d1ae9e60-ddbb-5c22-be66-fa91f0f674ea\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"293c1cb4-014e-5de9-ac18-c78f19a7f4a2\",\"text\":\"Dataset statistics\\nThe available datasets in TARGET can be found in Table 3. All publicly available splits of each\\ndatasets are included except for BIRD\\u2019s train split. To ensure consistency across datasets, we stan-\\ndardized their structures by aligning the column headers and component formats for all datasets.\\nIn addition to table contents and table identifiers, each corpus entry includes a \\\"context\\\" field for\"}\n{\"score\":0.6429,\"pdf_id\":\"ff860bc6-9474-58c9-8a5b-19d92fff2932\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"ad09c879-02ff-51da-931d-c483d243b160\",\"text\":\"A\\nDatasets\\nWe present detailed lists of datasets included in our research in Tables 3 and 4. They include language,\\ncategory, dataset size, class balance, and basic dataset characteristics.\\nTable 3: List of all monolingual datasets used in experiments. Category (Cat.): R - Reviews, SM -\\nSocial Media, C - Chats, N - News, P - Poems, M - Mixed. HL - human labeled, #Words and #Chars\\nare mean values\\nPaper\\nLang\\nCat.\\nHL\\nSamples\\nNEG\\/NEU\\/POS\\n#Words\\n#Char.\\nAl Omari et al. [1]\\nar\\nR\\nNo\\n3096\\n13.0\\/10.2\\/76.8\\n9\\n51\"}\n{\"score\":0.6389,\"pdf_id\":\"0e3f6c92-099b-5343-a589-7095452ddf16\",\"page_number\":14,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"68ea3cc2-4408-5410-a612-735bde647ea3\",\"text\":\"5794\\nTable 5: Dataset Table.\\nWe place more details about our datasets and experimental setups here. First, in Table 5 we show task\\ntype, prediction classes, and number of training data points in each dataset. MedAbs, Cancer, Finance,\\nand French are considered to be more challenging settings, where these datasets typically need\\ndomain expertise to provide labels. Waterbirds is considered to test for a more complex modality.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results still do not provide the specific datasets used in the GenRec paper or the content of Table 1. I will try a more direct approach by querying the content of the paper for the specific datasets and Table 1.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT page_content FROM pages WHERE ref_paper_id = '2abcf6e2-b4e3-5cf9-8c80-4497539805cc' AND page_content LIKE '%datasets used in the experiments%' OR page_content LIKE '%Table 1%' ORDER BY page_number ASC;\")"}
{"role": "user", "content": "[Observation]:\n{\"page_content\":\"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\nVolume 1: Long Papers, pages 4380\\u20134397\\nJuly 9-14, 2023 \\u00a92023 Association for Computational Linguistics\\nMVP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction\\nZhibin Gou\\u2217\\n, Qingyan Guo\\u2217\\n, Yujiu Yang\\u2020\\nTsinghua University\\nzebgou@gmail.com\\ngqy22@mails.tsinghua.edu.cn\\nyang.yujiu@sz.tsinghua.edu.cn\\nAbstract\\nGenerative methods greatly promote aspect-\\nbased sentiment analysis via generating a se-\\nquence of sentiment elements in a specified\\nformat. However, existing studies usually pre-\\ndict sentiment elements in a fixed order, which\\nignores the effect of the interdependence of the\\nelements in a sentiment tuple and the diver-\\nsity of language expression on the results. In\\nthis work, we propose Multi-view Prompting\\n(MVP) that aggregates sentiment elements gen-\\nerated in different orders, leveraging the intu-\\nition of human-like problem-solving processes\\nfrom different views. Specifically, MVP in-\\ntroduces element order prompts to guide the\\nlanguage model to generate multiple sentiment\\ntuples, each with a different element order, and\\nthen selects the most reasonable tuples by vot-\\ning. MVP can naturally model multi-view and\\nmulti-task as permutations and combinations\\nof elements, respectively, outperforming previ-\\nous task-specific designed methods on multiple\\nABSA tasks with a single model. Extensive\\nexperiments show that MVP significantly ad-\\nvances the state-of-the-art performance on 10\\ndatasets of 4 benchmark tasks, and performs\\nquite effectively in low-resource settings. De-\\ntailed evaluation verified the effectiveness, flex-\\nibility, and cross-task transferability of MVP.1\\n1\\nIntroduction\\nAspect-based sentiment analysis (ABSA) aims to\\npredict tuples of sentiment elements of interest for\\na given text. There are four sentiment elements that\\nconstitute the main line of ABSA research: aspect\\nterm (a), aspect category (c), opinion term (o) and\\nsentiment polarity (s) (Zhang et al., 2022). Given\\nan example sentence, \\u201cI love the sushi badly!\\u201d, the\\ncorresponding elements are \\u201csushi\\u201d, \\u201cfood quality\\u201d,\\n\\u2217Equal contribution.\\n\\u2020Corresponding author.\\n1Code and data released at https:\\/\\/github.com\\/\\nZubinGou\\/multi-view-prompting\\nTask\\nOutput\\nAspect Category Opinion Sentiment (ACOS) (a, c, o, s)\\nAspect Sentiment Quad Prediction (ASQP)\\n(a, c, o, s)\\nAspect Sentiment Triplet Extraction (ASTE)\\n(a, o, s)\\nTarget Aspect Sentiment Detection (TASD)\\n(a, c, s)\\nTable 1: Aspect sentiment tuple prediction tasks with\\ntheir corresponding outputs. Notably, although both\\nACOS and ASQP are the most complex quadratic pre-\\ndiction tasks, ACOS focuses on implicit aspects and\\nopinions compared to ASQP. Detailed tasks and dataset\\nstatistics are shown in Appendix A.\\n\\u201clove\\u201d and \\u201cpositive\\u201d, respectively. Early studies fo-\\ncus on a single sentiment element like aspect term\\n(Liu et al., 2015; Ma et al., 2019), aspect category\\n(Zhou et al., 2015) or sentiment polarity (Wang\\net al., 2016; Chen et al., 2017). Recent works\\npropose compound ABSA tasks involving multi-\\nple associated elements, such as aspect sentiment\\ntriplet extraction (ASTE) (Peng et al., 2020), tar-\\nget aspect sentiment detection (TASD) (Wan et al.,\\n2020), aspect sentiment quad prediction (ASQP)\\n(Zhang et al., 2021a) and aspect category opinion\\nsentiment (ACOS) (Cai et al., 2020a). Their target\\nformats are shown in Table 1.\\nRecently, generative methods have been used to\\nhandle various ABSA tasks uniformly and achieved\\ngood performance (Zhang et al., 2022), where the\\ncommon practice is to generate a sequence of sen-\\ntiment elements in a specified format to leverage\\nlabel semantics. To be specific, they use class in-\\ndex (Yan et al., 2021), sentiment element sequence\\n(Zhang et al., 2021d), natural language (Liu et al.,\\n2021a; Zhang et al., 2021b), structured extraction\\nschema (Lu et al., 2022b) or opinion tree (Bao et al.,\\n2022) as the target of the generation models.\\nHowever, previous works usually generate the\\nsequence of sentiment elements in a left-to-right\\nfixed order, which ignores the influence of the in-\\nterdependence of the elements in a sentiment tuple\\n4380\\n\"}\n{\"page_content\":\"Findings of the Association for Computational Linguistics: ACL 2023, pages 2589\\u20132605\\nJuly 9-14, 2023 \\u00a92023 Association for Computational Linguistics\\nRethinking Translation Memory Augmented Neural Machine Translation\\nHongkun Hao1\\u2217\\nGuoping Huang2\\nLemao Liu2\\u2020\\nZhirui Zhang2\\nShuming Shi2\\nRui Wang1\\u2020\\n1Shanghai Jiao Tong University\\n{haohongkun, wangrui12}@sjtu.edu.cn\\n2Tencent AI Lab\\n{donkeyhuang, redmondliu, shumingshi}@tencent.com\\nzrustc11@gmail.com\\nAbstract\\nThis\\npaper\\nrethinks\\ntranslation\\nmemory\\naugmented neural machine translation (TM-\\naugmented NMT) from two perspectives, i.e., a\\nprobabilistic view of retrieval and the variance-\\nbias decomposition principle.\\nThe finding\\ndemonstrates that TM-augmented NMT is\\ngood at the ability of fitting data (i.e., lower\\nbias) but is more sensitive to the fluctuations\\nin the training data (i.e., higher variance),\\nwhich provides an explanation to a recently\\nreported contradictory phenomenon on the\\nsame translation task: TM-augmented NMT\\nsubstantially advances vanilla NMT under the\\nhigh-resource scenario whereas it fails under\\nthe low-resource scenario. Then we propose\\na simple yet effective TM-augmented NMT\\nmodel to promote the variance and address\\nthe contradictory phenomenon.\\nExtensive\\nexperiments show that the proposed TM-\\naugmented NMT achieves consistent gains\\nover both conventional NMT and existing\\nTM-augmented NMT under two variance-\\npreferable (low-resource and plug-and-play)\\nscenarios as well as the high-resource scenario.\\n1\\nIntroduction\\nThe effectiveness of Translation Memory (TM)\\nin Machine Translation has long been recog-\\nnized (Garcia, 2009; Koehn and Senellart, 2010;\\nUtiyama et al., 2011; Wang et al., 2013; Liu et al.,\\n2019), because a TM retrieved from a bilingual\\ndataset (i.e., training data or an external dataset)\\nmay provide valuable knowledge for the source\\nsentence to be translated. Many notable approaches\\nrecently have been proposed to enhance neural\\nmachine translation (NMT) by using a TM (Feng\\net al., 2017; Gu et al., 2018; Cao et al., 2020; Hoang\\net al., 2022b; Cai et al., 2021; Huang et al., 2021).\\nFor example, on the standard JRC-Acquis\\ntask, TM-augmented NMT achieves substantial\\n\\u2217Partial work was done when Hongkun Hao was interning\\nat Tencent AI Lab.\\n\\u2020Lemao Liu and Rui Wang are corresponding authors.\\nModel\\nHigh-Resource\\nLow-Resource\\nw\\/o TM\\n60.83\\n54.54\\nw\\/ TM\\n63.76 \\u2191\\n53.92 \\u2193\\nTable 1: Testing BLEU comparison on JRC-Acquis\\nGerman\\u21d2English task. w\\/o TM and w\\/ TM denote the\\nvanilla Transformer and TM-augmented Transformer,\\nrespectively; High-Resource and Low-Resource denote\\nfull and quarter train data are used for NMT training\\nand TM retrieval.\\ngains over the vanilla NMT (without TM) under\\nthe conventional high-resource training scenario.\\nUnfortunately, Cai et al. (2021) surprisingly find\\nthat TM-augmented NMT fails to advance the\\nvanilla NMT model on the same task under a low-\\nresource scenario, if a quarter of the full data is\\nused for training and TM retrieval, as reproduced\\nin Table 1. Due to the lack of theoretical insights,\\nit is unclear the reason why such a contradictory\\nphenomenon happens. This motivates us to rethink\\nthe working mechanism of TM-augmented NMT\\nas well as its statistical principle.\\nIn this paper, we first cast TM-augmented NMT\\nas an approximation of a latent variable model\\nwhere the retrieved TM is the latent variable\\nthrough a probabilistic view of retrieval. From\\nthis statistical viewpoint, we identify that the\\nsuccess of such an approximation depends on the\\nvariance of TM-augmented NMT with respect to\\nthe latent variable. Then, we empirically estimate\\nthe variance of TM-augmented NMT from the\\nprinciple of variance-bias decomposition in the\\nlearning theory. Our findings demonstrate that TM-\\naugmented NMT is worse than the vanilla NMT\\nin terms of variance which indicates the sensitivity\\nto fluctuations in the training set, although TM-\\naugmented NMT is better in terms of the bias which\\nindicates the ability of fitting data. The finding\\nabout the variance takes the responsibility of the\\ncontradictory phenomenon in Table 1, because\\n2589\\n\"}\n{\"page_content\":\"Building Scalable Video Understanding Benchmarks through Sports\\nAniket Agarwal1 \\u2020\\nAlex Zhang2 \\u2020\\nKarthik Narasimhan2\\nIgor Gilitschenski3\\nVishvak Murahari2 *\\nYash Kant3 *\\n1IIT Roorkee\\n2 Princeton University\\n3University of Toronto\\n\\u2020Denotes equal contribution\\n*Denotes equal advising\\nhttps:\\/\\/asap-benchmark.github.io\\/\\nAbstract\\nExisting benchmarks for evaluating long video\\nunderstanding falls short on two critical aspects,\\neither lacking in scale or quality of annotations.\\nThese limitations arise from the difficulty in col-\\nlecting dense annotations for long videos, which\\noften require manually labeling each frame. In\\nthis work, we introduce an automated Annota-\\ntion and Video Stream Alignment Pipeline (ab-\\nbreviated ASAP). We demonstrate the generality\\nof ASAP by aligning unlabeled videos of four\\ndifferent sports with corresponding freely avail-\\nable dense web annotations (i.e. commentary).\\nWe then leverage ASAP\\u2019s scalability to create\\nLCric, a large-scale long video understanding\\nbenchmark, with over 1000 hours of densely an-\\nnotated long Cricket videos (with an average\\nsample length of \\u223c50 mins) collected at virtually\\nzero annotation cost. We benchmark and analyze\\nstate-of-the-art video understanding models on\\nLCric through a large set of compositional multi-\\nchoice and regression queries. We establish a\\nhuman baseline that indicates significant room\\nfor new research to explore. Our human studies\\nindicate that ASAP can align videos and anno-\\ntations with high fidelity, precision, and speed.\\nThe dataset along with the code for ASAP and\\nbaselines will be publicly released.\\n1 Introduction\\nHumans learn and master skills (e.g. playing guitar)\\nby associating and reasoning over episodic memories\\ncaptured over days, months, and years of failed and\\nsuccessful attempts (Byrne, 2008). Thus, building\\nsystems capable of understanding and reasoning over\\nvery long streams of visual data is a long-standing and\\ncrucial problem in Computer Vision.\\nLong-horizon Video Understanding (LVU) is the\\nproblem of reasoning over a long stream of video\\ndata, such as understanding the plot of a movie or\\nanalyzing the performance of a player in a lengthy\\ngame. Progress toward LVU has been greatly limited\\nby the lack of densely annotated data. Creating an\\nLVU benchmark requires manually annotating videos\\nframe-by-frame, which is incredibly tedious and hard\\nto scale. This constraint has limited the length of ex-\\nisting densely-annotated video understanding bench-\\nmarks (Table 1) from a few seconds (Jang et al., 2017;\\nSigurdsson et al., 2016; Gupta et al., 2021; Xu et al.,\\n2016) to a few minutes (Krishna et al., 2017; Wu\\nand Krahenbuhl, 2021; Zhou et al., 2018; Gella et al.,\\n2018; Bain et al., 2020).\\nA line of previous works (Huang et al., 2020;\\nTapaswi et al., 2016; Lei et al., 2018) in LVU have\\nused readily available subtitles of TV shows or en-\\ntire movies as dense annotations. While these videos\\nare sufficiently long, manual annotations are still re-\\nquired to build non-trivial queries to evaluate LVU\\nskills (Tapaswi et al., 2016; Lei et al., 2018), which\\ngreatly limits their scale. Another work (Wu and Kra-\\nhenbuhl, 2021) addresses this problem by extracting\\nsupervision from easily accessible YouTube metadata\\nof nearly \\u223c30K movie clips spanning 1 \\u22123 minutes.\\nHowever, the annotated clips are short, and the pro-\\nposed prediction tasks rely on noisy (and obscure)\\nattributes (e.g. YouTube views, like-to-dislike ratio,\\nand so on).\\nSports matches are a rich source of long videos\\n(e.g. a one-day Cricket match lasts nearly 8 hours) and\\nusually have a brief scorecard embedded in the screen\\n(shown in Figure 1) that tracks the state of the match.\\nMost sports matches also have dense annotations from\\nexperts available online (sports commentary describ-\\ning major events in the game e.g. (ESPN, 2022a,b)).\\nHowever, the annotations or the videos are not helpful\\nindividually unless they are aligned with each other.\\nTherefore, we introduce ASAP, an automated an-\\nnotation and video stream alignment pipeline, to auto-\\n1\\n\"}\n{\"page_content\":\"Findings of the Association for Computational Linguistics: ACL 2023, pages 4101\\u20134114\\nJuly 9-14, 2023 \\u00a92023 Association for Computational Linguistics\\nSETI: Systematicity Evaluation of Textual Inference\\nXiyan Fu\\nDept. of Computational Linguistics\\nHeidelberg University\\nfu@cl.uni-heidelberg.de\\nAnette Frank\\nDept. of Computational Linguistics\\nHeidelberg University\\nfrank@cl.uni-heidelberg.de\\nAbstract\\nWe propose SETI (Systematicity Evaluation of\\nTextual Inference), a novel and comprehensive\\nbenchmark designed for evaluating pre-trained\\nlanguage models (PLMs) for their systematicity\\ncapabilities in the domain of textual inference.\\nSpecifically, SETI offers three different NLI\\ntasks and corresponding datasets to evaluate\\nvarious types of systematicity in reasoning pro-\\ncesses. In order to solve these tasks, models\\nare required to perform compositional infer-\\nence based on known primitive constituents.\\nWe conduct experiments of SETI on six widely\\nused PLMs. Results show that various PLMs\\nare able to solve unseen compositional infer-\\nences when having encountered the knowledge\\nof how to combine primitives, with good perfor-\\nmance. However, they are considerably limited\\nwhen this knowledge is unknown to the model\\n(40-100 % points decrease). Furthermore, we\\nfind that PLMs can improve drastically once\\nexposed to crucial compositional knowledge\\nin minimalistic shots. These findings position\\nSETI as the first benchmark for measuring the\\nfuture progress of PLMs in achieving system-\\naticity generalization in the textual inference.\\n1\\nIntroduction\\nNatural Language Inference (NLI) determines\\nwhether a hypothesis follows from a premise (Da-\\ngan et al., 2013; Bowman et al., 2015; Williams\\net al., 2018) and has been explored for decades.\\nExisting large pre-trained language models (PLMs)\\nhave shown remarkable performance on this task\\n(Devlin et al., 2019; Raffel et al., 2019; Lan et al.,\\n2020). To better assess the true capabilities of mod-\\nels to perform NLI, various associated tasks and\\nbenchmarks have been proposed. These works\\nconcentrate on exploring how models make pre-\\ndictions, e.g. by establishing \\u2018hard\\u2019 NLI datasets\\n(Koreeda and Manning, 2021) or asking models\\nto \\u2018explain\\u2019 their predictions through highlighting\\n(Camburu et al., 2018), or by generating plausible\\nTask\\nTrain\\nTest\\nPrimitive Concepts\\nCompositional Concepts\\nCompositional Concepts\\nTask1\\nTask2\\nTask3\\n.\\nTable 1: Illustrating three visual tasks realizing different\\nforms of systematicity in compositional generalization.\\nexplanations (Bhagavatula et al., 2020). But little\\nis known about how well such models are able to\\naddress compositional generalization.\\nCompositional generalization focuses on how to\\ncombine primitive units to predict larger compou-\\nnds (Hupkes et al., 2020). A key property under-\\nlying compositional generalization is systematicity\\n(Fodor and Pylyshyn, 1988), a hallmark of human\\ncognition. Systematicity concerns the ability of\\n(re)combining known constituents and composing\\nrules. For example, humans who understand \\u2018red\\napple\\u2019 and \\u2018green train\\u2019 are able to conceptualize\\n\\u2018red train\\u2019 by recombining \\u2018red\\u2019 and \\u2018train\\u2019 into a\\nnew concept. Similar effects of systematicity (gen-\\neralization) can be studied in Natural Language Un-\\nderstanding (NLU) (Lake and Baroni, 2018; Kim\\nand Linzen, 2020). Since PLMs have achieved\\nresults on par with human performance by fitting\\nNLI training data (Wang et al., 2019), we aim to\\nevaluate to what extent these models can master\\ndifferent types of systematicity in textual inference.\\nWe propose a novel benchmark SETI (System-\\naticity Evaluation of Textural Inference), which\\nextensively explores systematicity in NLI. SETI\\ncontains three interrelated yet independent tasks\\ncovering various types of systematicity: 1) Task1:\\nprimitives \\u2192compositions aims to evaluate if\\nmodels can perform compositional inference if\\nprimitive constituents of the given inference task\\nhave been learned independently. 2) Task2: com-\\npositions \\u2192compositions aims to evaluate if mod-\\nels can perform novel compositional inferences if\\n4101\\n\"}\n\n... # only display 4 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved results do not contain the specific datasets used in the GenRec paper or the content of Table 1. I will try a more targeted SQL query to find the exact page content that mentions the datasets and Table 1.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT page_content FROM pages WHERE ref_paper_id = '2abcf6e2-b4e3-5cf9-8c80-4497539805cc' AND (page_content LIKE '%datasets used in the experiments%' OR page_content LIKE '%Table 1%') ORDER BY page_number ASC LIMIT 5;\")"}
{"role": "user", "content": "[Observation]:\n{\"page_content\":\"bring more attention to the target category. Here, p(y|c(\\u02c6\\nzi)) comes from the classification branch in\\nGenRec. Finally, we can use the same EDM sampling procedure with the derived class information\\nto generate samples.\\nStandard video recognition.\\nBased on Equation (6), GenRec can do the classical video recognition\\nby setting constant no-mask, and thus z0 is replaced by z0 and the prediction follows:\\n\\u02c6\\ny = \\u03d5(Fhead([cin(\\u03c3) \\u02dc\\nzi, z0])).\\n(18)\\nVideo recognition with partially observed frames.\\nBased on Equation (6), GenRec can be applied\\nto video recognition with partially observed frames, e.g., early action prediction that aims to predict\\nfuture events based on the initial frames, sparse video recognition where videos are sparsely encoded\\nand transmitted due to bandwidth limitations. By masking the invisible frames to get \\u02dc\\nzi, and replacing\\nthe noisy latent with random noise \\u223cobeying Gaussian distribution, GenRec can do the prediction\\nfor partially visible videos, following:\\n\\u02c6\\ny = \\u03d5(Fhead([\\u223c, z0])).\\n(19)\\n4\\nExperiments\\n4.1\\nExperimental Setup\\nDatasets.\\nIn our experiments, we use the following four datasets: Something-Something V2\\n(SSV2) [17], Kinetics-400 (K400) [24], UCF-101 [35] and Epic-Kitchen-100 (EK-100) [10]. SSV2\\ndataset is designed for fine-grained action recognition and it contains 174 action classes, 220,847\\nshort video clips with an average duration of 4 seconds. K400 contains 400 action classes, 306,245\\nvideo clips with an average duration of 10 seconds. The UCF-101 dataset comprises 13,320 videos\\nfrom 101 action categories and is widely utilized for human action recognition. The EK-100 dataset\\nfocuses on egocentric vision. It contains a total of 90,000 annotated action segments, encompassing\\n97 verb classes and 300 noun classes.\\nEvaluation protocols.\\nGenRec performs both generation and recognition tasks. For generation,\\nwe use the Fr\\u00e9chet Video Distance (FVD) [41] metric to assess the quality of the generated videos.\\nA lower FVD score indicates higher fidelity and realism. For recognition, we measure the top-1\\naccuracy that reflects the portion of correctly classified videos. We validate our model performance\\nin formal video recognition, partial video recognition, class-conditioned image-to-video generation\\nand frame completion with the above metrics.\\nImplementation details.\\nWe initially set the learning rate to 1.0 \\u00d7 10\\u22125 and set the total batch\\nsize as 32. Only generation loss will be retained for model adaptation on specific datasets. We train\\n200k steps on EK-100 and UCF, and 300k steps on SSV2 and K400, respectively. Subsequently, we\\nfinetune GenRec with both generation and recognition losses. The learning rate is set to 1.25 \\u00d7 10\\u22125\\nand decayed to 2.5\\u00d710\\u22127 using a cosine decay scheduler. We warm up models with 5 epochs, during\\nwhich the learning rate is initially set as 2.5 \\u00d7 10\\u22127 and linearly increases to the initial learning rate\\n1.25 \\u00d7 10\\u22125. The loss balance ratio \\u03b3 is set to 10, and the learning rate for the classifier head is ten\\ntimes higher than the base learning rate. We drop out the conditions 10% of the time for supporting\\nclassifier-free guidance [20], and we finetune on K400 for 40 epochs and 30 epochs on other datasets.\\nThe training is executed on 8 A100s and each contains a batch of 8 samples. We sample 16 frames\\nfor each video.\\n4.2\\nMain Results\\nComparison to state-of-the-art in video recognition and video generation.\\nWe compare with\\nstate-of-the-art methods in terms of their recognition accuracy and generation quality. The results are\\nsummarized in Table 1. The first two blocks of the table presents current advanced video recognition\\nmodels, while the third block demonstrates the performance of the diffusion-based class-guided\\nimage-to-video generation.\\nAs shown in the table, GenRec achieves optimal results or performs on par with the state-of-the-art\\napproaches. In terms of video recognition, GenRec achieves 75.8% accuracy on SSV2 dataset,\\n6\\n\"}\n{\"page_content\":\"Table 1: Performance of Video Recognition and Generation Methods. We evaluate on video recog-\\nnition and class-conditioned image-to-video generation tasks. SEER\\u2020 predicts 16 frames, while\\nothers predict 12 frames. Top-1 accuracy and FVD scores are reported. Baseline I adapts SVD to\\ndatasets with generative fine-tuning and then uses attentive-probing for classification. Baseline II\\nfully finetunes SVD with classification supervision only in traditional classification framework.\\nClassification Acc (\\u2191)\\nGeneration FVD (\\u2193)\\nMethod\\nResolution\\nParam.\\nSSV2\\nK400\\nSSV2\\nEK-100\\nw\\/o multi-modal align.\\nVideoMAE-L [40]\\n224\\u00d7224\\n305M\\n74.3\\n85.2\\n-\\n-\\nVideoMAE-H [40]\\n224\\u00d7224\\n633M\\n-\\n86.6\\n-\\n-\\nOmniMAE-H [16]\\n224\\u00d7224\\n650M\\n75.5\\n85.4\\n-\\n-\\nMVD-H [44]\\n224\\u00d7224\\n633M\\n77.3\\n87.2\\n-\\n-\\nHiera-L [32]\\n224\\u00d7224\\n214M\\n75.1\\n87.3\\n-\\n-\\nHiera-H [32]\\n224\\u00d7224\\n673M\\n-\\n87.8\\n-\\n-\\nMaskFeat-L [47]\\n312\\u00d7312\\n218M\\n75.0\\n86.4\\n-\\n-\\nw\\/ multi-modal align.\\nInternVideo [45]\\n224\\u00d7224\\n1.3B\\n77.2\\n91.1\\n-\\n-\\nInternVideo2 [46]\\n224\\u00d7224\\n6B\\n77.4\\n92.1\\n-\\n-\\nOmniVec [36]\\n-\\n-\\n85.4\\n91.1\\n-\\n-\\nOmniVec-2 [37]\\n-\\n-\\n86.1\\n93.6\\n-\\n-\\nTATS [14]\\n128\\u00d7128\\n-\\n-\\n-\\n428.1\\n920.0\\nMCVD [42]\\n256\\u00d7256\\n> 3.5B\\n-\\n-\\n1407\\n4804\\nSimVP [13]\\n64\\u00d764\\n-\\n-\\n-\\n537.2\\n1991\\nVideoFusion [28]\\n256\\u00d7256\\n1.8B\\n-\\n-\\n163.2\\n349.9\\nTune-A-Video [50]\\n256\\u00d7256\\n> 860M\\n291.4\\n365.0\\nSEER [18]\\n256\\u00d7256\\n> 860M\\n-\\n-\\n112.9\\n271.4\\nSEER\\u2020 [18]\\n256\\u00d7256\\n> 860M\\n-\\n-\\n355.4\\n-\\nBaseline I\\n256\\u00d7256\\n2.1B\\n63.7\\n82.0\\n50.3\\n53.6\\nBasline II\\n256\\u00d7256\\n1.9B\\n75.9\\n86.6\\n-\\n-\\nGenRec\\n256\\u00d7256\\n2.1B\\n75.8\\n87.2\\n46.5\\n49.3\\nsurpassing the majority of current state-of-the-art methods. On K400, GenRec achieves 87.2%\\naccuracy, which is on par with the performance of MVD-H (87.2%) and Hiera (87.3%, 87.8%),\\nand surpasses other advanced methods. These results indicate the effectiveness of our approach in\\nvideo recognition. In addition, GenRec shows a slight performance gap compared to the methods\\nin the second block. It is important to note that these advanced methods benefit significantly from\\npretraining on large-scale multimodal alignment datasets, which provide extensive cross-modal\\nsupervision that enhances their ability to capture semantic relationships across video frames.\\nWe further construct two strong baselines. Baseline I adapts SVD to the respective dataset through\\ngenerative fine-tuning, followed by attentive-probing for classification, where the backbone is frozen\\nand all frames are used as input. Baseline II involves fully fine-tuning the original SVD model with\\nclassification supervision only, ensuring that all frames are visible during training. Compared with\\nthem, GenRec performs on par or better. GenRec performs good in supporting not only classification\\nbut also generation, demonstrating its comprehensive capability in handling both tasks effectively.\\nIn terms of video generation, we evaluate the model on class-conditioned image-to-video generation\\ntask following [18]. Comparing the FVD scores of SEER:112.9 and SEER\\u2020:355.4, it can be inferred\\nthat generating longer videos with 16 frames is more difficult than generating 12 frames. GenRec\\ngenerates videos with 16 frames and achieves much lower FVD scores than the other methods,\\ndemonstrating the effectiveness of our approach in video generation.\\nIt is worth highlighting that, current research always treats video recognition and generation tasks\\nin a separate manner, and most of the advanced methods focus primarily on either recognition or\\ngeneration tasks. For instance, SEER method excels in class-conditioned image-to-video generation,\\nbut lacks the ability to do video recognition. While current research on representation learning, shown\\nas the first and second blocks in Table 1, lacks the ability to do video generation tasks. In contrast,\\nGenRec not only unifies these tasks, but also achieves competitive results compared to the specialized\\nmethods.\\n7\\n\"}\n{\"page_content\":\"A\\nAppendix \\/ supplemental material\\nA.1\\nEarly Prediction on EK-100 and UCF-101\\nMore detailed evaluation results of the video recognition with limited frames on EK-100 and UCF-101\\ncan be seen here.\\nTable 7: Early action prediction on EK-100.\\nMethod\\nVerb Obs. Ratio(\\u03c1)\\nNoun Obs. Ratio(\\u03c1)\\nAction Obs. Ratio(\\u03c1)\\n0.1\\n0.3\\n0.5\\n0.7\\n0.9\\n0.1\\n0.3\\n0.5\\n0.7\\n0.9\\n0.1\\n0.3\\n0.5\\n0.7\\n0.9\\nTemPr [38]\\n21.4\\n34.6\\n54.2\\n63.8\\n67.0\\n22.8\\n32.3\\n43.4\\n49.2\\n53.5\\n7.4\\n15.4\\n28.9\\n37.3\\n40.8\\nMVD\\u2020 [44]\\n49.3\\n60.0\\n64.7\\n68.8\\n71.3\\n35.7\\n44.7\\n49.1\\n53.4\\n55.2\\n22.5\\n32.2\\n37.5\\n42.3\\n44.5\\nGenRec\\n55.8\\n63.6\\n67.9\\n71.7\\n73.1\\n40.1\\n47.8\\n52.0\\n55.3\\n56.7\\n28.1\\n36.1\\n40.8\\n45.0\\n46.6\\nTable 8: Early action prediction on UCF dataset.\\nMethod\\nObservation Ratio(\\u03c1)\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nTemPr [38]\\n88.6\\n93.5\\n94.9\\n94.9\\n95.4\\n95.2\\n95.3\\n96.6\\n96.2\\nMVD\\u2020 [44]\\n90.8\\n94.2\\n94.8\\n95.7\\n95.5\\n96.2\\n96.5\\n96.8\\n96.9\\nGenRec\\n94.4\\n96.0\\n96.5\\n96.6\\n96.9\\n97.4\\n97.8\\n97.9\\n97.9\\nA.2\\nTraining and Inference Time Cost Compared with Previous Classification Methods\\nWe compare the training and inference time cost between MVD-H, Baseline II and GenRec, fairly\\non the same hardware resource: 4-nodes * 8 V100s, and same batch size. Baseline II refers to fully\\nfine-tuning the original SVD model with classification supervision only.\\nAs shown in the Table 9, GenRec and the Baseline II consumes more testing time than MVD-H.\\nThe difference primarily arises from the varying number of parameters. Our model is derived\\nfrom a generative model, and the complexity of the generative task necessitates a larger number\\nof parameters for effective learning. Compared with Baseline II, since GenRec requires additional\\ndecoder blocks for video generation training, the training time will get increased a little bit. As the\\nadditional up-sampling blocks will not be used when doing action recognition, GenRec shares the\\nsame testing time with Baseline II. It is worth noting that \\u201d\\u2020\\u201d in MVD-H is to highlight that MVD\\nmethod would use repeated augmentation technique during training, but such augmentation will\\nsignificantly increase the training time. Our approach does not need to use that augmentation. All the\\nmethods do the down-stream finetuning for 30 epochs.\\nTable 9: Comparison with recognition methods in terms of parameters, training time, and test time.\\nMethod\\nTrainable\\nTotal\\nTraining\\nTraining\\nTest Time\\nParams\\nParams\\nTime\\nEpochs\\n(2 \\u00d7 3 clips)\\nMVD-H\\u2020\\n633M\\n633M\\n3038 s\\/Epoch\\n30\\n441 s\\nBaseline II\\n1.3B\\n1.9B\\n2954 s\\/Epoch\\n30\\n1500 s\\nGenRec\\n1.5B\\n2.1B\\n3751 s\\/Epoch\\n30\\n1500 s\\nA.3\\nAblation on Loss Balance Ratio\\nWe conduct an ablation study on the loss balance ratio \\u03bb as shown in Equation 9. Results presented in\\nTable 10 show that varying the loss ratio has a minor impact on action recognition accuracy. However,\\nsetting the ratio too low negatively affects the generation performance. In particular, when the ratio is\\nset to zero, significant forgetting occurs, severely compromising the model\\u2019s generation ability.\\n15\\n\"}\n{\"page_content\":\"Table 10: Ablation study on the effect of different loss balance ratios on SSv2 accuracy and FVD.\\nHigher SSV2 accuracy and lower FVD indicate better performance.\\nBalance Ratio \\u03bb\\n0\\n1\\n5\\n10\\n20\\nSSv2 Acc \\u2191\\n75.6\\n75.5\\n75.6\\n75.8\\n75.5\\nSSv2 FVD \\u2193\\n1579.2\\n52.0\\n47.4\\n46.5\\n46.9\\nA.4\\nComparision with SVD Baseline\\nComparing with the open-source Stable Video Diffusion directly is meaningful. We conduct frame\\nprediction tests using SVD model on the SSV2 and Epic-Kitchen datasets, as shown in Table 11. Since\\nthe SVD model performs better at higher resolutions, we generated videos at 512x512 resolution and\\nthen downsampled them to 256x256 for FVD calculations. The results show that while SVD achieves\\ncompetitive scores compared to previous state-of-the-art methods shown in Table 1, it is suboptimal\\ncompared to GenRec. This is likely due to SVD\\u2019s design for general scenarios. Baseline I represents\\nour enhanced SVD baseline, which fine-tunes SVD on the target datasets for better results and fairer\\ncomparison with GenRec.\\nTable 11: Comparison of SSv2 FVD and Epic FVD scores in frame prediction tests.\\nMethod\\nSSv2 FVD\\nEpic FVD\\nSVD\\n99.7\\n180.8\\nBaseline I\\n50.3\\n53.6\\nGenRec\\n46.5\\n49.3\\nA.5\\nImpact of Classification Training on Generative Ability\\nTo demonstrate classification and generative can coexist well in our training without negatively\\nimpacting each other, we construct the comparison with \\\"Baseline I (SVD baseline)\\\", which adapts\\nSVD on the target dataset without classification supervision, and then trains another classifier head\\nfreeze the generation backbone, ensuring no influence from classification supervision. We conduct\\ncomparisons on SSv2: FP (frame prediction) and CFP (class-condition frame prediction), as shown\\nin Table 12. By comparing the FP results, the similar scores between the two methods conclude that\\nclassification loss does not degrade the model\\u2019s generative ability in our approach. Moreover, the\\nCFP results indicate that our method, with its more accurate classification performance, can guide the\\nmodel to achieve higher-quality video frame predictions.\\nTable 12: Comparison of SSv2 Acc and SSv2 FVD scores for Baseline I and GenRec methods in FP\\n(frame prediction) and CFP (class-condition frame prediction) scenarios.\\nMethod\\nSSv2 Acc \\u2191\\nSSv2 FVD \\u2193\\nBaseline I (FP)\\n-\\n55.5\\nGenRec (FP)\\n-\\n55.3\\nBaseline I (CFP)\\n63.7\\n50.3\\nGenRec (CFP)\\n75.8\\n46.5\\nA.6\\nCase Study for Class-Conditioned Image-to-Video Generation and Video Interpolation\\nWe show the generated visualization of the GenRec. The model can support video generation given\\nvarious numbers of frames, as well as category-guided generation. We show two of the most difficult\\ngenerative scenarios, which are: (1) given the first frame and different action categories to guide the\\nvideo generation, and (2) given the start and end frames, the model is expected to complement the\\nvideo. We also compare our methods with SEER [18] with cases picked from its official website. The\\ngeneration results can be seen in Figure 4, Figure 5 and Figure 6.\\n16\\n\"}\n\nIn total, 4 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results contain the datasets used in the experiments and the content of Table 1. I will extract the relevant information to answer the question.\n[Action]:\nGenerateAnswer(answer=['EK-100', [9.0, 10.0]])"}
{"role": "user", "content": "[Observation]: ['EK-100', [9.0, 10.0]]"}
