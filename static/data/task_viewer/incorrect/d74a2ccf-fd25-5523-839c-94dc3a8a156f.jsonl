{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. IN THE RESPONSE, YOU SHOULD ONLY CONTAIN ONE TURN, THAT IS TO SAY, YOU SHOULD OUTPUT EXACTLY ONE THOUGHT AND ONTE ACTION. Now, let's start!\n"}
{"role": "user", "content": "[Question]: Which categories of KV cache compression techniques within PoD does WindowKV belong to?\n[Answer Format]: Your answer should be a string that represents categories of KV cache compression techniques in PoD.\n[Anchor PDFs]: ['a8347203-3f01-5af1-82e9-056f254eb790', '60f8a312-8f3d-53cc-99bb-24686b1ef6fe']\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: To answer the question, I need to find the information about the categories of KV cache compression techniques within PoD, specifically where WindowKV belongs. I will start by querying the database to find relevant sections or content from the anchor PDFs.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_title, sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id IN ('a8347203-3f01-5af1-82e9-056f254eb790', '60f8a312-8f3d-53cc-99bb-24686b1ef6fe') AND sections.section_content LIKE '%KV cache compression techniques%' OR sections.section_content LIKE '%WindowKV%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"Introduction\",\"section_content\":\"Introduction\\nTasks requiring long-context understanding, such\\nas long-text reading comprehension (Trivedi et al.,\\n2022), in-context learning (Dong et al., 2024b),\\ndocument summarization (Huang et al., 2021) and\\ncode completion (Zheng et al., 2023), have gained\\nsignificant prominence in the era of LLMs. As\\na result, LLMs that are capable of processing ex-\\ntended context lengths have become increasingly\\n1Our code is available at GitHub.\\nprevalent (Huang et al., 2023). For example, mod-\\nels like GPT-4 and DeepSeek-V3 support context\\nlengths of up to 128K tokens, while Claude-3.5 and\\nYi extend this capability to 200K tokens (Achiam\\net al., 2023; Liu et al., 2024b; Fu et al., 2024). How-\\never, the self-attention mechanism in transformer\\narchitectures exhibits quadratic complexity with\\nrespect the context length (Vaswani, 2017), lead-\\ning to significant increases in inference latency for\\nlong-context scenarios. One potential method to\\nmitigate this latency is to cache the key and value\\n(KV) states of previous tokens, thereby avoiding\\nthe recomputation of historical contexts (Wadding-\\nton et al., 2013). Nevertheless, as both the input\\ncontext length and the number of layers increase,\\nthe memory required to store the KV states in-\\ncreases substantially (Luohe et al., 2024). For in-\\nstance, storing a KV cache for 100K tokens in\\nthe LLaMA2-7B model (Touvron et al., 2023) de-\\nmands over 50GB of memory, whereas a 2K token\\ncontext requires less than 1GB (Wu et al., 2024).\\nOverall, KV cache compression is essential to ad-\\ndressing issues such as memory demands, compu-\\ntational efficiency, energy consumption, and costs\\nin LLMs, directly impacting their deployment and\\napplication effectiveness in industrial scenarios.\\nRecent studies have alleviated the aforemen-\\ntioned memory constraints by modifying attention\\narchitectures (Shazeer, 2019; Ainslie et al., 2023;\\nLiu et al., 2024a), or by implementing cross-layer\\nsharing of the KV cache (Brandon et al., 2024;\\nSun et al., 2024; Wu and Tu, 2024). However,\\nthese approaches require additional training. In\\ncontrast, another line of research has focused on\\ncompressing the KV cache during the inference\\nphase. For example, some approaches evict the\\nKV states of non-essential tokens under a fixed\\nlayer budget (Zhang et al., 2023; Xiao et al., 2024;\\nAdnan et al., 2024; Ge et al., 2024). However,\\nthese methods overlook differences in attention\\nsparsity between layers. PyramidInfer (Yang et al.,\\narXiv:2503.17922v2  [cs.CL]  27 Mar 2025\\n2024b) and PyramidKV (Cai et al., 2024) observe\\nthat dense attention is particularly prevalent in the\\nlower layers, while sparse attention dominates in\\nthe higher layers (Wang et al., 2023). They propose\\nallocating varying KV cache sizes across layers to\\nmaintain a pyramid-like structure.\\nHowever, the aforementioned methods individu-\\nally select KV cache tokens, and the selected dis-\\ncrete tokens disrupt the semantic consistency of the\\ncontext. This also contradicts human reading habits.\\nWhen processing long texts, humans typically do\\nnot read token by token but rather process informa-\\ntion in windows (Rayner, 1998). Moreover, these\\nmethods employ a uniform compression strategy\\nacross all tasks, limiting their task-specific adapt-\\nability and overall performance. In fact, based on\\nhuman experience, the information processing ap-\\nproaches for different tasks vary significantly. For\\ninstance, in a question-answering task, this can be\\nseen as an information localization task 2, where\\nthe entire window is processed to capture compre-\\nhensive semantic details, thus facilitating accurate\\nanswer generation. In contrast, in a summariza-\\ntion task, the goal is information aggregation 3,\\nwhere extract the most salient information from\\neach window to generate a concise summary. The\\nchallenges outlined above motivate us to propose a\\ntask-adaptive KV cache window selection method.\\nAdditionally, to enhance computational efficiency,\\nwe integrated intra-group layer KV cache indices\\nsharing strategy to better sustain the model\\u2019s per-\\nformance under constrained memory budgets.\\nIn summary, our contributions are as follows:\\n\\u2022 Different from previous KV cache compres-\\nsion methods that select discrete tokens, we\\nintroduce a task-adaptive window selection\\nmethod, WindowKV. WindowKV dynami-\\ncally compresses the KV cache based on task-\\nspecific characteristics while preserving the\\nsemantic coherence within local windows.\\n\\u2022 Additionally, we propose an intra-group layer\\nKV cache indices sharing strategy to reduce\\ncomputational overhead, achieving a balance\\nbetween performance and efficiency.\\n2Information Localization Task: Identify the critical para-\\ngraphs within the given context, and then answer the relevant\\nquestions based on the information provided in these critical\\nparagraphs.\\n3Information Aggregation Task: Extract essential infor-\\nmation from each paragraph and compile it into a cohesive\\nsummary.\\n\\u2022 Extensive experiments are conducted on the\\nLongBench and Needle-in-a-Haystack bench-\\nmarks.\\nThe results demonstrate that Win-\\ndowKV achieves the highest number of state-\\nof-the-art results across various backbone\\nLLMs and KV cache configurations on Long-\\nBench, while also surpassing other baseline\\nmethods on the Needle-in-a-Haystack test.\\n2\"}\n{\"section_title\":\"Methodology\",\"section_content\":\"Methodology\\n2.1\\nOverview of KV Cache Compression\\nIn autoregressive transformer-based LLMs, the gen-\\neration of the i-th token requires the attention mod-\\nule to access the KV states of all preceding i \\u22121\\ntokens. To enhance the computational efficiency\\nand avoid redundant calculations, these KV states\\nare stored in the KV cache upon their initial com-\\nputation. This caching mechanism significantly ac-\\ncelerates the inference process by eliminating the\\nneed for repeated computations. However, the KV\\ncache can impose substantial memory demands,\\nparticularly for lengthy contexts. As a potential\\nmethod, KV cache compression has been proposed\\nto save memory while minimizing information loss\\nas much as possible.\\n2.2\\nWindowKV\\nIn this section, we introduce WindowKV, a novel\\napproach that employs a window level KV cache\\nselection method according to the specific require-\\nments of the task, as shown in Figure 1 (d). Un-\\nlike previous token level methods, our window se-\\nlection method enhances semantic coherence in\\nlong-context inputs, by dynamically adapting to\\nthe task\\u2019s characteristic, prioritizing relevant con-\\ntext, and closely mimicking human information\\nprocessing. Additionally, to balance performance\\nand efficiency, we integrate an intra-group layer\\nKV cache indices sharing strategy.\\n2.2.1\\nTask-Adaptive Window Selection\\nIn our method, we retain the last \\u03b1 tokens as an\\nobservation window, while the remaining context,\\nreferred to as the review context, is divided into\\nmultiple review windows. The observation window\\nis utilized to identify important review windows\\nfor caching across all layers, tailored to the specific\\ntask.\\nFor clarity, we illustrate the attention mechanism\\nusing a single head. In a standard LLM, this is\\nPrompt\\n(b) Token-selected KV Cache\\n(fixed layer budget)\\nH2O\\nLayer 0\\nLayer n\\n(a) Full KV Cache\\nPrompt\\nDense\\nPrompt\\n(c) Token-selected KV Cache\\n(dynamic layer budget)\\nPyramidKV\\nIntra-group layer KV\\u00a0\\ncache indices\\u00a0sharing\\nTask-adaptive window selection\\n(d) Window-selected KV Cache\\n(dynamic group budget)\\nWindowKV (Ours)\\nQuery\\nQuery\\nQuery\\nFigure 1: Comparison of WindowKV with state-of-the-art KV cache compression methods. (a) Full KV retains all\\ntokens in the KV cache for each layer, with cache size growing linearly with input length. (b) H2O maintains a\\nfixed cache size across layers, selecting tokens based on attention scores. (c) PyramidKV adopts a pyramid-shaped\\ncache structure, allocating varying cache budgets to different layers. These methods uniformly apply token level\\nselection strategies across all tasks. (d) WindowKV, in contrast, introduces a task-adaptive window selection method\\ncombined with intra-group layer KV cache indices sharing strategy, dynamically allocating group budgets across\\ndifferent groups.\\ncomputed as follows:\\nA = softmax\\n\\u0012Q \\u00b7 KT\\n\\u221adk\\n\\u0013\\n,\\n(1)\\nwhere dk denotes the dimension of the key states.\\nTo assess the importance of each token within\\nthe review context, we compute the attention score\\nfor each token based on the attention it receives\\nfrom the observation window. This is formally\\nexpressed as:\\ntj =\\nX\\ni\\u2208[n\\u2212\\u03b1,n]\\nAij,\\nj \\u2208[0, n \\u2212\\u03b1],\\n(2)\\nwhere tj represents the score of the j-th token in the\\nreview context, n denotes the input context length.\\nHowever, this token selection approach disrupts\\nsemantic coherence. To preserve semantic coher-\\nence and accommodate the variability in human\\nreading patterns across different tasks, we further\\npropose a task-adaptive window selection method.\\nSpecifically, in question-answering tasks, which\\ncan be viewed as Information Localization, hu-\\nmans must comprehend the semantic content of the\\nentire review window to give accurate responses. In\\ncontrast, for tasks such as summarization and code\\ncompletion, which can be viewed as Information\\nAggregation, humans only need to focus on the\\nmost critical and relevant context within individual\\nreview windows.\\nThe review windows in the context can be repre-\\nsented as follows:\\nWk = {tj, \\u00b7 \\u00b7 \\u00b7 , tj+\\u03c9\\u22121},\\nk \\u2208\\n\\u0014\\n1,\\n\\u0018n \\u2212\\u03b1\\n\\u03c9\\n\\u0019\\u0015\\n,\\n(3)\\nwhere \\u03c9 denotes the review window size, window\\nWk consists of tokens tj, \\u00b7 \\u00b7 \\u00b7 , tj+\\u03c9\\u22121, and \\u2308\\u00b7\\u2309de-\\nnotes the ceiling function.\\nTo facilitate this process, a task-adaptive clas-\\nsifier is trained, with detailed training procedures\\ndescribed in Appendix A.3 and A.5. Consequently,\\nthe scoring function for evaluating the importance\\nof review windows in the context for a specific task\\nis defined as follows:\\nsk =\\n1\\nmin(p, \\u03c9) \\u00b7 sum(Top-p(Wk)),\\n(4)\\nwhere Top-p(Wk) = {t\\u2032\\n0, t\\u2032\\n1, \\u00b7 \\u00b7 \\u00b7 , t\\u2032\\np\\u22121} represents\\nthe selection of the p tokens with the highest scores\\nfrom the w tokens in the window. When p = \\u03c9, it\\naligns with the information localization task. When\\np < \\u03c9, the scenario aligns with the information\\naggregation task. The task type, identified by the\\ntask-adaptive classifier from the input context, is\\nused to invoke the corresponding window selection\\nmethod. Based on the allocated budget, a subset of\\nhigh-scoring windows is retained from the review\\ncontext to maintain the desired budget of KV cache.\\nThe detailed budget allocation strategy is described\\nin Section 2.2.3.\\nHowever, performing review window selection\\nat every layer is computationally expensive. Ma\\net al. (Ma et al., 2024) demonstrate that the at-\\ntention scores of adjacent layers in LLMs exhibit\\nsimilarity. Additionally, Liu et al.\\n(Liu et al.,\\n2025) proposed a layer-wise index reuse method\\nunder fixed layer budgets, which further validates\\nthe inter-layer similarity in LLMs. Inspired by the\\ninter-layer similarity characteristics of LLMs, we\\npropose the intra-group layer KV cache indices\\nsharing strategy to optimize the trade-off between\\nperformance and efficiency in WindowKV.\\n2.2.2\\nEfficient Intra-Group Layer KV Cache\\nIndices Sharing Strategy\\nTo enhance the efficiency of review window selec-\\ntion, an intra-group layer KV cache indices sharing\\nstrategy is employed.\\nAssume that the transformer layers of a model\\nare denoted as L = {l0, l1, . . . , lm\\u22121}, where m\\nrepresents the number of layers in the model. The\\nlayers in L are divided into H = m\\n\\u03b3 groups, each\\ncontaining \\u03b3 layers.\\nFor a given group G =\\n{lg, lg+1, . . . , lg+\\u03b3\\u22121}, we apply the method from\\nSection 2.2.1 to perform task-adaptive window se-\\nlection on the first layer lg, obtaining the KV cache\\nindices Ilg to be retained for that layer. For the\\nremaining layers in the group {lg+1, . . . , lg+\\u03b3\\u22121},\\nthey share the same KV cache indices Ilg as lg.\\nBy adopting this approach, the computational\\ncost can be significantly reduced, as the window se-\\nlection algorithm is executed only once per group.\\n2.2.3\\nDynamic Budget Allocation\\nInspired by PyramidKV (Cai et al., 2024), we al-\\nlocate budgets to each group using an arithmetic\\nsequence. The total budget for all groups is defined\\nas:\\nbtotal =\\nX\\nh\\u2208[0,H\\u22121]\\nbh,\\n(5)\\nwhere H represents the number of groups.\\nFor all groups {G0, \\u00b7 \\u00b7 \\u00b7 , GH\\u22121}, we first com-\\npute the budgets for the top group GH\\u22121 and the\\nbottom group G0 as:\\nbH\\u22121 = btotal\\n\\u03bb \\u00b7 H\\nand\\nb0 = 2 \\u00b7 btotal\\nH\\n\\u2212bH\\u22121,\\n(6)\\nwhere \\u03bb is a hyperparameter that controls the shape\\nof the pyramid. The budgets for the remaining\\ngroups are calculated using the following equation:\\nbh = b0 \\u2212b0 \\u2212bH\\u22121\\nH \\u22121\\n\\u00d7 h.\\n(7)\\nFinally, the budget for each group is averagely dis-\\ntributed across all layers within the group.\\n3\"}\n\n... # only display 2 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved sections provide detailed information about the WindowKV method and its categorization within the KV cache compression techniques. From the \"Introduction\" and \"Methodology\" sections, it is clear that WindowKV is a task-adaptive window selection method that enhances semantic coherence in long-context inputs. This method is part of the KV cache compression techniques that focus on preserving semantic consistency within local windows. I will now formulate the answer based on this information.\n[Action]:\nGenerateAnswer(answer='Task-adaptive window selection method that enhances semantic coherence in long-context inputs')"}
{"role": "user", "content": "[Observation]: Task-adaptive window selection method that enhances semantic coherence in long-context inputs"}
