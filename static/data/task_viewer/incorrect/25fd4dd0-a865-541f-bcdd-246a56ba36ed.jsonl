{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: Both the papers use the Model Performance on EditEval to test their models. What existing models' data do they use in common?\n[Answer Format]: Your answer should be a python list, each elemet is a string , which refers to a model name.\n[Anchor PDFs]: ['cabc7bed-6a8b-5030-a199-716eac881799', 'f9c34aba-31a0-5b67-83a6-3cde37f2aecb']\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: To find the common existing models' data used in both papers, I will first retrieve the relevant sections or tables that mention \"EditEval\" and \"model performance\" from both papers. I will use the `sections` and `tables` tables in the database to find this information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_title, sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id IN ('cabc7bed-6a8b-5030-a199-716eac881799', 'f9c34aba-31a0-5b67-83a6-3cde37f2aecb') AND sections.section_content LIKE '%EditEval%model performance%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"Experiment Settings\",\"section_content\":\"Experiment Settings\\nModel Training Setting\\nOur pre-trained checkpoint is PaLM 2-XXS3. We leverage pre-trained PALM 2-L as the off-the-shelf\\nLLM for data generation, LLM filtering, and critique distillation. The training hyper-parameters for\\ninstruction tuning and reinforcement learning are listed in the Appendix Section Hyper-parameter\\nSetting.\\nEvaluation Datasets\\nMessageRewriteEval\\nTo evaluate the model performance in the on-device messaging scenario, we introduce MES-\\nSAGEREWRITEEVAL, a novel evaluation dataset specifically designed for message-level rewrite\\nassessments. All text message pairs are sourced from real-life, human-written daily use cases and\\nevaluated by human raters for data quality. To ensure comprehensive evaluation, these pairs encom-\\npass five text rewrite tasks: Formalize, Paraphrase, Shorten, Elaborate, and Proofread. Each text\\npair in the dataset consists of three components: source, target, and instruction. The task distribution\\nstatistics and example instructions are provided in Appendix Section MESSAGEREWRITEEVAL Data.\\nThe data collection guidelines are given in Appendix Section Data Guidelines.\\nEditEval\\nBesides the on-device messaging scenario, we evaluate the model performance on more general text\\nrewriting tasks. We use the public rewrite benchmark EditEval 4 (Dwivedi-Yu et al., 2022) which\\ncovers rewriting task at both sentence and paragraph levels. The detailed description of the different\\ndatasets in this benchmark can be found in Appendix Section EditEval Dataset.\\nAutomatic Evaluation Metrics\\nWe employ various metrics to evaluate the model\\u2019s quality:\\n\\u2022 NLI (Bowman et al., 2015) and Reversed NLI (Section Heuristic Reward).\\n\\u2022 Edit Distance Ratio (Edit Ratio) (Section Heuristic Reward).\\n\\u2022 SARI (Xu et al., 2016) is an n-gram based metric that measures the similarity of a prediction\\nto both the source and reference texts. The scores of add, retain and delete operations are\\ncomputed by averaging n-gram scores. The SARI metric is obtained using an arithmetic\\naverage of the F1 scores of add and retain operations and the precision of the delete operation.\\n\\u2022 BLEU (Papineni et al., 2002) is computed as a geometric mean of n-gram precisions of\\ndifferent orders. n-gram precision is the fraction of n-grams in the output that also occur in\\nthe reference. Though originally intended for machine translation, it is now widely used for\\nevaluating rewrite tasks (Wei Xu and Callison-Burch, 2016).\\n\\u2022 Update-ROUGE (Updated-R) (Iv et al., 2022) measures the recall of n-grams between the\\nmodel\\u2019s prediction and the references. It is a modified version of ROUGE (Lin and Hovy,\\n2003). Updated-R specifically computes ROUGE-L on the updated sentences rather than\\nthe full text.\\n\\u2022 Success Rate We use the LLM to assess whether or not the response follows the instruction\\n(i.e. \\u201cgood\\u201d or not). Although a binary classification might be too coarse grained for\\nevaluating rewrite quality, it is a very intuitive and straightforward metric to show the\\nmerit of cascading. The LLM prompts are provided in Appendix Table 10. For cascading\\nexperiments, we also measure On-device Inference Ratio which is the percentage of\\ninference calls that are serviced by the on-device model. A higher ratio means a smaller\\npercent of server calls.\\n3We follow the size notations in PaLM 2 tech report (Passos et al., 2023). Model size XXS is over 20 times\\nsmaller than model size S and over 5 times smaller than model size XS.\\n4https:\\/\\/github.com\\/facebookresearch\\/EditEval\\n7\\nBaselines\\nSince it is designed for on-device application, our model has a compact size in comparison to other\\nLLMs. In choosing baseline models, we prioritize the ones that are similar in size to ours. We\\nchoose the state-of-the-art pre-trained models PaLM 2 (Passos et al., 2023), LLaMA (Touvron et al.,\\n2023) and the instruction tuned models Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023),\\nFlan-PaLM 2 (Passos et al., 2023) as our baseline models. We also provide Alpaca-PaLM 2 for\\ncomparison. The Alpaca\\u2019s instruction dataset is finetuned using a PaLM 2 baseline checkpoint.\\nFor a fair comparison, we leveraged in-context learning with CoT few-shot prompting (we share the\\ndetails in Appendix Section Few-shot Prompts for Pre-trained LLM Inference) to instruct the model\\nto provide reasonable responses for the pre-trained models since they are not instruction tuned. In\\ncontrast, for the instruction tuned LLMs including ours, we use zero-shot settings. For cascading,\\nwe note that constructing a powerful large language model is not within the the scope of this study.\\nTherefore, our experiments utilize the 175B InsGPT (Ouyang et al., 2022) as the server model.\\nOn-device Inference\\nTo demonstrate the effectiveness of running our models using limited resources, we obtain bench-\\nmark numbers on popular mobile phones to obtain two primary metrics: Inference Latency per\\nToken, measured in milliseconds, and Memory Consumption, quantified in gigabytes during model\\noperation. We introduce an inference engine utilizing OpenCL that harnesses the computational\\ncapabilities of on-device GPUs. We adopt similar optimizations reported in Chen et al. (2023b) and\\nfurther devise special kernels tailored for our on-device Instruct-oriented models. To accommodate\\nmodels within constrained memory capacities, we employ 8-bit post-training quantization as the\\nstandard setting for reporting quality metrics. The latency\\/memory numbers of both 8- and lower-bit\\nquantized model are presented to compare with commonly adopted configurations. We note that the\\nquality implication of lower-bit quantization and quantization-aware training is beyond the scope of\\nthis paper.\"}\n{\"section_title\":\"Experiments and Results\",\"section_content\":\"EXPERIMENTS AND RESULTS\\nThis section provides an overview of our experimental settings, baselines, and result analysis. De-\\ntailed information about the hyperparameters can be found in Appendix A.4.\\n5.1\\nBASELINES\\nWe use the following baseline models for quality comparison in the later sections:\\n\\u2022 PaLM (Chowdhery et al., 2022) is a large, densely activated transformer-based language\\nmodel that can generate text in an open-ended fashion.\\n\\u2022 PaLM 2 (Passos et al., 2023), is an advanced language model which surpasses its pre-\\ndecessor PaLM in terms of multilingual and reasoning abilities while being more com-\\nputationally efficient. It is a Transformer-based model that underwent training using a\\nblend of objectives. In this paper, we employ PaLM 2-S. This \\u201cS\\u201d size is comparable to\\nLLaMA\\/Alpaca\\/Vicuna-13B, which is why we opted to train using it rather than the largest\\nPaLM 2. Note that the specific number of parameters for the PaLM 2 series has not been\\n7\\nRewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting\\nEdit Ratio\\nNLI (s-p)\\nNLI (p-s)\\nSARI\\nGLEU\\nUpdate-R\\nPretrained LLMs\\nPaLM (Chowdhery et al., 2022)\\n62B\\n0.31\\n0.25\\n0.11\\n28.24\\n0.74\\n11.99\\nPaLM 2 (Passos et al., 2023)\\nS\\n1.22\\n0.63\\n0.37\\n28.62\\n0.48\\n8.14\\nLLaMA (Touvron et al., 2023)\\n65B\\n0.71\\n0.83\\n0.83\\n27.98\\n2.10\\n21.35\\nInstruction-Tuned LLMs\\nAlpaca (Taori et al., 2023)\\n13B\\n0.11\\n0.90\\n0.85\\n36.12\\n6.81\\n34.88\\nAlpaca-PaLM 2\\nS\\n0.12\\n0.9\\n0.84\\n38.51\\n8.31\\n36.56\\nVicuna (Chiang et al., 2023)\\n13B\\n0.23\\n0.89\\n0.77\\n39.05\\n6.84\\n33.31\\nFlan-PaLM (Chung et al., 2022)\\n62B\\n0.12\\n0.58\\n0.42\\n24.52\\n1.87\\n6.23\\nInsGPT (text davinci 001)\\n-\\n0.09\\n0.66\\n0.61\\n27.17\\n3.72\\n18.69\\nChatGPT (GPT 3.5 Turbo)\\n-\\n0.13\\n0.95\\n0.87\\n40.04\\n8.47\\n37.78\\nRewriteLMs\\nRewrite-PaLM\\n62B\\n0.14\\n0.88\\n0.76\\n37.02\\n7.40\\n36.68\\nRewrite-PaLM 2\\nS\\n0.25\\n0.93\\n0.79\\n40.92\\n9.64\\n39.36\\nRewrite-RL-PaLM 2\\nS\\n0.27\\n0.94\\n0.81\\n40.97\\n9.43\\n39.36\\nRewrite-RLr\\/w-PaLM 2\\nS\\n0.29\\n0.96\\n0.87\\n40.66\\n9.64\\n40.10\\nTable 4: Model Performance on OPENREWRITEEVAL. Edit distance ratio (Edit Ratio) between the\\nmodel prediction and the source text; NLI score with source as premise and model prediction as\\nhypothesis (NLI s-p) and vice versa (NLI p-s); SARI, GLEU and Updated-ROUGE (Updated-R)\\nbetween the gold reference and the model prediction are reported here.\\nJFL\\nTRK\\nAST\\nWNC\\nFRU\\nWFI\\nSARI\\nGLEU\\nSARI\\nSARI\\nSARI\\nSARI\\nUpdate-R\\nSARI\\nUpdated-R\\nCopy\\n-\\n26.7\\n40.5\\n26.3\\n20.7\\n31.9\\n29.8\\n0\\n33.6\\n-\\nTk (Wang et al., 2022b)\\n3B\\n31.8\\n39\\n32.8\\n29.9\\n31.3\\n12.6\\n3.6\\n1.3\\n4.5\\nT0 (Sanh et al., 2022)\\n3B\\n42\\n38.8\\n34.4\\n32.3\\n22.3\\n14.2\\n9.6\\n5.1\\n16.3\\nT0++ (Sanh et al., 2022)\\n11B\\n34.7\\n43.2\\n32.9\\n28.2\\n29.3\\n12.6\\n3.7\\n4.4\\n8.1\\nPEER-3 (Schick et al., 2022)\\n3B\\n55.5\\n54.3\\n32.5\\n30.5\\n53.3\\n39.1\\n30.9\\n34.4\\n18.7\\nPEER-11 (Schick et al., 2022)\\n11B\\n55.8\\n54.3\\n32.1\\n29.5\\n54.5\\n39.6\\n31.4\\n34.9\\n20.4\\nOPT (Zhang et al., 2022b)\\n175B\\n47.3\\n47.5\\n32.6\\n31.8\\n31.2\\n35.9\\n27.3\\n26.7\\n11.2\\nGPT-3 (Brown et al., 2020)\\n175B\\n50.3\\n51.8\\n33\\n30.5\\n31.7\\n36\\n21.5\\n27.2\\n10.6\\nInsGPT (Ouyang et al., 2022)\\n175B\\n61.8\\n59.3\\n38.8\\n38\\n35.4\\n36.3\\n24.7\\n23.6\\n16.1\\nPaLM 2 (Passos et al., 2023)\\nS\\n36.07\\n2.18\\n34.32\\n35.92\\n25.2\\n24.28\\n26.39\\n11.41\\n20.42\\nRewrite-PaLM 2 (Ours)\\nS\\n56.95\\n40.38\\n40.81\\n42.11\\n37.11\\n37.51\\n53.54\\n26.55\\n47.06\\nRewrite-RLr\\/w-PaLM 2 (Ours)\\nS\\n55\\n22.89\\n40.87\\n41.71\\n37.81\\n38.56\\n53.93\\n29.25\\n49.53\\nTable 5: Model Performance on EditEval (Dwivedi-Yu et al., 2022).\\nmade public. Instead, the PaLM 2 Tech Report uses T-shirt sizes to represent model sizes,\\nranging from XXS to L. We follow its notations.\\n\\u2022 LLaMA (Touvron et al., 2023) is an efficient, open-source foundation language model.\\n\\u2022 Flan-PaLM (Chung et al., 2022) is fine-tuned on a large variety of tasks and chain-of-\\nthought data using PaLM as the base model.\\n\\u2022 Alpaca (Taori et al., 2023) is a language model that is fine-tuned from LLaMA using 52,000\\ninstruction-following demonstrations.\\n\\u2022 Alpaca-PaLM: We fine-tune the PaLM model on Alpaca instruction-following datasets.\\n\\u2022 Vicuna (Chiang et al., 2023) is an open-source chatbot trained by fine-tuning LLaMA on\\nuser-shared conversations collected from ShareGPT3.\\n\\u2022 ChatGPT (gpt-3.5-turbo)4 and InsGPT (text-davinci-001) Ouyang et al. (2022) are mem-\\nbers of the GPT family, developed by OpenAI.\\nWe follow the same zero-shot prompt setting for all the baseline models as Schick et al. (2022). The\\npre-trained models without any instruction tuning generally exhibit slightly lower performance in\\nfollowing instructions compared to the instruction tuned models under zero-shot scenario.\\n3https:\\/\\/sharegpt.com\\/\\n4https:\\/\\/openai.com\\/chatgpt\\n8\\nRewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting\\nInstruction\\nSuccess\\nContent\\nPreservation\\nFactuality Coherence Fluency AVG\\nAgreement\\n0.784\\n0.781\\n0.769\\n0.933\\n0.804\\n0.814\\nHuman Expert\\n1.833\\n1.949\\n1.985\\n1.99\\n1.99\\n1.949\\nAlpaca 13B\\n1.441\\n1.754\\n1.934\\n1.962\\n1.977\\n1.814\\nAlpaca-PaLM 2\\n1.489\\n1.719\\n1.99\\n2\\n2\\n1.839\\nChatGPT\\n1.478\\n1.775\\n1.959\\n1.962\\n1.975\\n1.83\\nRewrite-PaLM 2\\n1.641\\n1.777\\n1.927\\n2\\n2\\n1.869\\nRewrite-RLr\\/w-PaLM 2 1.648\\n1.835\\n1.959\\n1.985\\n2\\n1.886\\nTable 6: Human Evaluation Results.\\n5.2\\nRESULTS ON OPENREWRITEEVAL BENCHMARK\\nThe automatic evaluation results for the OPENREWRITEEVAL dataset are presented in Table 4.\\nRewrite-PaLM and Rewrite-PaLM 2 are supervised fine-tuned versions (as discussed in Section\\n3.2) based on PaLM, and PaLM 2, respectively. Rewrite-RL-PaLM 2 and Rewrite-RLr\\/w-PaLM 2\\nare reinforcement learning models tuned over Rewrite-PaLM 2. The reward model from the former\\ndoes not use our synthetic preference dataset (as discussed in Section 3.2), whereas the reward model\\nfrom the latter incorporates it.\\nAs shown in Table 4, our RL tuned model Rewrite-RLr\\/w-PaLM 2 has the highest scores in almost\\nall the metrics (i.e., NLI scores, SARI, GLEU, and Update-R). This indicates that our model is good\\nat generating outputs faithful to the original input, while other models might generate more \\u201challuci-\\nnations\\u201d. For edit ratio, Rewrite-RLr\\/w-PaLM 2 has a better score than all the models except PaLM\\n2. Pre-trained models such as PaLM 2 without any instruction tuning are prone to generating \\u201challu-\\ncinations\\u201d, resulting in a significantly high edit ratio score (i.e.1.22). Therefore, our model is good\\nat keeping all the essential content and meaning of the source text, while also being able to rewrite\\nwith varied language and structures. Given that Rewrite-RLr\\/w-PaLM 2 consistently outperforms\\nRewrite-RL-PaLM 2 across nearly all metrics, this strongly suggests the effectiveness and value of\\nemploying synthetic preference data.\\nSee Appendix A.6 for more metrics on OpenRewriteEval dataset (see Table 10) and a breakdown by\\neach subgroup (see Tables 11, 12, 13, 14, 15, 16).\\n5.3\\nRESULTS ON EDITEVAL\\nWe also evaluated the performance of our models using the publicly available sentence-level rewrite\\nbenchmark EditEval5 (Dwivedi-Yu et al., 2022). This benchmark comprises various datasets that\\ncover different language tasks. Specifically, JFL (Napoles et al., 2017) focuses on language fluency;\\nTRK (Xu et al., 2016) and AST (Alva-Manchego et al., 2020) target at sentence simplification; WNC\\n(Pryzant et al., 2020) addresses text neutralization; FRU (Iv et al., 2022)) and WFI (Petroni et al.,\\n2022) involve updating information that requires external references. More data statistics for each\\ndataset can be found in Table 9.\\nWe only report the results on EditEval datasets that containing more than 100 test examples (see\\nTable 5). The results of LLM baselines and the Copy baseline (which treats the source text as the\\nprediction) are taken directly from the EditEval paper (Dwivedi-Yu et al., 2022). We can observe\\nthat the zero-shot performance of Rewrite-PaLM 2 and Rewrite-RLr\\/w-PaLM 2 is mostly on par\\nwith or better than the best baselines (i.e.PEER-11 and InsGPT). While our model is specifically\\ndesigned for long-form text rewriting, it does not sacrifice its capability to handle sentence-level\\nrewriting tasks.\\n5.4\\nRESULTS ON HUMAN EVALUATION\\nThe human evaluation results, detailed in Table 6, reveal notable insights. The inter-annotator agree-\\nments, quantified using the Fleiss kappa coefficient Fleiss (1971), underscore the reliability of the\\n5https:\\/\\/github.com\\/facebookresearch\\/EditEval\\n9\\nRewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting\\nevaluations. Notably, Rewrite-PaLM 2 and Rewrite-RWr\\/w-PaLM 2 demonstrate superior perfor-\\nmance over Alpaca, Alpaca-PaLM 2, and ChatGPT in instruction success and content preservation.\\nThis alignment with the automatic evaluation metrics underscores the efficacy of these models in\\nadhering to given instructions while maintaining the integrity of the original content. In terms of\\ncoherence and fluency, all models, including the human rewrites, scored above 1.96, indicative of\\ntheir ability to generate clear, unambiguous, and logically coherent outputs. Such high scores sug-\\ngest that these models\\u2019 outputs are not only understandable but also align closely with human-level\\nlanguage proficiency. Human expertise still prevails in aspects of instruction success and content\\npreservation, suggesting room for further improvement in model performance to reach human-level\\nproficiency in rewriting tasks.\\n6\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The sections retrieved from both papers mention the EditEval benchmark and the baseline models used for comparison. I will now extract the specific models mentioned in both papers and identify the common models.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id IN ('cabc7bed-6a8b-5030-a199-716eac881799', 'f9c34aba-31a0-5b67-83a6-3cde37f2aecb') AND sections.section_content LIKE '%EditEval%' AND sections.section_content LIKE '%baseline%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Experiment Settings\\nModel Training Setting\\nOur pre-trained checkpoint is PaLM 2-XXS3. We leverage pre-trained PALM 2-L as the off-the-shelf\\nLLM for data generation, LLM filtering, and critique distillation. The training hyper-parameters for\\ninstruction tuning and reinforcement learning are listed in the Appendix Section Hyper-parameter\\nSetting.\\nEvaluation Datasets\\nMessageRewriteEval\\nTo evaluate the model performance in the on-device messaging scenario, we introduce MES-\\nSAGEREWRITEEVAL, a novel evaluation dataset specifically designed for message-level rewrite\\nassessments. All text message pairs are sourced from real-life, human-written daily use cases and\\nevaluated by human raters for data quality. To ensure comprehensive evaluation, these pairs encom-\\npass five text rewrite tasks: Formalize, Paraphrase, Shorten, Elaborate, and Proofread. Each text\\npair in the dataset consists of three components: source, target, and instruction. The task distribution\\nstatistics and example instructions are provided in Appendix Section MESSAGEREWRITEEVAL Data.\\nThe data collection guidelines are given in Appendix Section Data Guidelines.\\nEditEval\\nBesides the on-device messaging scenario, we evaluate the model performance on more general text\\nrewriting tasks. We use the public rewrite benchmark EditEval 4 (Dwivedi-Yu et al., 2022) which\\ncovers rewriting task at both sentence and paragraph levels. The detailed description of the different\\ndatasets in this benchmark can be found in Appendix Section EditEval Dataset.\\nAutomatic Evaluation Metrics\\nWe employ various metrics to evaluate the model\\u2019s quality:\\n\\u2022 NLI (Bowman et al., 2015) and Reversed NLI (Section Heuristic Reward).\\n\\u2022 Edit Distance Ratio (Edit Ratio) (Section Heuristic Reward).\\n\\u2022 SARI (Xu et al., 2016) is an n-gram based metric that measures the similarity of a prediction\\nto both the source and reference texts. The scores of add, retain and delete operations are\\ncomputed by averaging n-gram scores. The SARI metric is obtained using an arithmetic\\naverage of the F1 scores of add and retain operations and the precision of the delete operation.\\n\\u2022 BLEU (Papineni et al., 2002) is computed as a geometric mean of n-gram precisions of\\ndifferent orders. n-gram precision is the fraction of n-grams in the output that also occur in\\nthe reference. Though originally intended for machine translation, it is now widely used for\\nevaluating rewrite tasks (Wei Xu and Callison-Burch, 2016).\\n\\u2022 Update-ROUGE (Updated-R) (Iv et al., 2022) measures the recall of n-grams between the\\nmodel\\u2019s prediction and the references. It is a modified version of ROUGE (Lin and Hovy,\\n2003). Updated-R specifically computes ROUGE-L on the updated sentences rather than\\nthe full text.\\n\\u2022 Success Rate We use the LLM to assess whether or not the response follows the instruction\\n(i.e. \\u201cgood\\u201d or not). Although a binary classification might be too coarse grained for\\nevaluating rewrite quality, it is a very intuitive and straightforward metric to show the\\nmerit of cascading. The LLM prompts are provided in Appendix Table 10. For cascading\\nexperiments, we also measure On-device Inference Ratio which is the percentage of\\ninference calls that are serviced by the on-device model. A higher ratio means a smaller\\npercent of server calls.\\n3We follow the size notations in PaLM 2 tech report (Passos et al., 2023). Model size XXS is over 20 times\\nsmaller than model size S and over 5 times smaller than model size XS.\\n4https:\\/\\/github.com\\/facebookresearch\\/EditEval\\n7\\nBaselines\\nSince it is designed for on-device application, our model has a compact size in comparison to other\\nLLMs. In choosing baseline models, we prioritize the ones that are similar in size to ours. We\\nchoose the state-of-the-art pre-trained models PaLM 2 (Passos et al., 2023), LLaMA (Touvron et al.,\\n2023) and the instruction tuned models Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023),\\nFlan-PaLM 2 (Passos et al., 2023) as our baseline models. We also provide Alpaca-PaLM 2 for\\ncomparison. The Alpaca\\u2019s instruction dataset is finetuned using a PaLM 2 baseline checkpoint.\\nFor a fair comparison, we leveraged in-context learning with CoT few-shot prompting (we share the\\ndetails in Appendix Section Few-shot Prompts for Pre-trained LLM Inference) to instruct the model\\nto provide reasonable responses for the pre-trained models since they are not instruction tuned. In\\ncontrast, for the instruction tuned LLMs including ours, we use zero-shot settings. For cascading,\\nwe note that constructing a powerful large language model is not within the the scope of this study.\\nTherefore, our experiments utilize the 175B InsGPT (Ouyang et al., 2022) as the server model.\\nOn-device Inference\\nTo demonstrate the effectiveness of running our models using limited resources, we obtain bench-\\nmark numbers on popular mobile phones to obtain two primary metrics: Inference Latency per\\nToken, measured in milliseconds, and Memory Consumption, quantified in gigabytes during model\\noperation. We introduce an inference engine utilizing OpenCL that harnesses the computational\\ncapabilities of on-device GPUs. We adopt similar optimizations reported in Chen et al. (2023b) and\\nfurther devise special kernels tailored for our on-device Instruct-oriented models. To accommodate\\nmodels within constrained memory capacities, we employ 8-bit post-training quantization as the\\nstandard setting for reporting quality metrics. The latency\\/memory numbers of both 8- and lower-bit\\nquantized model are presented to compare with commonly adopted configurations. We note that the\\nquality implication of lower-bit quantization and quantization-aware training is beyond the scope of\\nthis paper.\"}\n{\"section_content\":\"Results\\nResults on EditEval\\nTable 1 summarizes the results. The metrics of the baseline models are directly obtained from the\\nEditEval paper (Dwivedi-Yu et al., 2022). We list only those models whose sizes are similar to our\\non-device models; Nevertheless, our model is substantially smaller than these models. We provide\\nSARI values for each dataset and extra Update-R scores for the two datasets relevant for the paragraph\\nupdate task.\\nThe results show that our on-device model with size XXS outperformed other models on most of\\nthe tasks despite being much smaller. For the fluency, coherence, paraphrase, simplification and\\nparagraph update tasks, our model wins by a large margin. Heuristic reinforcement learning generally\\nboosts the model\\u2019s performance on all tasks.\\nPerformance of the On-device Model\\nTo show that our approach can generally enhance the model\\u2019s rewriting ability, we first report\\nperformance of our SFT model and RL model on EDITEVAL. We then focus on message rewriting\\nand evaluate the same SFT model and RL model on MESSAGEREWRITEEVAL. Finally we present\\nlatency and memory metrics for on-device inference.\\nResults on MESSAGEREWRITEEVAL\\nThe automatic evaluation results for the MESSAGEREWRITEEVAL dataset are shown in Table 2. We\\nfirst examine results of three sets of models: pre-trained LLMs, Instruction-Tuned LLMs and our\\non-device Instruction-Tuned LLMs.\\n8\\nJFL IT RFLU IT RCLA IT RCOH STS TRK AST WNC\\nFRU\\nWFI\\nSARI\\nSARI\\nSARI\\nSARI\\nSARI SARI SARI SARI SARI Update-R SARI Updated-R\\nCopy\\nSize 26.7\\n32.3\\n29.5\\n31.3\\n21.1\\n26.3\\n20.7\\n31.9\\n29.8\\n0\\n33.6\\n-\\nT0++ (Sanh et al., 2022)\\n11B 34.7\\n35.5\\n37.6\\n32.7\\n28.4\\n32.9\\n28.2\\n29.3\\n12.6\\n3.7\\n4.4\\n8.1\\nPEER-11 (Schick et al., 2022)\\n11B 55.8\\n52.1\\n32.5\\n32.7\\n28.2\\n32.1\\n29.5\\n54.5\\n39.6\\n31.4\\n34.9\\n20.4\\nTk (Wang et al., 2022b)\\n3B\\n31.8\\n32.4\\n38.4\\n33.8\\n30.2\\n32.8\\n29.9\\n31.1\\n12.6\\n3.6\\n1.3\\n4.5\\nT0 (Sanh et al., 2022)\\n3B\\n42\\n24.6\\n32.6\\n22.2\\n34.3\\n34.4\\n32.3\\n22.3\\n14.2\\n9.6\\n5.1\\n16.3\\nPEER-3 (Schick et al., 2022)\\n3B\\n55.5\\n51.4\\n32.1\\n32.1\\n28.6\\n32.5\\n30.5\\n53.3\\n39.1\\n30.9\\n34.4\\n18.7\\nPaLM 2 (Passos et al., 2023)\\nS\\n36.07\\n22.68\\n28.79\\n27.82\\n34.45 34.32 35.92 25.2 24.28\\n26.39\\n11.41\\n20.42\\nFlan PaLM 2 (Passos et al., 2023) XS 30.03\\n36.01\\n34.81\\n33.17\\n31.91 34.32 31.4 27.75 15.19\\n5.34\\n6.86\\n3.12\\nFlan PaLM 2 (Passos et al., 2023) XXS 34.43\\n30.12\\n34.08\\n31.32\\n29.25 33.06 35.92 17.5\\n13.6\\n2.75\\n4.78\\n0.97\\nAlpaca PaLM 2\\nXXS 29.33\\n17.01\\n24.42\\n23.81\\n32.59 31.56 33.46 28.11 23.53\\n14.22\\n6.5\\n3.72\\nSFT (Ours)\\nXXS 58.36\\n37.67\\n33.85\\n36.03\\n37.49 38.88 41.95 32.35 35.44\\n47.49\\n22.03\\n32.36\\nSFT + heuristic RL (Ours)\\nXXS 61.1\\n40.26\\n34.81\\n37.33\\n38.25 40.21 41.95 35.28 35.81\\n49.49\\n26.32\\n40.71\\nTable 1: Model Performance on EditEval (Dwivedi-Yu et al., 2022). Only models with reasonable\\nsizes are listed. Size XXS is less than half the size of T0\\/Tk models. Despite their reduced sizes, our\\nmodels achieve even better performance than most of the other larger models. Relative to similar-sized\\ninstruction-tuned models, our models win by a large margin.\\nSize Edit Ratio NLI Reversed NLI SARI BLEU Update-R\\nGiant LLM\\nInsGPT (Ouyang et al., 2022)\\n175B\\n0.18\\n0.91\\n0.88\\n51.14\\n35.0\\n58.91\\nPre-trained LLMs\\nLLaMA (Touvron et al., 2023)\\n7B\\n3.98\\n0.68\\n0.74\\n31.58 16.65\\n29.24\\nPaLM 2 (Passos et al., 2023)\\nXS\\n0.98\\n0.83\\n0.72\\n38.92 22.98\\n37.45\\nPaLM 2 (Passos et al., 2023)\\nXXS\\n1.59\\n0.76\\n0.82\\n31.49 18.81\\n31.85\\nInstruction-Tuned LLMs\\nAlpaca (Taori et al., 2023)\\n7B\\n0.26\\n0.76\\n0.76\\n42.21 24.80\\n45.15\\nVicuna (Chiang et al., 2023)\\n7B\\n1.27\\n0.46\\n0.52\\n38.18 14.30\\n30.17\\nFlan-PaLM 2 (Passos et al., 2023)\\nXS\\n0.11\\n0.94\\n0.83\\n29.50 25.89\\n34.63\\nFlan-PaLM 2 (Passos et al., 2023)\\nXXS\\n0.11\\n0.93\\n0.80\\n27.41 17.59\\n15.43\\nAlpaca-PaLM 2\\nXXS\\n0.3\\n0.84\\n0.78\\n43.14 25.88\\n47.76\\nOur Intruction-Tuned On-device LLMs\\nSFT\\nXXS\\n0.17\\n0.89\\n0.75\\n46.23 27.78\\n51.84\\nSFT + heuristic RL\\nXXS\\n0.16\\n0.93\\n0.85\\n47.14 30.50\\n54.97\\nCascades\\nSFT + heuristic RL + critique distillation XXS\\n0.16\\n0.93\\n0.84\\n48.6\\n32.43\\n55.72\\nOurs + InsGPT (40% server calls)\\n-\\n0.16\\n0.93\\n0.87\\n49.87 34.59\\n58.87\\nOurs + InsGPT (15% server calls)\\n-\\n0.16\\n0.92\\n0.86\\n49.03 33.76\\n57.41\\nTable 2: Model Performance on MESSAGEREWRITEEVAL. Our models achieves best performance\\ncompared with all listed Pre-trained LLMs and Instruction-Tuned LLMs, which have either same or\\nlarger size then ours. When cascaded with InsGPT, the performance is further improved.\\nAs discussed earlier, a larger Edit Ratio does not always correlate with better rewrite performance, as\\nit may result from hallucinations. After conducting human inspection of the results, we observed\\nthat Edit Ratios larger than 0.2 often arise from undesired hallucinations. In terms of SARI, BLEU,\\nand Update-R metrics, our on-device size models outperform LLaMA, Alpaca-7B and Vicuna-7B,\\ndespite having a much smaller size. We also compare our results to Alpaca-PaLM 2 and Flan-PaLM\\n2, which share the same base architecture and model size. The fact that our model achieves much\\nbetter SARI, BLEU, and Update-R scores validates the effectiveness of our approach. Moreover, the\\ngap in performance between the SFT and RL models shows that our heurisic reinforment learning is\\nvery effective.\\nOn-device Metrics\\nTable 3 presents the performance benchmarks of our inference engine on both the Samsung S23\\nand Pixel 7 Pro. These evaluations were conducted using 1024 input tokens and decoding over\\n100 tokens. Results for both 8-bit and 4-bit quantized models are provided. It is noteworthy that,\\non the S23, the mean latency per token during the prompt parsing phase is 1.2ms (equivalent to\\n>800 tokens\\/second), with the decoding latency being 35ms (29 tokens\\/second). To the best of our\\n9\\nOn-device Inference Ratio\\nSuccess Rate\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\n0\\n0.25\\n0.5\\n0.75\\n1\\nsuffix score, s=1\\nsuffix score, s=8\\nLM score,s=1\\nLM score,s=8\\n(a) Suffix score VS LM score for paraphrase, with\\nsample 1 or 8.\\nOn-device Inference Ratio\\nSuccess Rate\\n0.75\\n0.8\\n0.85\\n0.9\\n0.95\\n1\\n0\\n0.25\\n0.5\\n0.75\\n1\\nFormal\\nShorten\\nElaborate\\nParaphrase\\nProofread\\n(b) Evaluation of the cascades across different\\nrewrite tasks.\\nFigure 3: Cascading our model with InsGPT on Rewriting Tasks. Figure (a) demonstrates the benefit\\nof using suffix score for cascading. Suffix score obtains better judgement for output quality than LM\\nscore when both decode with 1 sample; As a result, this capability could be further strengthened\\nwhen decoding with multiple samples and choosing the one with highest score amongst the samples.\\nFigure (b) evaluates the success rate with cascading across different rewrite types. The curves indicate\\nthat formalizing, shortening and proofreading task obtain a smaller from cascading compared to\\nparaphrasing and elaborating. With around 25% server calls on the latter two tones, the success rate\\ncan be improved from 83% to 95%.\\nS23\\nPixel 7\\n8-bits\\n4-bits\\n8-bits\\n4-bits\\nP. Parsing (ms)\\n1.2\\n1.2\\n4.2\\n4.2\\nDecoding (ms)\\n48.0\\n35.0\\n67.7\\n55.6\\nMemory (Gb)\\n1.6\\n0.9\\n1.7\\n1.0\\nTable 3: Benchmark results of our model. The average latency per token for the prompt parsing and\\ndecoding phases are reported in milliseconds. The last row shows the total memory consumption in\\ngigabytes.\\nknowledge, the latency of our model on a cell phone is greatly faster than the reported numbers (i.e.\\n18 - 22 tokens\\/second) benchmarked on Macbook M1 Pro 32GB Ram for a 7B Llama model with\\n4-bits quantization (Gerganov, 2023).\\nPerformance of Cascading\\nOur cascading experiments are conducted on the top of the on-device model with RL using MES-\\nSAGEREWRITEEVAL benchmark. Here we choose it over EDITEVAL for cascading experiments as it\\nis more aligned with the mobile cases. We first evaluate how the critique distillation is impacting\\nthe model\\u2019s performance. Next we show the end-to-end cascading performance as a function of\\nsampled thresholds \\u03b3. We then present a detailed analysis and demonstrate that our suffix score is\\nmore effective than the baseline LM score. Finally we look into the performance for subcategories.\\nThe Effect of Critique Distillation\\nIn Table 2 we show that the model\\u2019s overall performance is further improved on SARI, BLEU, and\\nUpdate-R with little regression on Reversed NLI when we combine the distilled discriminative dataset\\nwith the generative dataset. This suggests that with the suffix score from critique distillation, the\\nmodel tends to pick sample with higher quality.\\nEnd-to-end Performance\\nThe on-device model\\u2019s reliance on the server model is controlled by the threshold \\u03b3. As shown in\\nTable 2, the performance of the cascaded models lies between the on-device and the server side model.\\n10\\nWith a higher number of server calls, we obtain higher SARI, BLEU, and Update-R, as expected.\\nWith 40% server calls, the overall performance is already quite close to the full server model.\\nSuffix Score vs LM Score\\nWe now provide more insight into our cascading approach with suffix score. We vary the threshold \\u03b3\\nfrom 0 to 1 to measure Success Rate as a function of the On-device Inference Ratio. The trade-off\\nbetween the two metrics is shown in Figure 3. To demonstrate the efficacy of the distilled suffix score\\nderived from larger LLM critiques as a reliable indicator of output quality, we compare it with an LM\\nscore, representing the likelihood of the generated text. As shown in Figure 3a, \\u201csuffix score with 1\\nsample\\u201d is outperforming \\u201cLM score with 1 sample\\u201d by large margin. This indicates that given a text\\noutput, suffix score offers higher quality estimates. As a result, when sampling multiple outputs (8\\nsamples), suffix score can accurately select the decoded candidate with the highest quality, which\\ngreatly improves performance. In contrast, the LM score stays almost unchanged when increasing\\nthe number of samples, showing that it is less helpful.\\nVariation across different Sub-categories\\nWe also compare the cascading effects for different rewriting sub-categories in Fig 3b. The figure\\nsuggests that cascading may help paraphrase and elaborate tasks more than proofread, shorten and\\nformalize. More importantly, around 25% server calls can improve its success rate from 83% to 95%.\"}\n\n... # only display 2 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: From the retrieved sections, both papers mention the same set of baseline models used for comparison on the EditEval benchmark. I will extract the model names and identify the common models.\n[Action]:\nGenerateAnswer(answer=['PaLM 2', 'LLaMA', 'Alpaca', 'Vicuna', 'Flan-PaLM 2', 'Alpaca-PaLM 2', 'InsGPT'])"}
{"role": "user", "content": "[Observation]: ['PaLM 2', 'LLaMA', 'Alpaca', 'Vicuna', 'Flan-PaLM 2', 'Alpaca-PaLM 2', 'InsGPT']"}
