{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: Which implicit bias mentioned in the paper \"Benchmarking Cognitive Biases in Large Language Models as Evaluators\" is not addressed in the works that directly overlap with the paper, particularly those exploring LLMs' capabilities as evaluators?\n[Answer Format]: Your answer should be a string, the name of the implicit bias as mentioned in the anchod PDF.\n[Anchor PDF]: '3d218d94-1aa0-5a70-b23e-accb254141bd'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to retrieve the content of the paper with the specified paper_id from the `metadata` table to get the title and abstract. Then, I can perform a similarity search in the vectorstore to find relevant sections that discuss the implicit biases mentioned in the paper. Finally, I will analyze the retrieved sections to identify the implicit bias that is not addressed in the overlapping works.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT title, abstract FROM metadata WHERE paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd';\")"}
{"role": "user", "content": "[Observation]:\n{\"title\":\"Benchmarking Cognitive Biases in Large Language Models as Evaluators\",\"abstract\":\"Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (40% of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: Now that I have the title and abstract of the paper, I can use this information to perform a similarity search in the vectorstore to find relevant sections that discuss the implicit biases mentioned in the paper. I will use the `text_sentence_transformers_all_minilm_l6_v2` collection for this purpose.\n[Action]:\nClassicRetrieve(query='Benchmarking Cognitive Biases in Large Language Models as Evaluators', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7914,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"799faff1-c0d7-5671-907c-3084e47e7e5d\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2024, pages 517\\u2013545\\nAugust 11-16, 2024 \\u00a92024 Association for Computational Linguistics\\nBenchmarking Cognitive Biases in Large Language Models as Evaluators\\nRyan Koo1 Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung Kim1 Dongyeop Kang1\\n1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu,\\nvipul.raheja@grammarly.com\\nAbstract\\nLarge Language Models (LLMs) have recently\"}\n{\"score\":0.758,\"pdf_id\":\"aa34c456-045a-5603-ae30-b84dac9ca2fe\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"31534f6e-e177-5d10-855a-7a56b81a6e0e\",\"text\":\"In this section, we evaluate our debiased models against the General Language Understanding Eval-\\nuation (GLUE) benchmark (Wang et al., 2018) to evaluate whether language models retain their\\ngeneral linguistic understanding ability after bias mitigation. As the GLUE benchmark results indi-\\ncate (Table 9), FAST achieves better downstream performance than 5 out of 6 baselines on average,\\nwhich indicates that FAST can mitigate the bias while also maintaining language modeling capabil-\\nity.\\nD.2\"}\n{\"score\":0.7553,\"pdf_id\":\"5d4f6d45-c725-5e01-a9ce-59dc0b7db79f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"efe180c0-0620-5bd7-812f-bfd0f41f2d38\",\"text\":\"preprint arXiv:1902.03680, 2019.\\n474\\nJacob-Junqi Tian, D. Emerson, Deval Pandya, Laleh Seyyed-Kalantari, and Faiza Khattak. Effi-\\n475\\ncient evaluation of bias in large language models through prompt tuning. In Proceedings of the\\n476\\nConference on Neural Information Processing Systems, 2023. URL https:\\/\\/openreview.net\\/\\n477\\nforum?id=v1WL01lgp8.\\n478\\nXiaogang Wang, Zhen Xu, Dongkuan Yang, Lai Tam, Holger Roth, and Dong Xu. Learning image\\n479\"}\n{\"score\":0.7517,\"pdf_id\":\"9e61daf4-6c9c-5260-b2a0-4a14bc8b11eb\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"89677014-c311-5ecf-9783-1dadf116b59b\",\"text\":\"A\\nAppendix\\nA.1\\nRelated Work\\nA.1.1\\nFairness Benchmark for Large Language Model\\nInternal Evaluation.\\nConsiderable research has been conducted on the evaluation of bias in\\nlanguage models, particularly through the lens of embeddings. One way to assess biases between\\ndemographic groups is by comparing the cosine similarities of their word embeddings [19, 34, 11].\\nAnother approach measures bias through the probabilities assigned by language models [15, 2]. For\"}\n{\"score\":0.7421,\"pdf_id\":\"72428886-f79d-5d21-bdce-d0b90a2edabe\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f806ebe7-17e4-5359-8ef6-b5487e86c5c6\",\"text\":\"for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP), pp. 2567\\u20132577, 2019.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop\\nKang. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint\\narXiv:2309.17012, 2023.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results include the abstract and some relevant sections from the paper. I need to focus on the sections that discuss the implicit biases mentioned in the paper. I will perform another similarity search specifically targeting the sections that discuss the biases.\n[Action]:\nClassicRetrieve(query='Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer)', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.8205,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"65f99797-980a-51e0-b782-add4cab12bc1\",\"text\":\"analyzed and the creation of an open benchmark.\\n3\\nCOBBLER: Cognitive Bias Benchmark\\nfor LLMs as Evaluators\\nThe following criteria are used to select each type\\nof evaluation bias:\\n\\u2022 General Applicability. Text evaluation tasks\\nshould be generalizable to most prompting sce-\\nnarios; tasks that observe too specific subtleties\\nwithin the prompt are not helpful.\\n\\u2022 Impartiality. The prompt should not involve\\nany leading statements to extract some desired\\nquality of the evaluations\"}\n{\"score\":0.8044,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"83909a25-dfef-5ae3-a2c2-c80852ddb6de\",\"text\":\"models and an overall different focus on QA as\\nopposed to other domains such as math and rea-\\nson. Furthermore, our benchmark emphasizes a\\nwider range of biases (implicit\\/induced) to better\\ndescribe machine artifacts when used as automatic\\nevaluators. Specifically, COBBLER measures the\\nextent to which each LM-as-evaluator is impacted\\nin each decision by certain artifacts within prompts\\n(i.e., prompting format, prompt information) over\\na comprehensive list of cognitive biases.\\nCognitive Biases in LLMs.\"}\n{\"score\":0.7658,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"711f1da6-c85f-5395-b38b-4ed27a1b95c8\",\"text\":\"similar to the evaluators\\u2019 generation qualities.\\nIn this work, we propose COBBLER, the\\nCOgnitive Bias Benchmark for evaluating the qual-\\nity and reliability of LLMs as EvaluatoRs, as de-\\npicted in Figure 1. We collect a set of 50 question-\\nanswering instructions from two well-established\\nbenchmarking datasets: BIGBENCH (Srivastava\\net al., 2023) and ELI5 (Fan et al., 2019).\\nWe\\nthen generate responses from 16 open- and closed-\\nsource LLMs and conduct a round-robin over every\"}\n{\"score\":0.7431,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":9,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1e7403f4-1456-542f-be4c-0d0ca94d2f84\",\"text\":\"exhibit cognitive biases to a greater extent than hu-\\nmans, suggesting that LLMs are still unsuitable as\\nfair and reliable automatic evaluators. In the future,\\npotential de-biasing methods provide another area\\nof interest in reducing each bias. For example, tech-\\nniques such as chain-of-thought (CoT) reasoning or\\nother alignment methods can perhaps be employed\\nto reduce the bias for current models.\\n525\"}\n{\"score\":0.7204,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f434edd2-adce-5921-a308-f4c1916c6a10\",\"text\":\"\\u2022 An examination of an exhaustive list of 6 (cog-\\nnitive) evaluation biases that have not been cov-\\nered by previous studies. We find that most\\nLLMs cannot perform as unbiased evaluators.\\n\\u2022 A comprehensive lineup of models (sizing\\nfrom 3B to >175B parameters) as evaluators,\\nencompassing the current state-of-the-art lan-\\nguage models covering over 630k comparisons.\\nFrom our benchmark, we find that most models\\nexhibit various cognitive biases when used as au-\\ntomatic evaluators, which may negatively impact\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results provide more detailed information about the cognitive biases mentioned in the paper. I will now analyze these sections to identify the specific biases discussed and determine which one is not addressed in the overlapping works.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_title LIKE '%Cognitive Biases%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Ryan $\\\\mathbf{Koo^{1}}$ Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung ${\\\\bf K i m^{1}}$ Dongyeop Kang1 1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu, vipul.raheja@grammarly.com\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section content does not provide the detailed list of cognitive biases. I will try another query to get the specific section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_title LIKE '%COBBLER%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"The following criteria are used to select each type of evaluation bias:\\n\\u2022 General Applicability. Text evaluation tasks should be generalizable to most prompting scenarios; tasks that observe too specifc subtleties within the prompt are not helpful. \\u2022 Impartiality. The prompt should not involve any leading statements to extract some desired quality of the evaluations \\u2022 Memorylessness. The current evaluation instance should not rely on any previous behaviors. Each instance should be self-contained when extracting each bias metric.\\nWe carefully hand-select these biases based on the above three criteria such that they can be widely applicable to most evaluation settings in assessing the performance of LLMs as automatic evaluators.\\nTable 1 summarizes defnitions of each bias type along with examples in COBBLER. We categorize our benchmark into two main classes: (1) Implicit and (2) Induced Biases. For implicit biases, we feed a general prompt that shows system outputs in a pairwise manner to extract any biased behaviors within the model\\u2019s evaluations implicitly. For induced biases, we feed prompts geared towards each different bias, similar to adversarial attacks, such as presenting false information that may infuence evaluator behaviors in a certain manner. Hence, we note that criterion 2 is not entirely fulflled due to the nature of induced biases, though they can still be generally observable in an evaluation setting.\\nand other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\\nSalience Bias (Length). (Schenk, 2010; Zheng et al., 2023) The evaluator tends to favor responses that are either shorter or longer in length. An unbiased evaluator would be split evenly between responses that are shorter or longer in length. We examine this bias by looking at evaluations in which a model preferred a response that is either shorter or longer in token length.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section content provides a summary of the biases but does not list all the specific biases. I will perform another similarity search to find the section that lists all the cognitive biases.\n[Action]:\nClassicRetrieve(query='List of cognitive biases in COBBLER', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6562,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"83909a25-dfef-5ae3-a2c2-c80852ddb6de\",\"text\":\"models and an overall different focus on QA as\\nopposed to other domains such as math and rea-\\nson. Furthermore, our benchmark emphasizes a\\nwider range of biases (implicit\\/induced) to better\\ndescribe machine artifacts when used as automatic\\nevaluators. Specifically, COBBLER measures the\\nextent to which each LM-as-evaluator is impacted\\nin each decision by certain artifacts within prompts\\n(i.e., prompting format, prompt information) over\\na comprehensive list of cognitive biases.\\nCognitive Biases in LLMs.\"}\n{\"score\":0.5993,\"pdf_id\":\"15a69f5b-6834-5449-ab00-161dde83e167\",\"page_number\":30,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b784df0e-9f9a-5180-b4d2-57b7987c1989\",\"text\":\"[63]. Conversely, the most adept human solvers necessitate approximately 50 moves to\\nachieve the same outcome.\\n\\u2022 Examples. Please refer to Tab. 17 for examples.\\nCognitive biases of human:\\n\\u2022 Source. Tversky and Kahneman [80] define human cognitive biases: systematic patterns of\\ndeviation from rational judgment. Here we list three types of cognitive biases of human.\\n30\"}\n{\"score\":0.5971,\"pdf_id\":\"796966dc-afdc-5052-9901-cd3494f37e9f\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"5aadae3e-9381-5dfc-af7a-2a61bde2830d\",\"text\":\"Cognitive biases describe the flawed human re-\\nsponse patterns for decision making under uncer-\\ntainty (Tversky and Kahneman, 1974, 1981; Ja-\\ncowitz and Kahneman, 1995; Kahneman and Fred-\\nerick, 2002; Meyer, 2004). For example, when\\npeople are biased by the availability heuristic, they\\nmake probability judgments based on the ease with\\nwhich information comes to mind (Tversky and\\nKahneman, 1973). Knowing cognitive biases can\\nhelp predict what types of error will be made,\"}\n{\"score\":0.5925,\"pdf_id\":\"b2bb7582-ee27-57be-8626-63ecca454a2c\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"33d0c7a1-86d6-5b97-bae3-10a9c926cc7f\",\"text\":\"and associated cognitive errors. Echterhoff et al. [2024] introduced a framework designed to reveal,\\nevaluate, and mitigate a variety of cognitive bias in LLMs in high-stakes decision-making tasks.\\nWhile their findings aligned with previous studies demonstrating the presence of cognitive biases, they\\nwere able to effectively mitigate them, resulting in more consistent decisions. Ultimately, Wang et al.\\n[2024] proved that certain cognitive biases, when properly balanced, can improve decision-making\"}\n{\"score\":0.5844,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1712916e-c833-5b1f-8b7b-037e2a8112b4\",\"text\":\"Table 1 summarizes definitions of each bias type\\nalong with examples in COBBLER. We categorize\\nour benchmark into two main classes: (1) Implicit\\nand (2) Induced Biases. For implicit biases, we\\nfeed a general prompt that shows system outputs in\\na pairwise manner to extract any biased behaviors\\nwithin the model\\u2019s evaluations implicitly. For in-\\nduced biases, we feed prompts geared towards each\\ndifferent bias, similar to adversarial attacks, such\\nas presenting false information that may influence\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results include a section that summarizes the definitions and examples of the cognitive biases in COBBLER. I will extract the specific list of biases from this section.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_title LIKE '%Table 1%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the section title might not be exactly \"Table 1\". I will try a more general query to find the section that lists the cognitive biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"The following criteria are used to select each type of evaluation bias:\\n\\u2022 General Applicability. Text evaluation tasks should be generalizable to most prompting scenarios; tasks that observe too specifc subtleties within the prompt are not helpful. \\u2022 Impartiality. The prompt should not involve any leading statements to extract some desired quality of the evaluations \\u2022 Memorylessness. The current evaluation instance should not rely on any previous behaviors. Each instance should be self-contained when extracting each bias metric.\\nWe carefully hand-select these biases based on the above three criteria such that they can be widely applicable to most evaluation settings in assessing the performance of LLMs as automatic evaluators.\\nTable 1 summarizes defnitions of each bias type along with examples in COBBLER. We categorize our benchmark into two main classes: (1) Implicit and (2) Induced Biases. For implicit biases, we feed a general prompt that shows system outputs in a pairwise manner to extract any biased behaviors within the model\\u2019s evaluations implicitly. For induced biases, we feed prompts geared towards each different bias, similar to adversarial attacks, such as presenting false information that may infuence evaluator behaviors in a certain manner. Hence, we note that criterion 2 is not entirely fulflled due to the nature of induced biases, though they can still be generally observable in an evaluation setting.\\nand other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\\nSalience Bias (Length). (Schenk, 2010; Zheng et al., 2023) The evaluator tends to favor responses that are either shorter or longer in length. An unbiased evaluator would be split evenly between responses that are shorter or longer in length. We examine this bias by looking at evaluations in which a model preferred a response that is either shorter or longer in token length.\"}\n{\"section_content\":\"Implicit Biases We frst examine the performance of each evaluator on the implicit bias benchmarks for ORDER BIAS, COMPASSION FADE, SALIENCE BIAS and EGOCENTRIC BIAS. For the ORDER BIAS benchmark in Table 2, we observe that most models (11\\/15) tend to be drawn towards either the frst- or last-ordered model in each of the pairwise comparisons. Notably, within the second size group $(>\\\\!40B)$ , the frst-ordered system was strongly favored in over $50\\\\%$ .\\nFor COMPASSION FADE, since it is diffcult to interpret its impact by the metrics independently, we jointly compare the results with the ones from ORDER BIAS. For an unbiased evaluator that is not infuenced by identifable names, we expect the results for COMPASSION FADE to be relatively similar to the ORDER BIAS benchmark. However, we see in Table 2 that all models are dramatically infuenced by real model names. Although this phenomenon may be akin to injecting random names, the disparity between ORDER and COMPASSION FADE results support our hypothesis that recognizable names infuence evaluations in contrast to anonymized ones. In addition, we also note that OLMO sees a drastic decrease in performance. This might be attributed to the model\\u2019s inability to follow more complex instructions from its training.\\n\\nFor EGOCENTRIC BIAS, in the anonymized aliases, the largest models as well as KOALA tend to prefer their own responses $(>50\\\\%)$ with the exception of INSTRUCTGPT. However, with real model names (COMPASSION), we see a large drop in self-preference for models in the largest size group $(>\\\\!100B)$ models, but this may be attributed to a large increase in bias for each position. On average, we see an increase in self-preference with real model names amongst the two smaller size groups, notably KOALA sees a $100\\\\%$ increase in preference.\\nFor SALIENCE BIAS, we observe that the larger models in the frst and second size groups are more strongly affected by longer responses, which align with fndings from other works (Wu and Aji, 2023; Zheng et al., 2023). However, smaller models (excluding MPT) tend to be less infuenced by the length of the responses, suggesting that smaller models in the third and fourth size groups are less impacted in their evaluations in consideration of the length of the text.\\nFor models such as ChatGPT, the EGOCENTRIC BIAS may be unfair because their generations are indeed better, or in SALIENCE, the longer generations indeed have higher quality. For further insight in decoupling these factors, we include supplementary experiments viewed in Appendix B.\\n5.1.1 Identifying Egocentric and Salience Bias We also discuss the evaluation criteria for identifying EGOCENTRIC and SALIENCE biases, which may be more appropriately evaluated conditioned on underlying generation quality and model size.\\nWe select a few model representative models for clarity viewed in Table 3. Generally, most models stay consistent with their preference for longer\\/shorter responses conditioned on either generation\\u2019s quality, although some fip their preferences (to only a small effect however). For further insight, we compute the generation quality using reference-based metrics via BERTScore. From this, all models produce nearly the same quality of generations with respect to the reference answer $_{\\\\sim0.81}$ to 0.86 for F1), highlighting that identifying EGOCENTRIC or SALIENCE bias is most likely not dependent on generation quality.\\nInduced Biases Next, we evaluate the performance of each evaluator on the induced bias benchmarks: BANDWAGON EFFECT and ATTENTIONAL BIAS. For BANDWAGON EFFECT, we observe that almost all models (11\\/15) are heavily infuenced in which $>70\\\\%$ of evaluations on average followed the bandwagon preference regardless of text quality. Although we only included a simple fake statistic (e.g. $85\\\\%$ of people preferred \\u201cSystem Star\\u201c), we see that evaluators can be heavily infuenced by this external information. To observe a correlation between the biased tendency and the percentage, we include additional results in Appendix B.1\\n\\nFor ATTENTIONAL BIAS, we see that around half of the models\\u2019 rankings are infuenced by irrelevant information. Specifcally, we see that models in the third size group $(>\\\\!10B)$ were the most strongly impacted by the distracting information, with $>80\\\\%$ of evaluations being counted as distracted. On the other hand, API-based models such as CHATGPT and COHERE remained robust against these distractions in their rankings. We include the list of distractions we use in Appendix C.\\nLastly, we address specifc models such as LLAMAV2, LLAMA, KOALA, and OLMO that show abnormal results on most of the benchmarks. This can be attributed to their low valid response rates, displayed in Table 12 in Appendix B, which may be explained by our prompting format or the capabilities of the model themselves, likely as they are not instruction-tuned. Although these models display lower performance when extracting evaluations, if a model is not strong enough to produce valid outputs, we assume those models are not strong enough to be used for evaluations. And as we don\\u2019t consider invalid responses within the study, we only apply our fndings to ones that produced valid evaluations, in which most models exhibit cognitive biases from our benchmark. Although the correlation between valid response rates and bias can provide more insight into model capabilities, it is not within the scope of our fndings.\"}\n{\"section_content\":\"We particularly focus on decoupling EGOCENTRIC and Salience, which are the most prone to having large correlations with each other (i.e. longer generations may indeed have overall higher quality generated by much stronger models).We highlight two important aspects regarding the identifcation of these biases:\\n\\u2022 If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest \\u201cegocentric\\u201d qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 & CHATGPT) that suggest the presence of EGOCENTRIC evaluations from their disagreement.\\n\\u2022 We employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \\u201chierarchical\\u201d rubric, where some biases take priority in an evaluation. Specifcally, if an evaluation shows signs of order bias by choosing A in (A frst, then B) and B in (B frst, then A), we do not evaluate it for SALIENCE or EGOCENTRIC bias.\\nTo get further insight into decoupling them, we examine additional statistics in Table 11 displaying the proportion of EGOCENTRIC samples where the model\\u2019s generation was longer\\/shorter than the other generation. In particular, since OLMO only won once, and LLAMA never won, their EGOCENTRIC ratios look weird. Otherwise, we view overall that most models (9\\/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length.\\nAs above, we see that SALIENCE may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Nevertheless, even in smaller models (e.g., Cohere, Koala), preference for their own generations occurs more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is diffcult to associate with SALIENCE as there is disagreement that is indicative of an EGOCENTRIC bias.\"}\n{\"section_content\":\"We adapt two statistical hypothesis tests based on the random bias threshold for the Order bias (frstorder and last-order) benchmark in Table 10. Since we have binary outputs (bias, not biased), we conduct a two-sample $Z$ test of proportions to determine the signifcance of each proportion of biased evaluations from each automatic evaluator with the random baseline. We conduct the test with the null hypothesis defned to be that \\u201cevaluator $\\\\Chi$ is just as likely to make the mistake of fipping its preference according to the order of the response to the frst-order as the random baseline\\u201d or equivalently:\\n$H_{0}$ : the mean of Evaluator $X$ for frst-order bias is not any different from random selection.\\nOn almost all of the ORDER benchmarks, the proportions of biased evaluations are statistically signifcant from ones by the random score. Notably, the p-values are critically small ( $\\\\mathbf{Z}$ -scores are blown up) due to our large sample size. Also, we note that the p-value is actually not statistically significant for last-order bias in InstructGPT; however, the frst-order proportions are statistically signifcant, indicating that one must consider the test for both positions to get the full picture of the evaluator\\u2019s tendencies in reference to the random baseline. For example, if both frst-order and last-order were not statistically signifcant from the random proportions, we might fnd that the evaluator is \\u201cunbiased,\\u201d but the following may also undermine the capabilities of the automatic evaluator reduced to just random choice.\"}\n{\"section_content\":\"We detail the general agreement between machine preferences as similarly conducted in the humanmachine correlation study. Figure 5 visualizes the average Rank-Based Overlap between LLMs. We fnd that LLMs in their own size group (excluding the smallest size group) have a relative agreement with each other. For example, models in the largest size group $(>\\\\!100B)$ are more in agreement amongst themselves than with models from other size groups. Furthermore, we also show the average valid response rate from different bias promptings in Table 12. We gather the proportion of valid responses by post-processing each \\u201ceval-gen\\u201d via pattern matching. After post-processing, we then label each output as a valid or invalid response, such that if a response is valid, we give one point to the preferred system.\"}\n{\"section_content\":\"We conduct a supplementary experiment analyzing the impact of each bias for different models scaled by size in Table 13. We present results from a range of model sizes with LLAMAV2 and VI\\nCUNA. Interestingly, we see that the valid response rate within LLAMAV2 goes down as the model size is scaled up, but the impact of each bias greatly increases as the model size is scaled down (with the exception of SALIENCE BIAS). On the implicit bias benchmarks, LLAMAV2 exhibits more robust performance with the proportion of responses affected by each bias SALIENCE BIAS in which longer responses are much more strongly preferred. For the induced bias benchmarks, a similar trend is viewed in which the effect of each bias on the model as an evaluator is dampened in correlation to the model scale. On the contrary, VICUNA exhibits a stronger valid response rate as the model size is scaled; however, certain implicit biases are much more amplifed, such as ORDER BIAS and SALIENCE BIAS. For implicit biases, VICUNA tends to prefer itself when actual model names are used as size is scaled smaller while tending to prefer much more verbose responses as model size is scaled higher. Across the induced biases, VICUNA performs more resiliently proportionally to scale, although still strongly infuenced by BANDWAGON EFFECT but much less affected by ATTENTIONAL BIAS. We include another visualization correlating the overall performance on each of the bias benchmarks with model size for the main results in Figure 2a.\"}\n{\"section_content\":\"For the experimental setup, we conduct a smaller study, generating 100 responses from each of the 4 different LLMs using the Databricks Dolly $15\\\\mathrm{k}$ dataset (Conover et al., 2023) via the same instruction prompt template from Appendix C and the same evaluation prompt template from the ORDER bias.\\nWe only employ this setting under the order bias setting in order to validate the complexity of the task that modern (smaller) LLMs aren\\u2019t capable of performing yet. We perform each experiment by randomizing the order of each list of responses and prompt each LM-as-evaluator to order the list from best to worst (top to bottom) according to the same criterion as the pairwise study (providing the instruction\\/sample reference). Furthermore, we also track ORDER bias, calculated by the proportion of responses in which the frst (randomly) placed model was also ranked frst by the evaluator.\\nAs viewed in Table 14, we fnd that most models besides the closed-source API models (e.g. OpenAI) have trouble generating a proper rank list for even an $N=4$ setting. This may be due to the increased complexity of the task (Dziri et al., 2023) where the ranking of $N$ generations may become much more diffcult as $N$ gets larger (since the task complexity increases).\\nTable 11: Additional comparisons examining the proportion of EGOCENTRIC samples where the (self-preferred) model\\u2019s generation was longer\\/shorter than the other generation.\\nTable 12: Ratio for generating valid evaluations. Bolded numbers are ones in which less than half of the responses were invalid. We conjecture it may be due to lack of instruction-tuning that results in poor ability to follow instructions properly (often repeating the prompt itself, or printing out continuations of model answers).\\nTable 13: Performance comparison in proportion to their model scale. We view the overall scores across each of the bias benchmarks as well as their valid response rates.\\nTable 14: We show the results of instructing models to perform a list-wise evaluation, by prompting each LM-asevaluator to organize a list of responses from 4 different models top to bottom with the frst being the best response and the last being the worst response. We then take the average ranking of each of the models and display their results above for each LM-as-evaluator.\"}\n{\"section_content\":\"### Instruction: Which system\\u2019s   \\nresponse is more coherent considering   \\nthe reference and instruction?   \\nThe instruction is: {{instruction}}   \\nThe reference is: {{reference}}   \\n\\u201c\\u2018   \\nSystem Star:   \\nSystem Square:   \\n\\u201c\\u2018   \\nPlease respond in the following   \\nformat strictly: System _ is better   \\nDo not provide any additional text or   \\nexplanation:   \\n### Response:\\nAlso, we underline that EGOCENTRIC bias was extracted under two dimensions, hence its double column for (ORDER \\/ COMPASSION) in Table 2. Since EGOCENTRIC is a bias extracted implicitly, it involves two different prompts from C.1 (Order) and C.2 (Compassion Fade). In the case of C.1, since model names are completely anonymous, the addition of \\u201c(You)\\u201d is omitted (as shown in Table 1, and models cannot tell if they are self-evaluating. In contrast, C.2 shows recognizable names; thus, \\u201c(You)\\u201d is appended to the system name when selfevaluating. We highlight that the added \\u201c(You)\\u201d within the EGOCENTRIC example in Table 1 serves to explicitly communicate an example of an egocentric bias to the reader and not necessarily the prompt itself.\"}\n\nIn total, 8 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections provide detailed information about the cognitive biases, but I need to identify the specific biases mentioned and which one is not addressed in the overlapping works. I will extract the list of biases from the relevant section.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1 summarizes definitions of each bias type%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the section content might not be exactly as expected. I will try a more general query to find the section that lists the cognitive biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1 summarizes%definitions of each bias type%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query is still not returning any results, so I will try a broader search to find the section that lists the cognitive biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Cognitive Biases in LLMs%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"LLMs as Evaluators. Owing to the effectiveness of LLMs, many recent research works have investigated their utility in various downstream tasks, such as machine translation (Kocmi and Federmann, 2023), summarization (Shen et al., 2023; Gao et al., 2023), code generation (Zhuo, 2023), writing assistance (Schick et al., 2023; Raheja et al., 2023), factual consistency (Cohen et al., 2023; Gekhman et al., 2023; Luo et al., 2023), and more. Additionally, many studies have leveraged LLMs for general-purpose NLG evaluation. For instance, Liu et al. (2023); Chen et al. (2023); Wang et al. (2023a) investigated the effectiveness of GPT-4 and ChatGPT against reference-free evaluation methods, whereas Fu et al. (2023) proposed an evaluation framework, GPTSCORE, to score generated texts. Recently, Li et al. (2023a) and Zheng et al. (2023) conducted similar experiments by employing LLMs as evaluators to judge the quality of generations in a pairwise setting. Although these works present promising results for LLMs as automatic evaluators, our work takes a closer look at machine artifacts that could be detrimental to data quality by benchmarking an exhaustive list of biases impacting LLMs-as-evaluators.\\nLLM Evaluation Benchmarks. It is becoming increasingly challenging to evaluate open-source LLMs as they become more powerful and performant. As a result, there has been an increasing need to develop better evaluation benchmarks for measuring the performance of LLMs. However, most of these benchmarks, such as LM-EVALHARNESS (Gao et al., 2021), MMLU (Hendrycks et al., 2021), HELM (Liang et al., 2022) and BIG\\nBENCH (Srivastava et al., 2023), only focus on general LLM performance but do not explore their capabilities as evaluators. Our work in this direction overlaps directly with Bai et al. (2023) and Zheng et al. (2023), who propose a Language-Model-asan-Examiner benchmark and LLM-as-a-judge to study the capability of LLMs to emulate human preferences. While our experimental setups are similar, we highlight key differences. We cover a wider demographic of current popular language models and an overall different focus on QA as opposed to other domains such as math and reason. Furthermore, our benchmark emphasizes a wider range of biases (implicit\\/induced) to better describe machine artifacts when used as automatic evaluators. Specifcally, COBBLER measures the extent to which each LM-as-evaluator is impacted in each decision by certain artifacts within prompts (i.e., prompting format, prompt information) over a comprehensive list of cognitive biases.\\nCognitive Biases in LLMs. While biases have been well-known to exist in LLMs (Wang et al., 2023b; Talboy and Fuller, 2023; Wu and Aji, 2023), many recent works investigating the behaviors of LLMs have also uncovered similarities with cognitive biases. Some recent works (Zhao et al., 2021; Liu et al., 2022; Lu et al., 2022) have shown that the order of training examples in GPT-3 could lead to differences in accuracy between near chance and near state-of-the-art. Jones and Steinhardt (2022) captured failures in GPT-3 and Codex and found that error patterns of LLMs resemble cognitive biases in humans. Our work overlaps with these in some of the biases we cover, but we present a much more holistic and comprehensive evaluation of LLMs. Along this aspect, while our work is close to Wu and Aji (2023), who investigate biases related to fabricated factual and grammatical errors, our work is much more comprehensive in terms of the number of LLMs analyzed, the types of biases analyzed and the creation of an open benchmark.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section does not contain the specific list of biases. I will perform a similarity search in the vectorstore to find the relevant context.\n[Action]:\nClassicRetrieve(query=\"List of cognitive biases in LLMs as evaluators in the paper 'Benchmarking Cognitive Biases in Large Language Models as Evaluators'\", limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7841,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"415e61b1-aad6-568b-9949-8c52095bc0dc\",\"text\":\"ferent cognitive biases in LLM evaluation out-\\nputs, such as the EGOCENTRIC bias where a\\nmodel prefers to rank its own outputs highly\\nin evaluation. We find that LLMs are biased\\ntext quality evaluators, exhibiting strong indica-\\ntions on our bias benchmark (\\u224840% of com-\\nparisons made by all models) within each of\\ntheir evaluations that question their robustness\\nas evaluators. Furthermore, we examine the\\ncorrelation between human and machine pref-\\nerences and calculate the average Rank-Biased\"}\n{\"score\":0.7831,\"pdf_id\":\"bdd90971-ecf1-5dcc-87cf-3b4a75ad4b01\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cb0b7e66-a8c2-52b2-a4bd-4a6c0fd31f2e\",\"text\":\"Sophia Yang, Szymon Antoniak, Teven Le Scao,\\nTh\\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimoth\\u00e9e Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.\\nShuyu Jiang, Xingshu Chen, and Rui Tang. 2023.\\nPrompt packer: Deceiving llms through composi-\\ntional instruction with hidden attacks.\\nDaniel Kahneman. 2011.\\nThinking, fast and slow.\\nmacmillan.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\\nZae Myung Kim, and Dongyeop Kang. 2023. Bench-\\nmarking cognitive biases in large language models as\"}\n{\"score\":0.7675,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"94c66e14-12c7-5b64-86f9-716a0fb9b71d\",\"text\":\"value-aligned language models make biased decisions that reflect these implicit biases. Using a\\none-sample t-test to compare bias scores against the unbiased 50% baseline, again, we find that on\\naverage LLMs were statistically significantly more likely to make biased decisions that disadvantage\\nmarginalized groups t(26, 528) = 36.25, p < .001. We also observe that decision biases are not as\\nstrong as implicit biases, partially due to larger variances in decisions (Figure 3).\"}\n{\"score\":0.7601,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"213bb150-186d-5f1e-a7e0-1dbeae9115e5\",\"text\":\"measured indirectly.\\nImplicit Bias in Explicitly Unbiased LLMs. The evolution of language models highlights an\\nintriguing parallel to how humans have managed and transformed their expressions of stereotype\\nbiases. Initially, pre-trained language models directly reflect the biases inherent in their training data,\\noften resulting in explicitly biased outputs [18, 39, 21]. To address these issues, fine-tuned language\"}\n{\"score\":0.7551,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"65f99797-980a-51e0-b782-add4cab12bc1\",\"text\":\"analyzed and the creation of an open benchmark.\\n3\\nCOBBLER: Cognitive Bias Benchmark\\nfor LLMs as Evaluators\\nThe following criteria are used to select each type\\nof evaluation bias:\\n\\u2022 General Applicability. Text evaluation tasks\\nshould be generalizable to most prompting sce-\\nnarios; tasks that observe too specific subtleties\\nwithin the prompt are not helpful.\\n\\u2022 Impartiality. The prompt should not involve\\nany leading statements to extract some desired\\nquality of the evaluations\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The similarity search results provide some context about the cognitive biases, but I need to find the specific list of biases mentioned in the paper. I will refine the search to focus on the section that lists the biases.\n[Action]:\nClassicRetrieve(query=\"List of cognitive biases in the paper 'Benchmarking Cognitive Biases in Large Language Models as Evaluators'\", limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7909,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"799faff1-c0d7-5671-907c-3084e47e7e5d\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2024, pages 517\\u2013545\\nAugust 11-16, 2024 \\u00a92024 Association for Computational Linguistics\\nBenchmarking Cognitive Biases in Large Language Models as Evaluators\\nRyan Koo1 Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung Kim1 Dongyeop Kang1\\n1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu,\\nvipul.raheja@grammarly.com\\nAbstract\\nLarge Language Models (LLMs) have recently\"}\n{\"score\":0.7655,\"pdf_id\":\"bdd90971-ecf1-5dcc-87cf-3b4a75ad4b01\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cb0b7e66-a8c2-52b2-a4bd-4a6c0fd31f2e\",\"text\":\"Sophia Yang, Szymon Antoniak, Teven Le Scao,\\nTh\\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimoth\\u00e9e Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.\\nShuyu Jiang, Xingshu Chen, and Rui Tang. 2023.\\nPrompt packer: Deceiving llms through composi-\\ntional instruction with hidden attacks.\\nDaniel Kahneman. 2011.\\nThinking, fast and slow.\\nmacmillan.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\\nZae Myung Kim, and Dongyeop Kang. 2023. Bench-\\nmarking cognitive biases in large language models as\"}\n{\"score\":0.7434,\"pdf_id\":\"796966dc-afdc-5052-9901-cd3494f37e9f\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1152f8c9-6dd2-5f75-a36b-007d1a71d70f\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2023, pages 5269\\u20135281\\nJuly 9-14, 2023 \\u00a92023 Association for Computational Linguistics\\nMind the Biases: Quantifying Cognitive Biases\\nin Language Model Prompting\\nRuixi Lin and Hwee Tou Ng\\nDepartment of Computer Science\\nNational University of Singapore\\n{ruixi,nght}@comp.nus.edu.sg\\nAbstract\\nWe advocate the importance of exposing uncer-\\ntainty on results of language model prompting\\nwhich display bias modes resembling cognitive\"}\n{\"score\":0.7427,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"94c66e14-12c7-5b64-86f9-716a0fb9b71d\",\"text\":\"value-aligned language models make biased decisions that reflect these implicit biases. Using a\\none-sample t-test to compare bias scores against the unbiased 50% baseline, again, we find that on\\naverage LLMs were statistically significantly more likely to make biased decisions that disadvantage\\nmarginalized groups t(26, 528) = 36.25, p < .001. We also observe that decision biases are not as\\nstrong as implicit biases, partially due to larger variances in decisions (Figure 3).\"}\n{\"score\":0.7406,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"5327015a-287d-5264-8d01-7401b0fa9968\",\"text\":\"Measuring Implicit Bias in\\nExplicitly Unbiased Large Language Models\\nXuechunzi Bai1,\\u2020\\nAngelina Wang2,\\u2021\\nIlia Sucholutsky2,\\u266f\\nThomas L. Griffiths1,2,\\u266f\\n1Department of Psychology\\n2Department of Computer Science\\n\\u2020The University of Chicago\\n\\u2021Stanford University\\n\\u266fPrinceton University\\nAbstract\\nLarge language models (LLMs) can pass explicit social bias tests but still harbor\\nimplicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The similarity search results still do not provide a clear list of cognitive biases. I will try a more specific query to find the section that lists the biases.\n[Action]:\nClassicRetrieve(query=\"Table 1 summarizes definitions of each bias type in the paper 'Benchmarking Cognitive Biases in Large Language Models as Evaluators'\", limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7576,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"799faff1-c0d7-5671-907c-3084e47e7e5d\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2024, pages 517\\u2013545\\nAugust 11-16, 2024 \\u00a92024 Association for Computational Linguistics\\nBenchmarking Cognitive Biases in Large Language Models as Evaluators\\nRyan Koo1 Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung Kim1 Dongyeop Kang1\\n1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu,\\nvipul.raheja@grammarly.com\\nAbstract\\nLarge Language Models (LLMs) have recently\"}\n{\"score\":0.7292,\"pdf_id\":\"031b99e6-bdfa-52fb-b26b-fa26cf80ea21\",\"page_number\":9,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1c73a8bb-48aa-528b-8aee-fbd2e627d0af\",\"text\":\"Proceedings of the 2021 CHI Conference on Human\\nFactors in Computing Systems, pages 1\\u201314.\\nDirk Hovy and Shrimai Prabhumoye. 2021.\\nFive\\nsources of bias in natural language processing. Lan-\\nguage and Linguistics Compass, 15(8):e12432.\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980.\\nJohn Lalor, Yi Yang, Kendall Smith, Nicole Forsgren,\\nand Ahmed Abbasi. 2022. Benchmarking intersec-\\ntional biases in NLP. In Proceedings of the 2022\"}\n{\"score\":0.7263,\"pdf_id\":\"bdd90971-ecf1-5dcc-87cf-3b4a75ad4b01\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cb0b7e66-a8c2-52b2-a4bd-4a6c0fd31f2e\",\"text\":\"Sophia Yang, Szymon Antoniak, Teven Le Scao,\\nTh\\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimoth\\u00e9e Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.\\nShuyu Jiang, Xingshu Chen, and Rui Tang. 2023.\\nPrompt packer: Deceiving llms through composi-\\ntional instruction with hidden attacks.\\nDaniel Kahneman. 2011.\\nThinking, fast and slow.\\nmacmillan.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\\nZae Myung Kim, and Dongyeop Kang. 2023. Bench-\\nmarking cognitive biases in large language models as\"}\n{\"score\":0.7223,\"pdf_id\":\"5d4f6d45-c725-5e01-a9ce-59dc0b7db79f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"efe180c0-0620-5bd7-812f-bfd0f41f2d38\",\"text\":\"preprint arXiv:1902.03680, 2019.\\n474\\nJacob-Junqi Tian, D. Emerson, Deval Pandya, Laleh Seyyed-Kalantari, and Faiza Khattak. Effi-\\n475\\ncient evaluation of bias in large language models through prompt tuning. In Proceedings of the\\n476\\nConference on Neural Information Processing Systems, 2023. URL https:\\/\\/openreview.net\\/\\n477\\nforum?id=v1WL01lgp8.\\n478\\nXiaogang Wang, Zhen Xu, Dongkuan Yang, Lai Tam, Holger Roth, and Dong Xu. Learning image\\n479\"}\n{\"score\":0.7163,\"pdf_id\":\"77cf04ea-fbbf-5cf9-901a-7cbc93b543ed\",\"page_number\":79,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"dbf65701-b191-506d-8656-2e2d6bfdbbcd\",\"text\":\"https:\\/\\/doi.org\\/10.1093\\/bioinformatics\\/btq302. (cited on p. 37)\\nGregory L. Murphy. Comprehending complex concepts. Cognitive Science, 12(4):529\\u2013562, 1988. doi: https:\\/\\/doi.org\\/10.1207\\/\\ns15516709cog1204\\\\_2. URL https:\\/\\/onlinelibrary.wiley.com\\/doi\\/abs\\/10.1207\\/s15516709cog1204_2. (cited on p. 30)\\nMoin Nadeem, Anna Bethke, and Siva Reddy. StereoSet: Measuring stereotypical bias in pretrained language models, 2020.\\nURL https:\\/\\/arxiv.org\\/abs\\/2004.09456. (cited on pp. 31 and 41)\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The similarity search results are still not providing the specific list of biases. I will try a more targeted SQL query to find the section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1 summarizes definitions of each bias type%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, so I will try a broader search to find the section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%cognitive biases%list%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"LLMs as Evaluators. Owing to the effectiveness of LLMs, many recent research works have investigated their utility in various downstream tasks, such as machine translation (Kocmi and Federmann, 2023), summarization (Shen et al., 2023; Gao et al., 2023), code generation (Zhuo, 2023), writing assistance (Schick et al., 2023; Raheja et al., 2023), factual consistency (Cohen et al., 2023; Gekhman et al., 2023; Luo et al., 2023), and more. Additionally, many studies have leveraged LLMs for general-purpose NLG evaluation. For instance, Liu et al. (2023); Chen et al. (2023); Wang et al. (2023a) investigated the effectiveness of GPT-4 and ChatGPT against reference-free evaluation methods, whereas Fu et al. (2023) proposed an evaluation framework, GPTSCORE, to score generated texts. Recently, Li et al. (2023a) and Zheng et al. (2023) conducted similar experiments by employing LLMs as evaluators to judge the quality of generations in a pairwise setting. Although these works present promising results for LLMs as automatic evaluators, our work takes a closer look at machine artifacts that could be detrimental to data quality by benchmarking an exhaustive list of biases impacting LLMs-as-evaluators.\\nLLM Evaluation Benchmarks. It is becoming increasingly challenging to evaluate open-source LLMs as they become more powerful and performant. As a result, there has been an increasing need to develop better evaluation benchmarks for measuring the performance of LLMs. However, most of these benchmarks, such as LM-EVALHARNESS (Gao et al., 2021), MMLU (Hendrycks et al., 2021), HELM (Liang et al., 2022) and BIG\\nBENCH (Srivastava et al., 2023), only focus on general LLM performance but do not explore their capabilities as evaluators. Our work in this direction overlaps directly with Bai et al. (2023) and Zheng et al. (2023), who propose a Language-Model-asan-Examiner benchmark and LLM-as-a-judge to study the capability of LLMs to emulate human preferences. While our experimental setups are similar, we highlight key differences. We cover a wider demographic of current popular language models and an overall different focus on QA as opposed to other domains such as math and reason. Furthermore, our benchmark emphasizes a wider range of biases (implicit\\/induced) to better describe machine artifacts when used as automatic evaluators. Specifcally, COBBLER measures the extent to which each LM-as-evaluator is impacted in each decision by certain artifacts within prompts (i.e., prompting format, prompt information) over a comprehensive list of cognitive biases.\\nCognitive Biases in LLMs. While biases have been well-known to exist in LLMs (Wang et al., 2023b; Talboy and Fuller, 2023; Wu and Aji, 2023), many recent works investigating the behaviors of LLMs have also uncovered similarities with cognitive biases. Some recent works (Zhao et al., 2021; Liu et al., 2022; Lu et al., 2022) have shown that the order of training examples in GPT-3 could lead to differences in accuracy between near chance and near state-of-the-art. Jones and Steinhardt (2022) captured failures in GPT-3 and Codex and found that error patterns of LLMs resemble cognitive biases in humans. Our work overlaps with these in some of the biases we cover, but we present a much more holistic and comprehensive evaluation of LLMs. Along this aspect, while our work is close to Wu and Aji (2023), who investigate biases related to fabricated factual and grammatical errors, our work is much more comprehensive in terms of the number of LLMs analyzed, the types of biases analyzed and the creation of an open benchmark.\"}\n\n... # only display 1 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved section content mentions a list of cognitive biases but does not explicitly list them. I will perform another similarity search to find the specific list of biases.\n[Action]:\nClassicRetrieve(query=\"List of cognitive biases in the paper 'Benchmarking Cognitive Biases in Large Language Models as Evaluators'\", limit=10)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7909,\"pdf_id\":\"3d218d94-1aa0-5a70-b23e-accb254141bd\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"799faff1-c0d7-5671-907c-3084e47e7e5d\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2024, pages 517\\u2013545\\nAugust 11-16, 2024 \\u00a92024 Association for Computational Linguistics\\nBenchmarking Cognitive Biases in Large Language Models as Evaluators\\nRyan Koo1 Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung Kim1 Dongyeop Kang1\\n1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu,\\nvipul.raheja@grammarly.com\\nAbstract\\nLarge Language Models (LLMs) have recently\"}\n{\"score\":0.7655,\"pdf_id\":\"bdd90971-ecf1-5dcc-87cf-3b4a75ad4b01\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cb0b7e66-a8c2-52b2-a4bd-4a6c0fd31f2e\",\"text\":\"Sophia Yang, Szymon Antoniak, Teven Le Scao,\\nTh\\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\\nTimoth\\u00e9e Lacroix, and William El Sayed. 2024. Mix-\\ntral of experts.\\nShuyu Jiang, Xingshu Chen, and Rui Tang. 2023.\\nPrompt packer: Deceiving llms through composi-\\ntional instruction with hidden attacks.\\nDaniel Kahneman. 2011.\\nThinking, fast and slow.\\nmacmillan.\\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,\\nZae Myung Kim, and Dongyeop Kang. 2023. Bench-\\nmarking cognitive biases in large language models as\"}\n{\"score\":0.7434,\"pdf_id\":\"796966dc-afdc-5052-9901-cd3494f37e9f\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1152f8c9-6dd2-5f75-a36b-007d1a71d70f\",\"text\":\"Findings of the Association for Computational Linguistics: ACL 2023, pages 5269\\u20135281\\nJuly 9-14, 2023 \\u00a92023 Association for Computational Linguistics\\nMind the Biases: Quantifying Cognitive Biases\\nin Language Model Prompting\\nRuixi Lin and Hwee Tou Ng\\nDepartment of Computer Science\\nNational University of Singapore\\n{ruixi,nght}@comp.nus.edu.sg\\nAbstract\\nWe advocate the importance of exposing uncer-\\ntainty on results of language model prompting\\nwhich display bias modes resembling cognitive\"}\n{\"score\":0.7427,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"94c66e14-12c7-5b64-86f9-716a0fb9b71d\",\"text\":\"value-aligned language models make biased decisions that reflect these implicit biases. Using a\\none-sample t-test to compare bias scores against the unbiased 50% baseline, again, we find that on\\naverage LLMs were statistically significantly more likely to make biased decisions that disadvantage\\nmarginalized groups t(26, 528) = 36.25, p < .001. We also observe that decision biases are not as\\nstrong as implicit biases, partially due to larger variances in decisions (Figure 3).\"}\n{\"score\":0.7406,\"pdf_id\":\"6e89c41f-ad8f-5898-840f-976a028af952\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"5327015a-287d-5264-8d01-7401b0fa9968\",\"text\":\"Measuring Implicit Bias in\\nExplicitly Unbiased Large Language Models\\nXuechunzi Bai1,\\u2020\\nAngelina Wang2,\\u2021\\nIlia Sucholutsky2,\\u266f\\nThomas L. Griffiths1,2,\\u266f\\n1Department of Psychology\\n2Department of Computer Science\\n\\u2020The University of Chicago\\n\\u2021Stanford University\\n\\u266fPrinceton University\\nAbstract\\nLarge language models (LLMs) can pass explicit social bias tests but still harbor\\nimplicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle\"}\n{\"score\":0.7403,\"pdf_id\":\"5d4f6d45-c725-5e01-a9ce-59dc0b7db79f\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"efe180c0-0620-5bd7-812f-bfd0f41f2d38\",\"text\":\"preprint arXiv:1902.03680, 2019.\\n474\\nJacob-Junqi Tian, D. Emerson, Deval Pandya, Laleh Seyyed-Kalantari, and Faiza Khattak. Effi-\\n475\\ncient evaluation of bias in large language models through prompt tuning. In Proceedings of the\\n476\\nConference on Neural Information Processing Systems, 2023. URL https:\\/\\/openreview.net\\/\\n477\\nforum?id=v1WL01lgp8.\\n478\\nXiaogang Wang, Zhen Xu, Dongkuan Yang, Lai Tam, Holger Roth, and Dong Xu. Learning image\\n479\"}\n{\"score\":0.7403,\"pdf_id\":\"9e61daf4-6c9c-5260-b2a0-4a14bc8b11eb\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"89677014-c311-5ecf-9783-1dadf116b59b\",\"text\":\"A\\nAppendix\\nA.1\\nRelated Work\\nA.1.1\\nFairness Benchmark for Large Language Model\\nInternal Evaluation.\\nConsiderable research has been conducted on the evaluation of bias in\\nlanguage models, particularly through the lens of embeddings. One way to assess biases between\\ndemographic groups is by comparing the cosine similarities of their word embeddings [19, 34, 11].\\nAnother approach measures bias through the probabilities assigned by language models [15, 2]. For\"}\n{\"score\":0.7306,\"pdf_id\":\"528712e7-3718-5b27-8836-d0d3b544f3e5\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"711ac26f-12be-5e8e-ac14-2f134d620c2a\",\"text\":\"mental design, both for bias and for evaluating\\nlanguage models more broadly.\\n1\\nIntroduction\\nConcurrent with the shift in NLP research towards\\nthe use of pretrained and generative models, there\\nhas been a growth in interrogating the biases\\ncontained in language models via prompts or\\ntemplates (henceforth bias tests). While recent\\nwork has empirically examined the robustness of\\nthese tests (Seshadri et al., 2022; Aky\\u00fcrek et al.,\\n2022), it remains unclear what normative concerns\"}\n{\"score\":0.7306,\"pdf_id\":\"74fcdaff-a58a-54fe-9926-d0615989e82d\",\"page_number\":1,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b1684618-5deb-5961-9367-eb856b563fb9\",\"text\":\"awareness of potential harms caused by biases, several studies point out the shortcomings in existing\\n27\\nbenchmarks for measuring the biases in generative language models Blodgett et al. [2021], Aky\\u00fcrek\\n28\\net al. [2022], Goldfarb-Tarrant et al. [2023].\\n29\\nIn this paper, we propose a causal framework(Section 2) and a list of desiderata for bias-measuring\\n30\\nmethodologies: (1) Prompts and stereotypes should be formed independently to eliminate the\\n31\"}\n{\"score\":0.7292,\"pdf_id\":\"031b99e6-bdfa-52fb-b26b-fa26cf80ea21\",\"page_number\":9,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"1c73a8bb-48aa-528b-8aee-fbd2e627d0af\",\"text\":\"Proceedings of the 2021 CHI Conference on Human\\nFactors in Computing Systems, pages 1\\u201314.\\nDirk Hovy and Shrimai Prabhumoye. 2021.\\nFive\\nsources of bias in natural language processing. Lan-\\nguage and Linguistics Compass, 15(8):e12432.\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980.\\nJohn Lalor, Yi Yang, Kendall Smith, Nicole Forsgren,\\nand Ahmed Abbasi. 2022. Benchmarking intersec-\\ntional biases in NLP. In Proceedings of the 2022\"}\n\nIn total, 10 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The similarity search results still do not provide the specific list of cognitive biases. I will try a more targeted SQL query to find the section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_title LIKE '%Cognitive Biases%'\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Ryan $\\\\mathbf{Koo^{1}}$ Minhwa Lee1 Vipul Raheja2 Jonginn Park1, Zae Myung ${\\\\bf K i m^{1}}$ Dongyeop Kang1 1University of Minnesota, 2Grammarly\\n{koo00017,lee03533,park2838,kim01756,dongyeop}@umn.edu, vipul.raheja@grammarly.com\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section content does not contain the list of cognitive biases. I will try a more specific query to find the section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1: Definitions of each bias type%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, so I will try a broader search to find the section that lists the biases.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '3d218d94-1aa0-5a70-b23e-accb254141bd' AND section_content LIKE '%Table 1: Definitions of each bias type%list%'\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
