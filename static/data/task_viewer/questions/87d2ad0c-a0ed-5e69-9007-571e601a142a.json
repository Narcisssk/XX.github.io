{
    "uuid": "87d2ad0c-a0ed-5e69-9007-571e601a142a",
    "question": "What is the collection process of the latest mainstream KBQA dataset used in the paper \"Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering\"?",
    "answer_format": "Your answer should be a python strings.",
    "tags": [
        "multiple",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "35a8f7a7-a28f-5a0b-8c54-15fb7237d66e"
    ],
    "reference_pdf": [
        "058d0055-8d50-5b52-ac1a-8c36d074e246"
    ],
    "conference": [],
    "reasoning_steps": [
        "Find the latest mainstream KBQA dataset used in the paper \"Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering\".",
        "Locate the relevant paper.",
        "Find the collection process of the dataset, which is usually in the data collection section."
    ],
    "evaluator": {
        "eval_func": "eval_scoring_points_with_llm",
        "eval_kwargs": {
            "question": "What is the collection process of the latest mainstream KBQA dataset used in the paper \"Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering\"?",
            "scoring_points": [
                "Canonical logical form generation. We leverage the logical form generation algorithm from Su et al.  It first traverses the KB ontology to generate graph-shaped templates that only consist of classes, relations, and functions, and then ground certain nodes to compatible entities to generate logical forms in their meaning representation called graph query.",
                "Canonical question annotation. Each validated canonical logical form is annotated with a canonical question by a graduate student, which is then cross-validated by another student to ensure its fidelity and fluency.",
                "Crowd-powered paraphrasing. We use Amazon Mechanical Turk to crowdsource paraphrases of the canonical question. The crowdsourcing framework with automated quality control mechanisms that contains three task: Paraphrasing, Cross-validation and Entity surface form mining.",
                "Grounding and sampling. We do controlled sampling to generate the final questions: From the pool of logical forms and paraphrases associated with the same canonical logical form, we sample one from each pool at a time to generate a question. Start with uniform weights and each time a logical form or paraphrase is selected, its weight is divided by rho_l and rho_p , respectively. We set rho_l to 2 and rho_p to 10 to enforce more linguistic diversity. Finally, we randomly replace entity surface forms with the ones mined in Task 3 (if there is any)."
            ]
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "zhangyuxin",
    "status": "failure"
}