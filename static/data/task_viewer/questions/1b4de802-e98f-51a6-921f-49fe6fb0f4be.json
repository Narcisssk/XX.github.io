{
    "uuid": "1b4de802-e98f-51a6-921f-49fe6fb0f4be",
    "question": "In paper \"What Do Language Models Learn in Context? The Structured Task Hypothesis.\", which hypothesis is mostly supported by the experiments?",
    "answer_format": "Your answer should be the context that defines the hypothesis.",
    "tags": [
        "single",
        "text",
        "objective"
    ],
    "anchor_pdf": [
        "What Do Language Models Learn in Context? The Structured Task Hypothesis."
    ],
    "reference_pdf": [],
    "conference": [],
    "evaluator": {
        "eval_func": "eval_reference_answer_with_llm",
        "eval_kwargs": {
            "reference_answer": "During pre-training, an LLM learns a set of tasks $\\bar{\\mathcal{T}}$. At inference time, the LLM uses the demonstration to compose a sequence of learned tasks $\\tau_1,\\tau_2,\\dots\\in\\bar{\\mathcal{T}}$ and uses this composition for prediction. The composition itself may result in a novel task not seen during pre-training.",
            "question": "In paper \"What Do Language Models Learn in Context? The Structured Task Hypothesis.\", which hypothesis is mainly supported by the experiments?"
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": true
    },
    "annotator": "hansenyu"
}