{
    "uuid": "ba924c39-78a1-5236-8861-cd718dfc4c9a",
    "question": "Which model, among GPT3.5, GPT-4, Llama-7B, and Mistral-7B, experiences the largest drop in overall accuracy from the Conversation History Task to MMLU AA, relative to zero-shot performance?",
    "answer_format": "Your answer should be a single model name, without any other text. The answer should be one of the following: GPT3.5, GPT-4, Llama-7B, or Mistral-7B.",
    "tags": [
        "single",
        "text",
        "image",
        "objective"
    ],
    "anchor_pdf": [
        "167f62e8-ba35-5166-b85d-8934f1967849"
    ],
    "reference_pdf": [],
    "conference": [],
    "evaluator": {
        "eval_func": "eval_string_exact_match",
        "eval_kwargs": {
            "gold": "Llama-7B"
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "jiangnan"
}