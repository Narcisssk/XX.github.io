{
    "uuid": "17465570-ab08-5b32-ad45-8e87439bb4ed",
    "question": "What new methods were proposed for multi-perspective mathematical augmentation in the dataset used for Stage-1 training in the paper \"MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\"?",
    "answer_format": "Your answer should be a python strings.",
    "tags": [
        "multiple",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "8528897c-c080-5a33-8af9-815ef526d204"
    ],
    "reference_pdf": [
        "7d207789-1284-52d4-8e6f-acd767beaf57"
    ],
    "conference": [],
    "reasoning_steps": [
        "Find the dataset used for Stage-1 training in the paper \"MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\".",
        "Locate the related papers about the dataset.",
        "Retrieve the new methods proposed for multi-perspective mathematical augmentation in the dataset, which are usually in the method section."
    ],
    "evaluator": {
        "eval_func": "eval_scoring_points_with_llm",
        "eval_kwargs": {
            "question": "What new methods were proposed for multi-perspective mathematical augmentation in the dataset used for Stage-1 training in the paper \"MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\"?",
            "scoring_points": [
                "Reorganization. After the reorganization through the LLM, the solving steps will be more logically organized and clearer. Phrases such as \"understand the problem\", \"define variables\", and \"calculate the number\" act as explicit instructions, leading us toward the final result by \"The answer is\".",
                "Backward-Forward Transformation. For a certain question-answer pair, we firstly utilize FOBAR to transform the original question Q into a backward one Q_b; secondly, we rephrase the FOBAR question into a new form where the masked value is requested directly instead of employing an unknown variable X, resulting in a “secondary forward” question which we called BF-Trans question, marked as Q_{bf}. Finally, we generate the solution S_{bf} for this BF-Trans question. Collecting all these BF-Trans augmented samples, we can have D_{bf} = {(Q_{bf} , S_{bf})}.",
                "Expression Replacement. First extract all mathematical expressions from the solution. Subsequently, an arithmetic expression is altered to form a novel equation. With the original problem statement and new equations as guides, a new question can be generated denoted as Q_{replace}.",
                "Majority Sampling Finetuning. They utilize majority voting with k = 30 to request solutions and only select one response with the majority answer for finetuning.",
                "Nested Multi-task Learning. For the main task of solving mathematical problems Q, they select two auxiliary tasks: summarizing the question and listing the solving plan. They prepend the text of question outline O, solving plan P , or both to the solution text S, assembling into an individual final solution S_{mt} = O \\oplus P \\oplus S, for each original question. Then they have D4 = {(Q, S_{mt})} as the nested multitask dataset."
            ]
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "zhangyuxin"
}