{
    "uuid": "99b40a01-942a-571c-9eb8-6be0ef070617",
    "question": "What are the four types of compression methods used for BERT in the paper? Which one of them can be combined with the other three model compression methods? Can you describe the sketch of this method's procedure?",
    "answer_format": "Your answer should be a brief text containing the four compression methods used for BERT in the paper and the method can be combined with other three methods with its procedure's sketch.",
    "tags": [
        "multiple",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "7077f225-4d0b-563d-8b81-7aa83a8ecd08"
    ],
    "reference_pdf": [
        "f5b8950d-8a87-588f-b78d-c6bad8c5d8c8"
    ],
    "conference": [],
    "reasoning_steps": [],
    "evaluator": {
        "eval_func": "eval_reference_answer_with_llm",
        "eval_kwargs": {
            "reference_answer": "The four compression methods used for BERT in the paper are Knowledge Distillation, Pruning, Quantization, and Vocabulary Transfer. Among them, Vocabulary Transfer can be combined with the other three model compression methods. The sketch of this method's procedure can be described as follows: First, the vocabulary is constructed on the in-domain data, then an embedding is assigned to each token, transferring information from the pre-trained representations of the general-purpose language model.",
            "question": "What are the four types of compression methods used for BERT in the paper? Which one of them can be combined with the other three model compression methods? Can you describe the sketch of this method's procedure?"
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "wangchenrun",
    "status": "failure"
}