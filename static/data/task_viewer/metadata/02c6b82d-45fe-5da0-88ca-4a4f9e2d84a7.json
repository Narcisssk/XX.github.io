{
    "uuid": "02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7",
    "title": "Bayesian low-rank adaptation for large language models",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "SoLaR Spotlight",
    "bibtex": "@inproceedings{\nyang2023bayesian,\ntitle={Bayesian low-rank adaptation for large language models},\nauthor={Adam Yang and Maxime Robeyns and Xi Wang and Laurence Aitchison},\nbooktitle={Socially Responsible Language Modelling Research},\nyear={2023},\nurl={https://openreview.net/forum?id=9O6hbiDLU7}\n}",
    "authors": [
        "Adam Yang",
        "Maxime Robeyns",
        "Xi Wang",
        "Laurence Aitchison"
    ],
    "pdf_url": "https://openreview.net/pdf/86c5c156f98e76c6a44ce58402a34d41cba73973.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/02c6b82d-45fe-5da0-88ca-4a4f9e2d84a7.pdf",
    "num_pages": 26,
    "abstract": "Low-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs). However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, which applies a Bayesian approach to the LoRA parameters. Specifically, Laplace-LoRA applies a Laplace approximation to the posterior over the LoRA parameters, considerably improving the calibration of fine-tuned LLMs.",
    "tldr": "Laplace-LoRA enhances fine-tuned LLMs' calibration using Bayesian methods.",
    "tags": [
        "Large language models",
        "Bayesian deep learning",
        "uncertainty calibration"
    ]
}