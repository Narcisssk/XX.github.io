{
    "uuid": "bbf39c89-c4a2-5696-b9e6-f38b2d766a01",
    "title": "On Evaluating Explanation Utility for Human-AI Decision Making in NLP",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: EMNLP 2024",
    "bibtex": "@inproceedings{hashemi-chaleshtori-etal-2024-evaluating,\n    title = \"On Evaluating Explanation Utility for Human-{AI} Decision Making in {NLP}\",\n    author = \"Hashemi Chaleshtori, Fateme  and\n      Ghosal, Atreya  and\n      Gill, Alexander  and\n      Bambroo, Purbid  and\n      Marasovic, Ana\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2024\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-emnlp.439/\",\n    doi = \"10.18653/v1/2024.findings-emnlp.439\",\n    pages = \"7456--7504\",\n    abstract = \"Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations help people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To aid with this, we first review existing metrics suitable for application-grounded evaluation. We then establish criteria to select appropriate datasets, and using them, we find that only 4 out of over 50 datasets available for explainability research in NLP meet them. We then demonstrate the importance of reassessing the state of the art to form and study human-AI teams: teaming people with models for certain tasks might only now start to make sense, and for others, it remains unsound. Finally, we present the exemplar studies of human-AI decision-making for one of the identified tasks --- verifying the correctness of a legal claim given a contract. Our results show that providing AI predictions, with or without explanations, does not cause decision makers to speed up their work without compromising performance. We argue for revisiting the setup of human-AI teams and improving automatic deferral of instances to AI, where explanations could play a useful role.\"\n}\n",
    "authors": [
        "Fateme Hashemi Chaleshtori",
        "Atreya Ghosal",
        "Alexander Gill",
        "Purbid Bambroo",
        "Ana Marasovic"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-emnlp.439.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/bbf39c89-c4a2-5696-b9e6-f38b2d766a01.pdf",
    "num_pages": 49,
    "abstract": "Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations help people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To aid with this, we first review existing metrics suitable for application-grounded evaluation. We then establish criteria to select appropriate datasets, and using them, we find that only 4 out of over 50 datasets available for explainability research in NLP meet them. We then demonstrate the importance of reassessing the state of the art to form and study human-AI teams: teaming people with models for certain tasks might only now start to make sense, and for others, it remains unsound. Finally, we present the exemplar studies of human-AI decision-making for one of the identified tasks â€” verifying the correctness of a legal claim given a contract. Our results show that providing AI predictions, with or without explanations, does not cause decision makers to speed up their work without compromising performance. We argue for revisiting the setup of human-AI teams and improving automatic deferral of instances to AI, where explanations could play a useful role.",
    "tldr": "Evaluates explainability's role in human-AI decision making in NLP tasks.",
    "tags": [
        "explainability",
        "human-AI collaboration",
        "NLP evaluation",
        "decision-making",
        "application-grounded metrics"
    ]
}