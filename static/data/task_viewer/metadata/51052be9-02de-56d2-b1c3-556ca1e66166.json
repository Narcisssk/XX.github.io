{
    "uuid": "51052be9-02de-56d2-b1c3-556ca1e66166",
    "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nwang2024customizable,\ntitle={Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning},\nauthor={Haowen Wang and Tao Sun and Congyun Jin and Yingbo Wang and Yibo Fan and Yunqi Xu and Yuliang Du and Cong Fan},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=G1Hlubz1fR}\n}",
    "authors": [
        "Haowen Wang",
        "Tao Sun",
        "Congyun Jin",
        "Yingbo Wang",
        "Yibo Fan",
        "Yunqi Xu",
        "Yuliang Du",
        "Cong Fan"
    ],
    "pdf_url": "https://openreview.net/pdf/02683ff6d65426efce2edd154c56b49cc95ba2f4.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/51052be9-02de-56d2-b1c3-556ca1e66166.pdf",
    "num_pages": 22,
    "abstract": "Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization.\nIn this paper, we introduce a novel approach Customized Polytropon ($\\texttt{C-Poly}$) that combines task-common skills and task-specific skills, while the skill parameters being highly parameterized using low-rank techniques.\nEach task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks. A skill assignment matrix is jointly learned. To evaluate our approach, we conducted extensive experiments on the Super-NaturalInstructions and the SuperGLUE benchmarks.\nOur findings demonstrate that $\\texttt{C-Poly}$ outperforms fully-shared, task-specific, and skill-indistinguishable baselines, significantly enhancing the sample efficiency in multi-task learning scenarios.",
    "tldr": "A novel paradigm of Parameter Efficient Fine-Tuning (PEFT) for multi-task learning, harnessing specialized and shared domain skills.",
    "tags": [
        "Modular skill learning",
        "Multi-task learning",
        "Parameter-Efficient",
        "Fine-Tuning"
    ]
}