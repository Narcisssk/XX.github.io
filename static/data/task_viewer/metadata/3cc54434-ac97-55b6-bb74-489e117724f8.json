{
    "uuid": "3cc54434-ac97-55b6-bb74-489e117724f8",
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\ndai2023instructblip,\ntitle={Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},\nauthor={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vvoWPYqZJA}\n}",
    "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
    ],
    "pdf_url": "https://openreview.net/pdf/5e6fb997c49ca789aaa1ac1782a6eab5bdb17348.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/3cc54434-ac97-55b6-bb74-489e117724f8.pdf",
    "num_pages": 18,
    "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-source.",
    "tldr": "We present InstructBLIP, an instruction tuning framework towards general-purpose vision-language models. InstructBLIP achieves state-of-the-art results in zero-shot and finetuning with various instruction following abilities. All models are released.",
    "tags": [
        "Vision-Language Models",
        "Instruction Tuning",
        "Zero-shot"
    ]
}