{
    "uuid": "574cb3eb-89dc-5384-a7b1-0518983790f6",
    "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
    "conference_full": "The 2023 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2023,
    "volume": "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    "bibtex": "@inproceedings{weber-etal-2023-mind,\n    title = \"Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning\",\n    author = \"Weber, Lucas  and\n      Bruni, Elia  and\n      Hupkes, Dieuwke\",\n    editor = \"Jiang, Jing  and\n      Reitter, David  and\n      Deng, Shumin\",\n    booktitle = \"Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.conll-1.20\",\n    doi = \"10.18653/v1/2023.conll-1.20\",\n    pages = \"294--313\",\n    abstract = \"Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of \\textit{task-tuned} models (TT), models that are adapted to tasks via in-context-learning (ICL) or instruction tuning (IT) are robust in some setups, but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels {--} a known issue in TT models {--} form only a minor problem for prompted models. Then we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned LLMs of different scale, and statistically analyse the results to show which factors are the most influential, the most interactive or the most stable. From our results, we deduce which factors can be used without precautions, should be avoided or handled with care in most settings.\",\n}\n",
    "authors": [
        "Lucas Weber",
        "Elia Bruni",
        "Dieuwke Hupkes"
    ],
    "pdf_url": "https://aclanthology.org/2023.conll-1.20.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2023/574cb3eb-89dc-5384-a7b1-0518983790f6.pdf",
    "num_pages": 20,
    "abstract": "Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) or instruction tuning (IT) are robust in some setups, but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels – a known issue in TT models – form only a minor problem for prompted models. Then we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned LLMs of different scale, and statistically analyse the results to show which factors are the most influential, the most interactive or the most stable. From our results, we deduce which factors can be used without precautions, should be avoided or handled with care in most settings.",
    "tldr": "Evaluates factors affecting stability in prompt-based learning for LLMs.",
    "tags": [
        "prompt-based learning",
        "language models",
        "consistency evaluation",
        "instruction tuning",
        "NLP stability"
    ]
}