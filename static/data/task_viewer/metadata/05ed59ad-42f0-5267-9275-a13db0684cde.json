{
    "uuid": "05ed59ad-42f0-5267-9275-a13db0684cde",
    "title": "Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules",
    "conference_full": "The 2023 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: EMNLP 2023",
    "bibtex": "@inproceedings{xiao-etal-2023-variator,\n    title = \"Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules\",\n    author = \"Xiao, Chaojun  and\n      Luo, Yuqi  and\n      Zhang, Wenbin  and\n      Zhang, Pengle  and\n      Han, Xu  and\n      Lin, Yankai  and\n      Zhang, Zhengyan  and\n      Xie, Ruobing  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2023\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-emnlp.666\",\n    doi = \"10.18653/v1/2023.findings-emnlp.666\",\n    pages = \"9947--9959\",\n    abstract = \"Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original LLMs frozen. Different from traditional model acceleration methods, which compress LLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a growing number of tasks. We validate the effectiveness of Variator on seven datasets. Experimental results show that Variator can save 53{\\%} computational costs using only 0.9{\\%} additional parameters with a performance drop of less than 2{\\%}. Moreover, when the model scales to billions of parameters, Variator matches the strong performance of uncompressed LLMs. Our code and checkpoints will be released to facilitate future work.\",\n}\n",
    "authors": [
        "Chaojun Xiao",
        "Yuqi Luo",
        "Wenbin Zhang",
        "Pengle Zhang",
        "Xu Han",
        "Yankai Lin",
        "Zhengyan Zhang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-emnlp.666.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2023/05ed59ad-42f0-5267-9275-a13db0684cde.pdf",
    "num_pages": 13,
    "abstract": "Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original LLMs frozen. Different from traditional model acceleration methods, which compress LLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a growing number of tasks. We validate the effectiveness of Variator on seven datasets. Experimental results show that Variator can save 53% computational costs using only 0.9% additional parameters with a performance drop of less than 2%. Moreover, when the model scales to billions of parameters, Variator matches the strong performance of uncompressed LLMs. Our code and checkpoints will be released to facilitate future work.",
    "tldr": "Variator accelerates large language models using efficient, dynamic compression plugins.",
    "tags": [
        "parameter-efficient acceleration",
        "plug-and-play compression",
        "large language models",
        "computational efficiency",
        "neural network optimization"
    ]
}