{
    "uuid": "36f7c548-f8c2-5fc9-ba12-a35ac045bc25",
    "title": "From Sparse to Soft Mixtures of Experts",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 spotlight",
    "bibtex": "@inproceedings{\npuigcerver2024from,\ntitle={From Sparse to Soft Mixtures of Experts},\nauthor={Joan Puigcerver and Carlos Riquelme Ruiz and Basil Mustafa and Neil Houlsby},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=jxpsAj7ltE}\n}",
    "authors": [
        "Joan Puigcerver",
        "Carlos Riquelme Ruiz",
        "Basil Mustafa",
        "Neil Houlsby"
    ],
    "pdf_url": "https://openreview.net/pdf/fd68ff38ff599fb1021a7e6add08b00e8fec95b9.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/36f7c548-f8c2-5fc9-ba12-a35ac045bc25.pdf",
    "num_pages": 11,
    "abstract": "Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs.\nDespite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning.\nIn this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs.\nSoft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert.\nAs in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost.\nIn the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice).\nSoft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.",
    "tldr": "Soft MoEs outperform Transformers and Sparse MoEs in computer vision.",
    "tags": [
        "transformers",
        "mixtures of experts",
        "computer vision"
    ]
}