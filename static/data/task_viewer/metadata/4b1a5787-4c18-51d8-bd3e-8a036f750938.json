{
    "uuid": "4b1a5787-4c18-51d8-bd3e-8a036f750938",
    "title": "Exploring Scientific Hypothesis Generation with Mamba",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on NLP for Science (NLP4Science)",
    "bibtex": "@inproceedings{chai-etal-2024-exploring,\n    title = \"Exploring Scientific Hypothesis Generation with Mamba\",\n    author = \"Chai, Miaosen  and\n      Herron, Emily  and\n      Cervantes, Erick  and\n      Ghosal, Tirthankar\",\n    editor = \"Peled-Cohen, Lotem  and\n      Calderon, Nitay  and\n      Lissak, Shir  and\n      Reichart, Roi\",\n    booktitle = \"Proceedings of the 1st Workshop on NLP for Science (NLP4Science)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, FL, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlp4science-1.17/\",\n    doi = \"10.18653/v1/2024.nlp4science-1.17\",\n    pages = \"197--207\",\n    abstract = \"Generating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage Long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very Long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba\"\n}\n",
    "authors": [
        "Miaosen Chai",
        "Emily Herron",
        "Erick Cervantes",
        "Tirthankar Ghosal"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlp4science-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/4b1a5787-4c18-51d8-bd3e-8a036f750938.pdf",
    "num_pages": 11,
    "abstract": "Generating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage Long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very Long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba",
    "tldr": "Mamba outperforms transformer models in generating scientific hypotheses efficiently.",
    "tags": [
        "hypothesis generation",
        "generative AI",
        "large language models",
        "state space models",
        "transformer models"
    ]
}