{
    "uuid": "4cb24634-a638-51e8-be8d-a15e108c945a",
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\ntang2024salmonn,\ntitle={{SALMONN}: Towards Generic Hearing Abilities for Large Language Models},\nauthor={Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian Tan and Wei Li and Lu Lu and Zejun MA and Chao Zhang},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=14rn7HpKVk}\n}",
    "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun MA",
        "Chao Zhang"
    ],
    "pdf_url": "https://openreview.net/pdf/d5a232debec14724cfe9fcabdbebc192eb470ba3.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/4cb24634-a638-51e8-be8d-a15e108c945a.pdf",
    "num_pages": 23,
    "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as \nautomatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
    "tldr": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "tags": [
        "Multimodal large language models",
        "speech and audio processing",
        "music processing"
    ]
}