{
    "uuid": "2c9c068a-72b3-544e-9944-2558a035e4f8",
    "title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\nalbalak2023improving,\ntitle={Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data},\nauthor={Alon Albalak and Colin Raffel and William Yang Wang},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=JDnLXc4NOn}\n}",
    "authors": [
        "Alon Albalak",
        "Colin Raffel",
        "William Yang Wang"
    ],
    "pdf_url": "https://openreview.net/pdf/debd2298d63c91dd5db7eff25ce57586f664ad1b.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/2c9c068a-72b3-544e-9944-2558a035e4f8.pdf",
    "num_pages": 27,
    "abstract": "Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging.\nIn this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization.\nPrevious works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality.\nIn this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods.\nWe propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the combination of exploration and exploitation is crucial.\nThrough extensive experimentation we find that our methods outperform all pre-existing FLAD methods by 4% and lead to the first 3 billion parameter language models that outperform the 175 billion parameter GPT-3.\nOverall, our work suggests that the discovery of better, more efficient mixing strategies for FLAD may provide a viable path towards substantially improving generalization in few-shot learning.",
    "tldr": "This work focuses on few-shot learning with auxiliary data to improve model generalization, presenting two methods inspired by multi-armed bandits.",
    "tags": [
        "Few-shot learning",
        "natural language processing",
        "few shot learning",
        "NLP",
        "multi-armed bandit",
        "multi armed bandit"
    ]
}