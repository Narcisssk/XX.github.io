{
    "uuid": "0ddf2132-7c61-583c-b27b-7d3edfd030ea",
    "title": "TabMT: Generating tabular data with masked transformers",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\ngulati2023tabmt,\ntitle={Tab{MT}: Generating tabular data with masked transformers},\nauthor={Manbir S Gulati and Paul F Roysdon},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=qs4swxtIAQ}\n}",
    "authors": [
        "Manbir S Gulati",
        "Paul F Roysdon"
    ],
    "pdf_url": "https://openreview.net/pdf/a55d0a40ef3808d267e8a0966329c33240f910c7.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/0ddf2132-7c61-583c-b27b-7d3edfd030ea.pdf",
    "num_pages": 10,
    "abstract": "Autoregressive and Masked Transformers are incredibly effective as generative models and classifiers.\n    While these models are most prevalent in NLP, they also exhibit strong performance in other domains, such as vision. \n    This work contributes to the exploration of transformer-based models in synthetic data generation for diverse application domains. \n    In this paper, we present TabMT, a novel Masked Transformer design for generating synthetic tabular data. \n    TabMT effectively addresses the unique challenges posed by heterogeneous data fields and is natively able to handle missing data. \n    Our design leverages improved masking techniques to allow for generation and demonstrates state-of-the-art performance from extremely small to extremely large tabular datasets. \n    We evaluate TabMT for privacy-focused applications and find that it is able to generate high quality data with superior privacy tradeoffs.",
    "tldr": "Masked Transformers generate high-quality tabular data, scale well, have better privacy, and can handle missing data/conditional generation",
    "tags": [
        "Tabular Data",
        "Deep Learning",
        "Generative Modeling",
        "Transformers",
        "Masked Transformers",
        "Synthetic data"
    ]
}