{
    "uuid": "4cc48a5b-be5f-57af-8697-1cc75c0f67d0",
    "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nxu2024an,\ntitle={An {LLM} can Fool Itself: A Prompt-Based Adversarial Attack},\nauthor={Xilie Xu and Keyi Kong and Ning Liu and Lizhen Cui and Di Wang and Jingfeng Zhang and Mohan Kankanhalli},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=VVgGbB9TNV}\n}",
    "authors": [
        "Xilie Xu",
        "Keyi Kong",
        "Ning Liu",
        "Lizhen Cui",
        "Di Wang",
        "Jingfeng Zhang",
        "Mohan Kankanhalli"
    ],
    "pdf_url": "https://openreview.net/pdf/ba23546abb3c1cd83f22d6160f328c40fdadc123.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/4cc48a5b-be5f-57af-8697-1cc75c0f67d0.pdf",
    "num_pages": 23,
    "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.",
    "tldr": "PromptAttack efficiently audits LLMs' robustness using prompt-based adversarial attacks.",
    "tags": [
        "large language model",
        "adversarial attack",
        "adversarial robustness"
    ]
}