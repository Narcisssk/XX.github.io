{
    "uuid": "0d89b506-7770-5702-a992-47a2e50eee4d",
    "title": "GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nlee2024gta,\ntitle={{GTA}: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning},\nauthor={Jaewoo Lee and Sujin Yun and Taeyoung Yun and Jinkyoo Park},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=kZpNDbZrzy}\n}",
    "authors": [
        "Jaewoo Lee",
        "Sujin Yun",
        "Taeyoung Yun",
        "Jinkyoo Park"
    ],
    "pdf_url": "https://openreview.net/pdf/0654ab64b73938184f454a5419d4545766a9f5c3.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/0d89b506-7770-5702-a992-47a2e50eee4d.pdf",
    "num_pages": 36,
    "abstract": "Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce GTA, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms across various tasks with unique challenges. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA",
    "tldr": "Novel data augmentation method for offline RL using conditional diffusion model.",
    "tags": [
        "Offline Reinforcement Learning",
        "Data Augmentation",
        "Diffusion Models."
    ]
}