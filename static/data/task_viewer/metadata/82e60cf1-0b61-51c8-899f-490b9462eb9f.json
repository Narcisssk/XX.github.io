{
    "uuid": "82e60cf1-0b61-51c8-899f-490b9462eb9f",
    "title": "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nliao2024equiformerv,\ntitle={EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations},\nauthor={Yi-Lun Liao and Brandon M Wood and Abhishek Das and Tess Smidt},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=mCOBKZmrzD}\n}",
    "authors": [
        "Yi-Lun Liao",
        "Brandon M Wood",
        "Abhishek Das",
        "Tess Smidt"
    ],
    "pdf_url": "https://openreview.net/pdf/a9cddac6c5dfd3489ef94cc24cb17f5ae2fb7ad1.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/82e60cf1-0b61-51c8-899f-490b9462eb9f.pdf",
    "num_pages": 28,
    "abstract": "Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions\nwith eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements â€“ attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting these all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on large-scale OC20 dataset by up to 9% on forces, 4% on energies, offers better speed-accuracy trade-offs, and 2$\\times$ reduction in DFT calculations needed for computing adsorption energies. Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC trained on both OC20 and OC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M\ndatasets to better understand the performance gain brought by higher degrees.",
    "tldr": "We propose a better equivariant Transformer architecture that better leverage higher degrees of representations and the proposed method achieves state-of-the-art results on OC20 and OC22 datasets.",
    "tags": [
        "equivariant neural networks",
        "graph neural networks",
        "computational physics",
        "transformer networks"
    ]
}