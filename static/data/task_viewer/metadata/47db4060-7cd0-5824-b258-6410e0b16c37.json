{
    "uuid": "47db4060-7cd0-5824-b258-6410e0b16c37",
    "title": "PepRec: Progressive Enhancement of Prompting for Recommendation",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{yu-etal-2024-peprec,\n    title = \"{P}ep{R}ec: Progressive Enhancement of Prompting for Recommendation\",\n    author = \"Yu, Yakun  and\n      Qi, Shi-ang  and\n      Li, Baochun  and\n      Niu, Di\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.995\",\n    doi = \"10.18653/v1/2024.emnlp-main.995\",\n    pages = \"17941--17953\",\n    abstract = \"With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, recent researchers have actively explored the potential of LLMs for recommendation systems by converting the input data into textual sentences through prompt templates. Although semantic knowledge from LLMs can help enrich the content information of items, to date it is still hard for them to achieve comparable performance to traditional deep learning recommendation models, partly due to a lack of ability to leverage collaborative filtering. In this paper, we propose a novel training-free prompting framework, PepRec, which aims to capture knowledge from both content-based filtering and collaborative filtering to boost recommendation performance with LLMs, while providing interpretation for the recommendation. Experiments based on two real-world datasets from different domains show that PepRec significantly outperforms various traditional deep learning recommendation models and prompt-based recommendation systems.\",\n}\n",
    "authors": [
        "Yakun Yu",
        "Shi-ang Qi",
        "Baochun Li",
        "Di Niu"
    ],
    "pdf_url": "https://aclanthology.org/2024.emnlp-main.995.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/47db4060-7cd0-5824-b258-6410e0b16c37.pdf",
    "num_pages": 13,
    "abstract": "With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, recent researchers have actively explored the potential of LLMs for recommendation systems by converting the input data into textual sentences through prompt templates. Although semantic knowledge from LLMs can help enrich the content information of items, to date it is still hard for them to achieve comparable performance to traditional deep learning recommendation models, partly due to a lack of ability to leverage collaborative filtering. In this paper, we propose a novel training-free prompting framework, PepRec, which aims to capture knowledge from both content-based filtering and collaborative filtering to boost recommendation performance with LLMs, while providing interpretation for the recommendation. Experiments based on two real-world datasets from different domains show that PepRec significantly outperforms various traditional deep learning recommendation models and prompt-based recommendation systems.",
    "tldr": "PepRec enhances LLMs for recommendations using content and collaborative filtering.",
    "tags": [
        "recommendation systems",
        "large language models",
        "collaborative filtering",
        "content-based filtering",
        "prompt engineering"
    ]
}