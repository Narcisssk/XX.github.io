{
    "uuid": "1d9b8fd8-3443-5191-9f86-602a3e1c7e6b",
    "title": "LG-VQ: Language-Guided Codebook Learning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nguotao2024lgvq,\ntitle={{LG}-{VQ}: Language-Guided Codebook Learning},\nauthor={Liang Guotao and Baoquan Zhang and Yaowei Wang and Yunming Ye and Xutao Li and Wanghuaibin and Luo Chuyao and kolaye and luolinfeng},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=vA4s3kN4QE}\n}",
    "authors": [
        "Liang Guotao",
        "Baoquan Zhang",
        "Yaowei Wang",
        "Yunming Ye",
        "Xutao Li",
        "Wanghuaibin",
        "Luo Chuyao",
        "kolaye",
        "luolinfeng"
    ],
    "pdf_url": "https://openreview.net/pdf/96a358e0e07a0284d43a8bd709729b6145adcd2a.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/1d9b8fd8-3443-5191-9f86-602a3e1c7e6b.pdf",
    "num_pages": 25,
    "abstract": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. \n  Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps.\n  In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment.   \n  In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
    "tldr": "We utilize pre-trained text semantics to guide the codebook to learn rich multi-modal knowledge to improve the performance of multi-modal downstream tasks",
    "tags": [
        "Codebook Learing",
        "VQ-GAN",
        "Vector-quantized Image Modeling"
    ]
}