{
    "uuid": "54d865d4-779e-525d-8e06-c9cc2207beb3",
    "title": "Tree of Problems: Improving structured problem solving with compositionality",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{zebaze-etal-2024-tree,\n    title = \"Tree of Problems: Improving structured problem solving with compositionality\",\n    author = \"Zebaze, Armel Randy  and\n      Sagot, Beno{\\^\\i}t  and\n      Bawden, Rachel\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1001\",\n    doi = \"10.18653/v1/2024.emnlp-main.1001\",\n    pages = \"18028--18047\",\n    abstract = \"Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.\",\n}\n",
    "authors": [
        "Armel Randy Zebaze",
        "Beno√Æt Sagot",
        "Rachel Bawden"
    ],
    "pdf_url": "https://aclanthology.org/2024.emnlp-main.1001.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/54d865d4-779e-525d-8e06-c9cc2207beb3.pdf",
    "num_pages": 20,
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.",
    "tldr": "Tree of Problems outperforms existing methods for complex structured problem solving.",
    "tags": [
        "large language models",
        "structured problem solving",
        "Chain-of-Thought",
        "Tree of Thoughts",
        "empirical results"
    ]
}