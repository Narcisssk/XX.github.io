{
    "uuid": "fef69202-2222-5c62-ae44-88cdc4d47f7b",
    "title": "MINERS: Multilingual Language Models as Semantic Retrievers",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: EMNLP 2024",
    "bibtex": "@inproceedings{winata-etal-2024-miners,\n    title = \"{MINERS}: Multilingual Language Models as Semantic Retrievers\",\n    author = \"Winata, Genta Indra  and\n      Zhang, Ruochen  and\n      Adelani, David Ifeoluwa\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2024\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-emnlp.155/\",\n    doi = \"10.18653/v1/2024.findings-emnlp.155\",\n    pages = \"2742--2766\",\n    abstract = \"Words have been represented in a high-dimensional vector space that encodes their semantic similarities, enabling downstream applications such as retrieving synonyms, antonyms, and relevant contexts. However, despite recent advances in multilingual language models (LMs), the effectiveness of these models' representations in semantic retrieval contexts has not been comprehensively explored. To fill this gap, this paper introduces the MINERS, a benchmark designed to evaluate the ability of multilingual LMs in semantic retrieval tasks, including bitext mining and classification via retrieval-augmented contexts. We create a comprehensive framework to assess the robustness of LMs in retrieving samples across over 200 diverse languages, including extremely low-resource languages in challenging cross-lingual and code-switching settings. Our results demonstrate that by solely retrieving semantically similar embeddings yields performance competitive with state-of-the-art approaches, without requiring any fine-tuning.\"\n}\n",
    "authors": [
        "Genta Indra Winata",
        "Ruochen Zhang",
        "David Ifeoluwa Adelani"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-emnlp.155.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/fef69202-2222-5c62-ae44-88cdc4d47f7b.pdf",
    "num_pages": 25,
    "abstract": "Words have been represented in a high-dimensional vector space that encodes their semantic similarities, enabling downstream applications such as retrieving synonyms, antonyms, and relevant contexts. However, despite recent advances in multilingual language models (LMs), the effectiveness of these modelsâ€™ representations in semantic retrieval contexts has not been comprehensively explored. To fill this gap, this paper introduces the MINERS, a benchmark designed to evaluate the ability of multilingual LMs in semantic retrieval tasks, including bitext mining and classification via retrieval-augmented contexts. We create a comprehensive framework to assess the robustness of LMs in retrieving samples across over 200 diverse languages, including extremely low-resource languages in challenging cross-lingual and code-switching settings. Our results demonstrate that by solely retrieving semantically similar embeddings yields performance competitive with state-of-the-art approaches, without requiring any fine-tuning.",
    "tldr": "MINERS benchmarks multilingual LMs for effective semantic retrieval tasks.",
    "tags": [
        "multilingual language models",
        "semantic retrieval",
        "bitext mining",
        "cross-lingual tasks",
        "low-resource languages"
    ]
}