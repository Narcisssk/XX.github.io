{
    "uuid": "3ef4f8bf-6e26-545b-b51c-e6a7969818c7",
    "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nbai2024digirl,\ntitle={Digi{RL}: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning},\nauthor={Hao Bai and Yifei Zhou and Jiayi Pan and Mert Cemri and Alane Suhr and Sergey Levine and Aviral Kumar},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=4XTvXMSZPO}\n}",
    "authors": [
        "Hao Bai",
        "Yifei Zhou",
        "Jiayi Pan",
        "Mert Cemri",
        "Alane Suhr",
        "Sergey Levine",
        "Aviral Kumar"
    ],
    "pdf_url": "https://openreview.net/pdf/53508ea1db0056abe7a6fb24ad516a8a40675570.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/3ef4f8bf-6e26-545b-b51c-e6a7969818c7.pdf",
    "num_pages": 35,
    "abstract": "Pre-trained vision language models (VLMs), though powerful, typically lack training on decision-centric data, rendering them sub-optimal for decision-making tasks such as in-the-wild device control through Graphical User Interfaces (GUIs) when used off-the-shelf. While training with static demonstrations has shown some promise, we show that such methods fall short when controlling real GUIs due to their failure to deal with real world stochasticity and dynamism not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL. We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\\% absolute improvement -- from 17.7 to 67.2\\% success rate -- over supervised fine-tuning with static human demonstration data. It is worth noting that such improvement is achieved without any additional supervision or demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\\% success rate) and the 17B CogAgent trained with AitW data (14.4\\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.",
    "tldr": "DigiRL outperforms existing methods for in-the-wild device control using RL.",
    "tags": [
        "LLM/VLM Agent",
        "Device Control",
        "GUI Navigation",
        "Web Agent",
        "Reinforcement Learning"
    ]
}