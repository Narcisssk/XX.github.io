{
    "uuid": "eee99caa-1041-588f-850b-67dc3a80524c",
    "title": "Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)",
    "bibtex": "@inproceedings{chronopoulou-etal-2024-language,\n    title = \"Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization\",\n    author = \"Chronopoulou, Alexandra  and\n      Pfeiffer, Jonas  and\n      Maynez, Joshua  and\n      Wang, Xinyi  and\n      Ruder, Sebastian  and\n      Agrawal, Priyanka\",\n    editor = {S\\\"alev\\\"a, Jonne  and\n      Owodunni, Abraham},\n    booktitle = \"Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.mrl-1.7/\",\n    doi = \"10.18653/v1/2024.mrl-1.7\",\n    pages = \"114--126\",\n    abstract = \"Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing expert modules trained separately on language or task data. Our method composes $\\textit{language}$ and $\\textit{task}$ PEFT adapters via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT adapters trained on languages related to the target. Empirical results on summarization demonstrate that our method is a strategy that obtains consistent gains using minimal training of PEFT parameters.\"\n}\n",
    "authors": [
        "Alexandra Chronopoulou",
        "Jonas Pfeiffer",
        "Joshua Maynez",
        "Xinyi Wang",
        "Sebastian Ruder",
        "Priyanka Agrawal"
    ],
    "pdf_url": "https://aclanthology.org/2024.mrl-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/eee99caa-1041-588f-850b-67dc3a80524c.pdf",
    "num_pages": 13,
    "abstract": "Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing expert modules trained separately on language or task data. Our method composes language and task PEFT adapters via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT adapters trained on languages related to the target. Empirical results on summarization demonstrate that our method is a strategy that obtains consistent gains using minimal training of PEFT parameters.",
    "tldr": "Proposes a method for zero-shot summarization using efficient language-task adapters.",
    "tags": [
        "zero-shot summarization",
        "parameter-efficient fine-tuning",
        "cross-lingual transfer",
        "language models",
        "unlabeled data"
    ]
}