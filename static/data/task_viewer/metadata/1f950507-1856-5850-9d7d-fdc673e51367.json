{
    "uuid": "1f950507-1856-5850-9d7d-fdc673e51367",
    "title": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\njiang2024kgfit,\ntitle={{KG}-{FIT}: Knowledge Graph Fine-Tuning Upon Open-World Knowledge},\nauthor={Pengcheng Jiang and Lang Cao and Cao Xiao and Parminder Bhatia and Jimeng Sun and Jiawei Han},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=rDoPMODpki}\n}",
    "authors": [
        "Pengcheng Jiang",
        "Lang Cao",
        "Cao Xiao",
        "Parminder Bhatia",
        "Jimeng Sun",
        "Jiawei Han"
    ],
    "pdf_url": "https://openreview.net/pdf/8e57c825b4a1824ff155a994d0dd93f8025cf334.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/1f950507-1856-5850-9d7d-fdc673e51367.pdf",
    "num_pages": 39,
    "abstract": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4\\%, 13.5\\%, and 11.9\\% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6\\%, 6.7\\%, and 17.7\\% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
    "tldr": "KG-FIT enhances knowledge graph embeddings using LLMs for better predictions.",
    "tags": [
        "Knowledge Graphs",
        "Large Language Models",
        "Link Prediction"
    ]
}