{
    "uuid": "1dffea3e-12d5-5a96-82db-480f1579040e",
    "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 spotlight",
    "bibtex": "@inproceedings{\nqin2024toolllm,\ntitle={Tool{LLM}: Facilitating Large Language Models to Master 16000+ Real-world {API}s},\nauthor={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and dahai li and Zhiyuan Liu and Maosong Sun},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=dHng2O0Jjr}\n}",
    "authors": [
        "Yujia Qin",
        "Shihao Liang",
        "Yining Ye",
        "Kunlun Zhu",
        "Lan Yan",
        "Yaxi Lu",
        "Yankai Lin",
        "Xin Cong",
        "Xiangru Tang",
        "Bill Qian",
        "Sihan Zhao",
        "Lauren Hong",
        "Runchu Tian",
        "Ruobing Xie",
        "Jie Zhou",
        "Mark Gerstein",
        "dahai li",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "pdf_url": "https://openreview.net/pdf/ccd998057a57b2ae3f5615915e57554028f1ae35.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/1dffea3e-12d5-5a96-82db-480f1579040e.pdf",
    "num_pages": 23,
    "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
    "tldr": "ToolLLM enhances LLMs' abilities to use 16,000+ real-world APIs effectively.",
    "tags": [
        "Large Language Model",
        "Tool Use",
        "API Use"
    ]
}