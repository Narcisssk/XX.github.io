{
    "uuid": "4c5fce26-2c1e-55c2-b4de-b1a11a58d66f",
    "title": "MOTOR: A Time-to-Event Foundation Model For Structured Medical Records",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 spotlight",
    "bibtex": "@inproceedings{\nsteinberg2024motor,\ntitle={{MOTOR}: A Time-to-Event Foundation Model For Structured Medical Records},\nauthor={Ethan Steinberg and Jason Alan Fries and Yizhe Xu and Nigam Shah},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=NialiwI2V6}\n}",
    "authors": [
        "Ethan Steinberg",
        "Jason Alan Fries",
        "Yizhe Xu",
        "Nigam Shah"
    ],
    "pdf_url": "https://openreview.net/pdf/4183f6aeee58dddcc690f9265adb21cd0dac6757.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/4c5fce26-2c1e-55c2-b4de-b1a11a58d66f.pdf",
    "num_pages": 40,
    "abstract": "We present a self-supervised, time-to-event (TTE) foundation model called MOTOR (Many Outcome Time Oriented Representations) which is pretrained on timestamped sequences of events in electronic health records (EHR) and health insurance claims. TTE models are used for estimating the probability distribution of the time until a specific event occurs, which is an important task in medical settings. TTE models provide many advantages over classification using fixed time horizons, including naturally handling censored observations, but are challenging to train with limited labeled data. MOTOR addresses this challenge by pretraining on up to 55M patient records (9B clinical events). We evaluate MOTOR's transfer learning performance on 19 tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative claims data). Task-specific models adapted from MOTOR improve time-dependent C statistics by 4.6\\% over state-of-the-art, improve label efficiency by up to 95\\%, and are more robust to temporal distributional shifts. We further evaluate cross-site portability by adapting our MOTOR foundation model for six prediction tasks on the MIMIC-IV dataset, where it outperforms all baselines. MOTOR is the first foundation model for medical TTE predictions and we release a 143M parameter pretrained model for research use at https://huggingface.co/StanfordShahLab/motor-t-base.",
    "tldr": "We introduce MOTOR, a foundation model for time-to-event predictions in healthcare that is trained on up to 55M patient records and outperforms existing state-of-the-art TTE methods.",
    "tags": [
        "foundation models",
        "time-to-event",
        "electronic health records",
        "deep learning",
        "self-supervised learning",
        "transfer learning"
    ]
}