{
    "uuid": "8a1e3915-e42d-581e-aa46-9b520f4b03ec",
    "title": "Towards Offline Opponent Modeling with In-context Learning",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\njing2024towards,\ntitle={Towards Offline Opponent Modeling with In-context Learning},\nauthor={Yuheng Jing and Kai Li and Bingyun Liu and Yifan Zang and Haobo Fu and QIANG FU and Junliang Xing and Jian Cheng},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=2SwHngthig}\n}",
    "authors": [
        "Yuheng Jing",
        "Kai Li",
        "Bingyun Liu",
        "Yifan Zang",
        "Haobo Fu",
        "QIANG FU",
        "Junliang Xing",
        "Jian Cheng"
    ],
    "pdf_url": "https://openreview.net/pdf/ebdbd616b536556e85afb869974a43d60b721e11.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/8a1e3915-e42d-581e-aa46-9b520f4b03ec.pdf",
    "num_pages": 35,
    "abstract": "Opponent modeling aims at learning the opponent's behaviors, goals, or beliefs to reduce the uncertainty of the competitive environment and assist decision-making. Existing work has mostly focused on learning opponent models online, which is impractical and inefficient in practical scenarios. To this end, we formalize an Offline Opponent Modeling (OOM) problem with the objective of utilizing pre-collected offline datasets to learn opponent models that characterize the opponent from the viewpoint of the controlled agent, which aids in adapting to the unknown fixed policies of the opponent. Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent (TAO), for OOM. Essentially, TAO tackles the problem by harnessing the full potential of the supervised pre-trained Transformers' in-context learning capabilities. The foundation of TAO lies in three stages: an innovative offline policy embedding learning stage, an offline opponent-aware response policy training stage, and a deployment stage for opponent adaptation with in-context learning. Theoretical analysis establishes TAO's equivalence to Bayesian posterior sampling in opponent modeling and guarantees TAO's convergence in opponent policy recognition. Extensive experiments and ablation studies on competitive environments with sparse and dense rewards demonstrate the impressive performance of TAO. Our approach manifests remarkable prowess for fast adaptation, especially in the face of unseen opponent policies, confirming its in-context learning potency.",
    "tldr": "Proposes TAO, a method for offline opponent modeling using in-context learning.",
    "tags": [
        "Opponent Modeling",
        "Offline",
        "Transformer"
    ]
}