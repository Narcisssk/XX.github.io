{
    "uuid": "069d474f-f4de-5eb8-a6a9-97aad73f0e74",
    "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nkarthik2024visionbylanguage,\ntitle={Vision-by-Language for Training-Free Compositional Image Retrieval},\nauthor={Shyamgopal Karthik and Karsten Roth and Massimiliano Mancini and Zeynep Akata},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=EDPxCjXzSb}\n}",
    "authors": [
        "Shyamgopal Karthik",
        "Karsten Roth",
        "Massimiliano Mancini",
        "Zeynep Akata"
    ],
    "pdf_url": "https://openreview.net/pdf/0f4e27a3a0c5ab1ab0875b9ae007336586ef0e0a.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/069d474f-f4de-5eb8-a6a9-97aad73f0e74.pdf",
    "num_pages": 16,
    "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we proposeto tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
    "tldr": "A simple method using off-the-shelf foundation models for Composed Image Retrieval without any training",
    "tags": [
        "Vision-Language Models",
        "Large Language Models"
    ]
}