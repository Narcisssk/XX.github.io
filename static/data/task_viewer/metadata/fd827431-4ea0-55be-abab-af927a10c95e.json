{
    "uuid": "fd827431-4ea0-55be-abab-af927a10c95e",
    "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 spotlight",
    "bibtex": "@inproceedings{\nli2023inferencetime,\ntitle={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},\nauthor={Kenneth Li and Oam Patel and Fernanda Vi{\\'e}gas and Hanspeter Pfister and Martin Wattenberg},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=aLLuYpn83y}\n}",
    "authors": [
        "Kenneth Li",
        "Oam Patel",
        "Fernanda Vi√©gas",
        "Hanspeter Pfister",
        "Martin Wattenberg"
    ],
    "pdf_url": "https://openreview.net/pdf/f8231bfc5b98fc20e2532901e878e7858d1c0970.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/fd827431-4ea0-55be-abab-af927a10c95e.pdf",
    "num_pages": 80,
    "abstract": "We introduce Inference-Time Intervention (ITI), a technique designed to enhance the \"truthfulness\" of large language models (LLMs). ITI operates by shifting model activations during inference, following a learned set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from $32.5\\%$ to $65.1\\%$. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.",
    "tldr": "We introduce Inference-Time Intervention (ITI), a light-weight technique that enhances the truthfulness of large language models (LLMs).",
    "tags": [
        "Large Language Model",
        "AI Safety"
    ]
}