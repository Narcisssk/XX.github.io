{
    "uuid": "2e59e537-6583-5f0c-b01d-e4963325edc4",
    "title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "MATH-AI 24",
    "bibtex": "@inproceedings{\nmishra2024mathcamps,\ntitle={Math{CAMPS}: Fine-grained Synthesis of Mathematical Problems From Human Curricula},\nauthor={Shubhra Mishra and Gabriel Poesia and Belinda Mo and Noah Goodman},\nbooktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},\nyear={2024},\nurl={https://openreview.net/forum?id=ICYF91UPjE}\n}",
    "authors": [
        "Shubhra Mishra",
        "Gabriel Poesia",
        "Belinda Mo",
        "Noah Goodman"
    ],
    "pdf_url": "https://openreview.net/pdf/d6451fbc34b1dacd625335abb2daa3d4811b7b3a.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/2e59e537-6583-5f0c-b01d-e4963325edc4.pdf",
    "num_pages": 16,
    "abstract": "Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses. Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained ``standards'' from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derive \\emph{follow-up questions} from symbolic structures and convert them into follow-up word problems---a novel task of mathematical dialogue that probes for robustness in understanding. Experiments on 23 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets. Project page: https://mathcamps.cc.",
    "tldr": "A synthetic, fine-grained dataset generation pipeline of math word problems grounded on abilities from human curricula",
    "tags": [
        "datasets",
        "mathematical reasoning",
        "word problems",
        "large language models"
    ]
}