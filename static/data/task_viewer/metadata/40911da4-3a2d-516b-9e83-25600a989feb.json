{
    "uuid": "40911da4-3a2d-516b-9e83-25600a989feb",
    "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 oral",
    "bibtex": "@inproceedings{\nlu2024mathvista,\ntitle={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},\nauthor={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=KUNzEQMWU7}\n}",
    "authors": [
        "Pan Lu",
        "Hritik Bansal",
        "Tony Xia",
        "Jiacheng Liu",
        "Chunyuan Li",
        "Hannaneh Hajishirzi",
        "Hao Cheng",
        "Kai-Wei Chang",
        "Michel Galley",
        "Jianfeng Gao"
    ],
    "pdf_url": "https://openreview.net/pdf/787a339a2bb6e601216540a43a659322ff3e4e9e.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/40911da4-3a2d-516b-9e83-25600a989feb.pdf",
    "num_pages": 116,
    "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
    "tldr": "We introduce MathVista, a novel benchmark for evaluating mathematical reasoning capabilities within visual contexts, and conduct extensive experiments on 11 foundation models.",
    "tags": [
        "large language models",
        "large multimodal models",
        "mathematical reasoning",
        "vision-language reasoning",
        "foundation models and their evaluations"
    ]
}