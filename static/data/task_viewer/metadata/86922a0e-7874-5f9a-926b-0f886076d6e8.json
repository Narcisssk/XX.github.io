{
    "uuid": "86922a0e-7874-5f9a-926b-0f886076d6e8",
    "title": "Visual Instruction Tuning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 oral",
    "bibtex": "@inproceedings{\nliu2023visual,\ntitle={Visual Instruction Tuning},\nauthor={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=w0H2xGHlkw}\n}",
    "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
    ],
    "pdf_url": "https://openreview.net/pdf/8ec2de30800edb13f33bf46d3f735b91f7561ce0.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/86922a0e-7874-5f9a-926b-0f886076d6e8.pdf",
    "num_pages": 25,
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.",
    "tldr": "Introduces LLaVA, a multimodal model for visual instruction tuning with GPT-4.",
    "tags": [
        "visual instruction tuning",
        "instruction tuning",
        "multimodal",
        "LLM",
        "GPT"
    ]
}