{
    "uuid": "c6940e70-2fd5-5155-9130-800533243df2",
    "title": "Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nshu2024realtime,\ntitle={Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices},\nauthor={Zhihao Shu and Xiaowei Yu and Zihao Wu and Wenqi Jia and Yinchen Shi and Miao Yin and Tianming Liu and Dajiang Zhu and Wei Niu},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=lD7ziaMHbf}\n}",
    "authors": [
        "Zhihao Shu",
        "Xiaowei Yu",
        "Zihao Wu",
        "Wenqi Jia",
        "Yinchen Shi",
        "Miao Yin",
        "Tianming Liu",
        "Dajiang Zhu",
        "Wei Niu"
    ],
    "pdf_url": "https://openreview.net/pdf/bcb3daacd691d78e3a034dacbc9da8c718a545ff.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/c6940e70-2fd5-5155-9130-800533243df2.pdf",
    "num_pages": 20,
    "abstract": "Mobile devices have become essential enablers for AI applications, particularly in scenarios that require real-time performance. Vision Transformer (ViT) has become a fundamental cornerstone in this regard due to its high accuracy. Recent efforts have been dedicated to developing various transformer architectures that offer im- proved accuracy while reducing the computational requirements. However, existing research primarily focuses on reducing the theoretical computational complexity through methods such as local attention and model pruning, rather than considering realistic performance on mobile hardware. Although these optimizations reduce computational demands, they either introduce additional overheads related to data transformation (e.g., Reshape and Transpose) or irregular computation/data-access patterns. These result in significant overhead on mobile devices due to their limited bandwidth, which even makes the latency worse than vanilla ViT on mobile. In this paper, we present ECP-ViT, a real-time framework that employs the core-periphery principle inspired by the brain functional networks to guide self-attention in ViTs and enable the deployment of ViT models on smartphones. We identify the main bottleneck in transformer structures caused by data transformation and propose a hardware-friendly core-periphery guided self-attention to decrease computation demands. Additionally, we design the system optimizations for intensive data transformation in pruned models. ECP-ViT, with the proposed algorithm-system co-optimizations, achieves a speedup of 4.6× to 26.9× on mobile GPUs across four datasets: STL-10, CIFAR100, TinyImageNet, and ImageNet.",
    "tldr": "ECP-ViT accelerates Vision Transformers for real-time mobile AI applications.",
    "tags": [
        "Mobile DNN Acceleration",
        "Model Pruning",
        "ViT"
    ]
}