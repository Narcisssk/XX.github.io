{
    "uuid": "8e85718a-1210-52bf-9aee-9b2bb2d0ae59",
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{shen-etal-2024-small,\n    title = \"Small {LLM}s Are Weak Tool Learners: A Multi-{LLM} Agent\",\n    author = \"Shen, Weizhou  and\n      Li, Chenliang  and\n      Chen, Hongzhan  and\n      Yan, Ming  and\n      Quan, Xiaojun  and\n      Chen, Hehong  and\n      Zhang, Ji  and\n      Huang, Fei\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.929\",\n    doi = \"10.18653/v1/2024.emnlp-main.929\",\n    pages = \"16658--16680\",\n    abstract = \"Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.\",\n}\n",
    "authors": [
        "Weizhou Shen",
        "Chenliang Li",
        "Hongzhan Chen",
        "Ming Yan",
        "Xiaojun Quan",
        "Hehong Chen",
        "Ji Zhang",
        "Fei Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.emnlp-main.929.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/8e85718a-1210-52bf-9aee-9b2bb2d0ae59.pdf",
    "num_pages": 23,
    "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.",
    "tldr": "Multi-LLM framework enhances tool learning by decomposing tasks among agents.",
    "tags": [
        "LLMs",
        "tool learning",
        "multi-agent systems",
        "task planning",
        "modular framework"
    ]
}