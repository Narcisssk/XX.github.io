{
    "uuid": "5e2682ca-f1c4-536d-bd61-0a2f0055f435",
    "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    "conference_full": "The 2023 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2023,
    "volume": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{zhong-etal-2023-mquake,\n    title = \"{MQ}u{AKE}: Assessing Knowledge Editing in Language Models via Multi-Hop Questions\",\n    author = \"Zhong, Zexuan  and\n      Wu, Zhengxuan  and\n      Manning, Christopher  and\n      Potts, Christopher  and\n      Chen, Danqi\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.971\",\n    doi = \"10.18653/v1/2023.emnlp-main.971\",\n    pages = \"15686--15702\",\n    abstract = \"The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model{'}s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.\",\n}\n",
    "authors": [
        "Zexuan Zhong",
        "Zhengxuan Wu",
        "Christopher Manning",
        "Christopher Potts",
        "Danqi Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.emnlp-main.971.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2023/5e2682ca-f1c4-536d-bd61-0a2f0055f435.pdf",
    "num_pages": 17,
    "abstract": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the modelâ€™s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
    "tldr": "MQuAKE benchmarks knowledge editing in language models with multi-hop questions.",
    "tags": [
        "MQuAKE",
        "knowledge editing",
        "multi-hop questions",
        "language models",
        "model evaluation"
    ]
}