{
    "uuid": "7476352a-0300-5ef7-9b32-346296a6b7be",
    "title": "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{liu-etal-2024-evedit,\n    title = \"{EVEDIT}: Event-based Knowledge Editing for Deterministic Knowledge Propagation\",\n    author = \"Liu, Jiateng  and\n      Yu, Pengfei  and\n      Zhang, Yuji  and\n      Li, Sha  and\n      Zhang, Zixuan  and\n      Sarikaya, Ruhi  and\n      Small, Kevin  and\n      Ji, Heng\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.282\",\n    doi = \"10.18653/v1/2024.emnlp-main.282\",\n    pages = \"4907--4926\",\n    abstract = \"The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as {''}deduction anchors{''}. However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6{\\%} consistency improvement while maintaining the naturalness of generation.\",\n}\n",
    "authors": [
        "Jiateng Liu",
        "Pengfei Yu",
        "Yuji Zhang",
        "Sha Li",
        "Zixuan Zhang",
        "Ruhi Sarikaya",
        "Kevin Small",
        "Heng Ji"
    ],
    "pdf_url": "https://aclanthology.org/2024.emnlp-main.282.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/7476352a-0300-5ef7-9b32-346296a6b7be.pdf",
    "num_pages": 20,
    "abstract": "The dynamic nature of real-world information necessitates knowledge editing (KE) in large language models (LLMs). The edited knowledge should propagate and facilitate the deduction of new information based on existing model knowledge. We term the existing related knowledge in LLM serving as the origination of knowledge propagation as ”deduction anchors”. However, current KE approaches, which only operate on (subject, relation, object) triple. We both theoretically and empirically observe that this simplified setting often leads to uncertainty when determining the deduction anchors, causing low confidence in their answers. To mitigate this issue, we propose a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor and enabling LLMs to propagate knowledge confidently. We curate a new benchmark dataset Evedit derived from the CounterFact dataset and validate its superiority in improving model confidence. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation.",
    "tldr": "Event-based knowledge editing enhances confidence in language model knowledge propagation.",
    "tags": [
        "knowledge editing",
        "language models",
        "knowledge propagation",
        "event-based learning",
        "deduction anchors"
    ]
}