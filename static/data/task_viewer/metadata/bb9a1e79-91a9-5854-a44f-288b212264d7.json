{
    "uuid": "bb9a1e79-91a9-5854-a44f-288b212264d7",
    "title": "Let's Verify Step by Step",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nlightman2024lets,\ntitle={Let's Verify Step by Step},\nauthor={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=v8L0pN6EOi}\n}",
    "authors": [
        "Hunter Lightman",
        "Vineet Kosaraju",
        "Yuri Burda",
        "Harrison Edwards",
        "Bowen Baker",
        "Teddy Lee",
        "Jan Leike",
        "John Schulman",
        "Ilya Sutskever",
        "Karl Cobbe"
    ],
    "pdf_url": "https://openreview.net/pdf/37b0e73493f53476771d827252aed413949ebacb.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/bb9a1e79-91a9-5854-a44f-288b212264d7.pdf",
    "num_pages": 24,
    "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
    "tldr": "We use step-level human feedback to build a robust verifier of LLM reasoning that gets 78% accuracy on a subset of the MATH test set.",
    "tags": [
        "LLMs",
        "chain of thought",
        "verifiers",
        "human feedback",
        "reasoning"
    ]
}