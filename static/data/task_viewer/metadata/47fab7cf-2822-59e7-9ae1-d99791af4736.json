{
    "uuid": "47fab7cf-2822-59e7-9ae1-d99791af4736",
    "title": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 spotlight",
    "bibtex": "@inproceedings{\nshaw2023from,\ntitle={From Pixels to {UI} Actions: Learning to Follow Instructions via Graphical User Interfaces},\nauthor={Peter Shaw and Mandar Joshi and James Cohan and Jonathan Berant and Panupong Pasupat and Hexiang Hu and Urvashi Khandelwal and Kenton Lee and Kristina Toutanova},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=3PjCt4kmRx}\n}",
    "authors": [
        "Peter Shaw",
        "Mandar Joshi",
        "James Cohan",
        "Jonathan Berant",
        "Panupong Pasupat",
        "Hexiang Hu",
        "Urvashi Khandelwal",
        "Kenton Lee",
        "Kristina Toutanova"
    ],
    "pdf_url": "https://openreview.net/pdf/a52a52e0e8e330a8e2a1416c429852ed72b1446a.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/47fab7cf-2822-59e7-9ae1-d99791af4736.pdf",
    "num_pages": 17,
    "abstract": "Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces.  This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use â€” via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.",
    "tldr": "We study GUI-based instruction following with a general observation and action space consisting of pixel-based inputs and low-level actions.",
    "tags": [
        "instruction following",
        "web tasks",
        "user interface tasks",
        "vision and language",
        "representation learning",
        "reinforcement learning",
        "imitation learning",
        "tree search",
        "language grounding",
        "web agents",
        "computer control"
    ]
}