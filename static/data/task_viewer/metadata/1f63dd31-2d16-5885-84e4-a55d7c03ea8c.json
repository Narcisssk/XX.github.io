{
    "uuid": "1f63dd31-2d16-5885-84e4-a55d7c03ea8c",
    "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
    "conference_full": "The 2023 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: EMNLP 2023",
    "bibtex": "@inproceedings{ferron-etal-2023-meep,\n    title = \"{MEEP}: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings\",\n    author = \"Ferron, Amila  and\n      Shore, Amber  and\n      Mitra, Ekata  and\n      Agrawal, Ameeta\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2023\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-emnlp.137\",\n    doi = \"10.18653/v1/2023.findings-emnlp.137\",\n    pages = \"2078--2100\",\n    abstract = \"As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the response without the conversation history, are designed for one dataset, or have limited correlation with human annotations. Furthermore, they have been tested exclusively on English conversations. Given that dialogue systems are increasingly available in languages beyond English, multilingual evaluation capabilities are essential. We propose that large language models (LLMs) may be used for evaluation of engagingness in dialogue through prompting, and ask how prompt constructs and translated prompts compare in a multilingual setting. We provide a prompt-design taxonomy for engagingness and find that using selected prompt elements with LLMs, including our comprehensive definition of engagingness, outperforms state-of-the-art methods on evaluation of engagingness in dialogue across multiple languages.\",\n}\n",
    "authors": [
        "Amila Ferron",
        "Amber Shore",
        "Ekata Mitra",
        "Ameeta Agrawal"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-emnlp.137.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2023/1f63dd31-2d16-5885-84e4-a55d7c03ea8c.pdf",
    "num_pages": 23,
    "abstract": "As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the response without the conversation history, are designed for one dataset, or have limited correlation with human annotations. Furthermore, they have been tested exclusively on English conversations. Given that dialogue systems are increasingly available in languages beyond English, multilingual evaluation capabilities are essential. We propose that large language models (LLMs) may be used for evaluation of engagingness in dialogue through prompting, and ask how prompt constructs and translated prompts compare in a multilingual setting. We provide a prompt-design taxonomy for engagingness and find that using selected prompt elements with LLMs, including our comprehensive definition of engagingness, outperforms state-of-the-art methods on evaluation of engagingness in dialogue across multiple languages.",
    "tldr": "Large language models improve multilingual dialogue engagingness evaluation.",
    "tags": [
        "dialogue evaluation",
        "engagingness",
        "large language models",
        "multilingual settings",
        "prompt design"
    ]
}