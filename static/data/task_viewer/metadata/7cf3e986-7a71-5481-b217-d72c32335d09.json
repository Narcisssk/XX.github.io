{
    "uuid": "7cf3e986-7a71-5481-b217-d72c32335d09",
    "title": "Rough Transformers for Continuous and Efficient Time-Series Modelling",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "TS4H Poster",
    "bibtex": "@inproceedings{\nmoreno-pino2024rough,\ntitle={Rough Transformers for Continuous and Efficient Time-Series Modelling},\nauthor={Fernando Moreno-Pino and Alvaro Arroyo and Harrison Waldon and Xiaowen Dong and Alvaro Cartea},\nbooktitle={ICLR 2024 Workshop on Learning from Time Series For Health},\nyear={2024},\nurl={https://openreview.net/forum?id=liFaAQ8C0z}\n}",
    "authors": [
        "Fernando Moreno-Pino",
        "´Alvaro Arroyo",
        "Harrison Waldon",
        "Xiaowen Dong",
        "´Alvaro Cartea"
    ],
    "pdf_url": "https://openreview.net/pdf/f4828521b7644e126d523789a3099ed6b32cecc5.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/7cf3e986-7a71-5481-b217-d72c32335d09.pdf",
    "num_pages": 11,
    "abstract": "Time-series data in real-world medical settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In such contents, traditional sequence-based recurrent models struggle. To overcome this, researchers replace recurrent architectures with Neural ODE-based models to model irregularly sampled data and use Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of moderate lengths and greater. To mitigate this, we introduce the Rough Transformer, a variation of the Transformer model which operates on continuous-time representations of input sequences and incurs significantly reduced computational costs, critical for addressing long-range dependencies common in medical contexts. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global dependencies in input data, while remaining robust to changes in the sequence length and sampling frequency. We find that Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the benefits of Neural ODE-based models using a fraction of the computational time and memory resources on synthetic and real-world time-series tasks.",
    "tldr": "We propose a Transformer architecture which operates on continuous-time representations of the sequential medical data and incurs significantly lower computational costs",
    "tags": [
        "efficient transformers",
        "continuous-time models",
        "sequential medical data"
    ]
}