{
    "uuid": "1d2eb7a7-8dfa-5bc0-a906-b27725667ff4",
    "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "Instruction Workshop @ NeurIPS 2023",
    "bibtex": "@inproceedings{\nwang2023fingpt,\ntitle={Fin{GPT}: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},\nauthor={Neng Wang and Hongyang Yang and Christina Wang},\nbooktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},\nyear={2023},\nurl={https://openreview.net/forum?id=FuOMomaQa8}\n}",
    "authors": [
        "Neng Wang",
        "Hongyang Yang",
        "Christina Wang"
    ],
    "pdf_url": "https://openreview.net/pdf/0d1fdb7d0131a1692996cee57ee13cf351314857.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/1d2eb7a7-8dfa-5bc0-a906-b27725667ff4.pdf",
    "num_pages": 12,
    "abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
    "tldr": "FinGPT benchmarks instruction tuning for open-source large language models in finance.",
    "tags": [
        "Open-Source Large Language Models",
        "Instruction Tuning Paradigm",
        "Financial Datasets",
        "GPT-Based Models"
    ]
}