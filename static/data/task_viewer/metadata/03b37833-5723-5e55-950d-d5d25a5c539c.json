{
    "uuid": "03b37833-5723-5e55-950d-d5d25a5c539c",
    "title": "ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations",
    "conference_full": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)",
    "conference": "icassp",
    "year": 2023,
    "volume": "ICASSP session SLT: Spoken Language Technology",
    "bibtex": "@inproceedings{DBLP:conf/icassp/HussainNHLG23,\n  author       = {Shehzeen Hussain and\n                  Paarth Neekhara and\n                  Jocelyn Huang and\n                  Jason Li and\n                  Boris Ginsburg},\n  title        = {{ACE-VC:} Adaptive and Controllable Voice Conversion Using Explicitly\n                  Disentangled Self-Supervised Speech Representations},\n  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing\n                  {ICASSP} 2023, Rhodes Island, Greece, June 4-10, 2023},\n  pages        = {1--5},\n  publisher    = {{IEEE}},\n  year         = {2023},\n  url          = {https://doi.org/10.1109/ICASSP49357.2023.10094850},\n  doi          = {10.1109/ICASSP49357.2023.10094850},\n  timestamp    = {Sun, 04 Aug 2024 19:36:21 +0200},\n  biburl       = {https://dblp.org/rec/conf/icassp/HussainNHLG23.bib},\n  bibsource    = {dblp computer science bibliography, https://dblp.org}\n}",
    "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
    ],
    "pdf_url": "https://doi.org/10.1109/ICASSP49357.2023.10094850",
    "pdf_path": "data/dataset/airqa/papers/icassp2023/03b37833-5723-5e55-950d-d5d25a5c539c.pdf",
    "num_pages": 5,
    "abstract": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
    "tldr": "Zero-shot voice conversion using disentangled self-supervised speech representations.",
    "tags": [
        "voice conversion",
        "self-supervised learning",
        "speech synthesis",
        "speaker adaptation",
        "Siamese networks"
    ]
}