{
    "uuid": "fffa8c85-c16f-566b-a036-0a584e6a445c",
    "title": "Transformers as Multi-Task Feature Selectors: Generalization Analysis of In-Context Learning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "M3L 2023 Poster",
    "bibtex": "@inproceedings{\nli2023transformers,\ntitle={Transformers as Multi-Task Feature Selectors: Generalization Analysis of In-Context Learning},\nauthor={Hongkang Li and Meng Wang and Songtao Lu and Hui Wan and Xiaodong Cui and Pin-Yu Chen},\nbooktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},\nyear={2023},\nurl={https://openreview.net/forum?id=BMQ4i2RVbE}\n}",
    "authors": [
        "Hongkang Li",
        "Meng Wang",
        "Songtao Lu",
        "Hui Wan",
        "Xiaodong Cui",
        "Pin-Yu Chen"
    ],
    "pdf_url": "https://openreview.net/pdf/fe1c4a5d2e31ace533e60b724b26f65061f8fffd.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/fffa8c85-c16f-566b-a036-0a584e6a445c.pdf",
    "num_pages": 28,
    "abstract": "Transformer-based large language models have displayed impressive capabilities in the domain of in-context learning, wherein they use multiple input-output pairs to make predictions on unlabeled test data. To lay the theoretical groundwork for in-context learning, we delve into the optimization and generalization of a single-head, one-layer Transformer in the context of multi-task learning for classification. Our investigation uncovers that lower sample complexity is associated with increased training-relevant features and reduced noise in prompts, resulting in improved learning performance.  The trained model exhibits the mechanism to first attend to demonstrations of training-relevant features and then decode the corresponding label embedding. Furthermore, we delineate the necessary conditions for successful out-of-domain generalization for in-context learning, specifically regarding the relationship between training and testing prompts.",
    "tldr": "We study the optimization and generalization of a single-head, one-layer Transformer for in-context learning on classification tasks.",
    "tags": [
        "in-context learning",
        "transformer",
        "deep learning theory",
        "generalization analysis"
    ]
}