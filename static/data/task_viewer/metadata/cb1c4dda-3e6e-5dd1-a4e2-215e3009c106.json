{
    "uuid": "cb1c4dda-3e6e-5dd1-a4e2-215e3009c106",
    "title": "Improved Baselines with Visual Instruction Tuning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "Instruction Workshop @ NeurIPS 2023",
    "bibtex": "@inproceedings{\nliu2023improved,\ntitle={Improved Baselines with Visual Instruction Tuning},\nauthor={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},\nbooktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},\nyear={2023},\nurl={https://openreview.net/forum?id=yx3Hkx5ved}\n}",
    "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
    ],
    "pdf_url": "https://openreview.net/pdf/f9c6886e50905d8aab3cc7084c39bb49e808bc27.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/cb1c4dda-3e6e-5dd1-a4e2-215e3009c106.pdf",
    "num_pages": 9,
    "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
    "tldr": "Enhancing visual instruction tuning to achieve state-of-the-art LMM performance.",
    "tags": [
        "visual instruction tuning",
        "instruction tuning",
        "multimodal",
        "LLM",
        "GPT"
    ]
}