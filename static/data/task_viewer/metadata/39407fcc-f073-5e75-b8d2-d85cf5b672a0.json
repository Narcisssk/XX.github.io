{
    "uuid": "39407fcc-f073-5e75-b8d2-d85cf5b672a0",
    "title": "Global Learning with Triplet Relations in Abstractive Summarization",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 28th Conference on Computational Natural Language Learning",
    "bibtex": "@inproceedings{lu-etal-2024-global,\n    title = \"Global Learning with Triplet Relations in Abstractive Summarization\",\n    author = \"Lu, Fengyu  and\n      Duan, Jiaxin  and\n      Liu, Junfei\",\n    editor = \"Barak, Libby  and\n      Alikhani, Malihe\",\n    booktitle = \"Proceedings of the 28th Conference on Computational Natural Language Learning\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, FL, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.conll-1.15/\",\n    doi = \"10.18653/v1/2024.conll-1.15\",\n    pages = \"198--208\",\n    abstract = \"Abstractive summarization models learned with token-level maximum likelihood estimation suffer from exposure bias, that the condition for predicting the next token is discrepant during training and inference. Existing solutions bridge this gap by learning to estimate semantic or lexical qualities of a candidate summary from the global view, namely global learning (GL), yet ignore maintaining rational triplet-relations among document, reference summary, and candidate summaries, e.g., the candidate and reference summaries should have a similar faithfulness degree judging by a source document. In this paper, we propose an iterative autoregressive summarization paradigm - IARSum, which fuses the learning of triplet relations into a GL framework and further enhances summarization performance. Specifically, IARSum develops a dual-encoder network to enable the simultaneous input of a document and its candidate (or reference) summary. On this basis, it learns to 1) model the relative semantics defined over tuples (candidate, document) and (reference, document) respectively and balance them; 2) reduce lexical differences between candidate and reference summaries. Furthermore, IARSum iteratively reprocesses a generated candidate at inference time to ground higher quality. We conduct extensive experiments on two widely used datasets to test our method, and IARSum shows the new or matched state-of-the-art on diverse metrics.\"\n}\n",
    "authors": [
        "Fengyu Lu",
        "Jiaxin Duan",
        "Junfei Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.conll-1.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/39407fcc-f073-5e75-b8d2-d85cf5b672a0.pdf",
    "num_pages": 11,
    "abstract": "Abstractive summarization models learned with token-level maximum likelihood estimation suffer from exposure bias, that the condition for predicting the next token is discrepant during training and inference. Existing solutions bridge this gap by learning to estimate semantic or lexical qualities of a candidate summary from the global view, namely global learning (GL), yet ignore maintaining rational triplet-relations among document, reference summary, and candidate summaries, e.g., the candidate and reference summaries should have a similar faithfulness degree judging by a source document. In this paper, we propose an iterative autoregressive summarization paradigm - IARSum, which fuses the learning of triplet relations into a GL framework and further enhances summarization performance. Specifically, IARSum develops a dual-encoder network to enable the simultaneous input of a document and its candidate (or reference) summary. On this basis, it learns to 1) model the relative semantics defined over tuples (candidate, document) and (reference, document) respectively and balance them; 2) reduce lexical differences between candidate and reference summaries. Furthermore, IARSum iteratively reprocesses a generated candidate at inference time to ground higher quality. We conduct extensive experiments on two widely used datasets to test our method, and IARSum shows the new or matched state-of-the-art on diverse metrics.",
    "tldr": "IARSum improves abstractive summarization by integrating triplet relations in global learning.",
    "tags": [
        "Global Learning",
        "Triplet Relations",
        "Abstractive Summarization",
        "Iterative Autoregressive Model",
        "Exposure Bias"
    ]
}