{
    "uuid": "709faba9-ce93-5d4d-a1d5-ce25e9bad2ba",
    "title": "Distinguished In Uniform: Self-Attention Vs. Virtual Nodes",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nrosenbluth2024distinguished,\ntitle={Distinguished In Uniform: Self-Attention Vs. Virtual Nodes},\nauthor={Eran Rosenbluth and Jan T{\\\"o}nshoff and Martin Ritzert and Berke Kisin and Martin Grohe},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=AcSChDWL6V}\n}",
    "authors": [
        "Eran Rosenbluth",
        "Jan Tönshoff",
        "Martin Ritzert",
        "Berke Kisin",
        "Martin Grohe"
    ],
    "pdf_url": "https://openreview.net/pdf/e1ac40eba4c49945d38feedf14666ad5b8b2974c.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/709faba9-ce93-5d4d-a1d5-ce25e9bad2ba.pdf",
    "num_pages": 21,
    "abstract": "Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network.\n\nWe first clarify that this form of universality is not unique to GTs: Using the same positional encodings, also pure MPGNNs and even 2-layer MLPs are non-uniform universal approximators. We then consider uniform expressivity: The target function is to be approximated by a single network for graphs of all sizes. There, we compare GTs to the more efficient MPGNN + Virtual Node architecture. The essential difference between the two model definitions is in their global computation method: Self-Attention Vs Virtual Node. We prove that none of the models is a uniform-universal approximator, before proving our main result: Neither model’s uniform expressivity subsumes the other’s. We demonstrate the theory with experiments on synthetic data. We further augment our study with real-world datasets, observing mixed results which indicate no clear ranking in practice as well.",
    "tldr": "Graph Transformers and MPGNNs with virtual nodes do not subsume each other in terms of uniform function approximation while neither is \"universal\" in this setting.",
    "tags": [
        "Graph Neural Networks",
        "Message Passing",
        "Graph Transformers",
        "Virtual Nodes",
        "Expressivity",
        "Uniform Expressivity"
    ]
}