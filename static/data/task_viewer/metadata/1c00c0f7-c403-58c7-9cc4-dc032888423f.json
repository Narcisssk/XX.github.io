{
    "uuid": "1c00c0f7-c403-58c7-9cc4-dc032888423f",
    "title": "kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 Track Datasets and Benchmarks Poster",
    "bibtex": "@inproceedings{\nmathai2024kgym,\ntitle={kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution},\nauthor={Alex Mathai and Chenxi Huang and Petros Maniatis and Aleksandr Nogikh and Franjo Ivancic and Junfeng Yang and Baishakhi Ray},\nbooktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2024},\nurl={https://openreview.net/forum?id=zQ3qU0xWZ5}\n}",
    "authors": [
        "Alex Mathai",
        "Chenxi Huang",
        "Petros Maniatis",
        "Aleksandr Nogikh",
        "Franjo Ivancic",
        "Junfeng Yang",
        "Baishakhi Ray"
    ],
    "pdf_url": "https://openreview.net/pdf/2a752de5b3162b26d4d55ec43f717266b0464044.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/1c00c0f7-c403-58c7-9cc4-dc032888423f.pdf",
    "num_pages": 26,
    "abstract": "Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. \nUnlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if machine learning (ML) models are useful while developing such large-scale systems-level software, we introduce kGym (a platform) and kBench (a dataset). The kGym platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use kGym to facilitate evaluation on kBench, a crash resolution benchmark drawn from real-world Linux kernel bugs.  An example bug in kBench contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72\\% and 5.38\\% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on kBench requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.",
    "tldr": "A platform and dataset to benchmark ML models at resolving bugs in the Linux kernel",
    "tags": [
        "Benchmarks",
        "Datasets",
        "Natural Language Processing",
        "Linux Kernel",
        "Code LLMs"
    ]
}