{
    "uuid": "eb4622eb-b4c7-5fae-b8e2-ff799aa81e4d",
    "title": "Grounding Multimodal Large Language Models to the World",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\npeng2024grounding,\ntitle={Grounding Multimodal Large Language Models to the World},\nauthor={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=lLmqxkfSIw}\n}",
    "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Qixiang Ye",
        "Furu Wei"
    ],
    "pdf_url": "https://openreview.net/pdf/0ea36b222b82ac76c018c9aa7a47f9f978c705b2.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/eb4622eb-b4c7-5fae-b8e2-ff799aa81e4d.pdf",
    "num_pages": 24,
    "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown, i.e., [text span](bounding boxes), where object descriptions are sequences of location tokens. To train the model, we construct a large-scale dataset about grounded image-text pairs (GrIT) together with multimodal corpora. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability to downstream applications, while maintaining the conventional capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning). Kosmos-2 is evaluated on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This study sheds a light on the big convergence of language, multimodal perception, and world modeling, which is a key step toward artificial general intelligence. Code can be found in [https://aka.ms/kosmos-2](https://aka.ms/kosmos-2).",
    "tldr": "Kosmos-2 is a multimodal model enhancing language grounding in visual contexts.",
    "tags": [
        "large language model",
        "multimodal large language model",
        "grounding",
        "referring"
    ]
}