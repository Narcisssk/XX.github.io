{
    "uuid": "a75bc472-a98f-567f-af29-5df27fdff64b",
    "title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nzhou2024what,\ntitle={What Algorithms can Transformers Learn? A Study in Length Generalization},\nauthor={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum Nakkiran},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=AssIuHnmHX}\n}",
    "authors": [
        "Hattie Zhou",
        "Arwen Bradley",
        "Etai Littwin",
        "Noam Razin",
        "Omid Saremi",
        "Joshua M. Susskind",
        "Samy Bengio",
        "Preetum Nakkiran"
    ],
    "pdf_url": "https://openreview.net/pdf/5d2d4bf238aa8f548ba72df177f3c7dbcf06d72a.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/a75bc472-a98f-567f-af29-5df27fdff64b.pdf",
    "num_pages": 29,
    "abstract": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. In this work, we focus on length generalization, and we propose a unifying framework to understand when and how Transformers can be expected to length generalize on a given task. First, we show that there exist algorithmic tasks for which standard\ndecoder-only Transformers trained from scratch naturally exhibit strong length generalization. For these tasks, we leverage the RASP programming language (Weiss et al., 2021) to show that the correct algorithmic solution which solves the task can be represented by a simple Transformer. We thus propose the RASP-Generalization Conjecture: Transformers tend to learn a length-generalizing solution if there exists a short RASP-L program that works for all input lengths. We present empirical evidence to support the correlation between RASP-simplicity and generalization. We leverage our insights to give new scratchpad formats which yield strong length generalization on traditionally hard tasks (such as parity and addition), and we illustrate how scratchpad can hinder generalization when it increases the complexity of the corresponding RASP-L program. Overall, our work provides a novel perspective on the mechanisms of length generalization and the algorithmic capabilities of Transformers.",
    "tldr": "We show that length generalization of Transformer models trained from scratch strongly correlates with the simplicity of the true RASP-L program for the task.",
    "tags": [
        "length generalization",
        "systematic generalization",
        "understanding",
        "transformer",
        "scratchpad",
        "LLM",
        "algorithmic reasoning"
    ]
}