{
    "uuid": "10e2193c-2fa2-5cf8-9b5d-bc0c32fe856a",
    "title": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nmeng2024deepstack,\ntitle={DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for {LMM}s},\nauthor={Lingchen Meng and Jianwei Yang and Rui Tian and Xiyang Dai and Zuxuan Wu and Jianfeng Gao and Yu-Gang Jiang},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=fXDpDzHTDV}\n}",
    "authors": [
        "Lingchen Meng",
        "Jianwei Yang",
        "Rui Tian",
        "Xiyang Dai",
        "Zuxuan Wu",
        "Jianfeng Gao",
        "Yu-Gang Jiang"
    ],
    "pdf_url": "https://openreview.net/pdf/58a15b88449f1356c5a407e4f805daa23e11edfc.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/10e2193c-2fa2-5cf8-9b5d-bc0c32fe856a.pdf",
    "num_pages": 24,
    "abstract": "Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). \nThe resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. \nThis paper presents a new architecture *DeepStack* for LMMs. \nConsidering $N$ layers in the language and vision transformer of LMMs, we stack the visual tokens into $N$ groups and feed each group to its aligned transformer layer from bottom to top. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply *DeepStack* to both language and vision transformer in LMMs, and \nvalidate the effectiveness of *DeepStack* LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by 2.7 and 2.9 on average across 9 benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts that use the full context length. These gains are particularly pronounced on high-resolution tasks, *e.g.*, 4.2, 11.0, and 4.0 improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply *DeepStack* to vision transformer layers, which brings us a similar amount of improvements, 3.8 on average compared with LLaVA-1.5-7B.",
    "tldr": "In this work, we present DeepStack, a simple yet effective way to connect vision and language in the context of LMMs.",
    "tags": [
        "Large Multi-modal Models",
        "High-resolution Visual Token"
    ]
}