{
    "uuid": "9c7c7762-0132-583c-987a-0fbc89847c55",
    "title": "Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nzhao2024identifying,\ntitle={Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model},\nauthor={Min Zhao and Hongzhou Zhu and Chendong Xiang and Kaiwen Zheng and Chongxuan Li and Jun Zhu},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=o9Lkiv1qpc}\n}",
    "authors": [
        "Min Zhao",
        "Hongzhou Zhu",
        "Chendong Xiang",
        "Kaiwen Zheng",
        "Chongxuan Li",
        "Jun Zhu"
    ],
    "pdf_url": "https://openreview.net/pdf/ce3c145fb629337eae7208aec2f4d75453caa367.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/9c7c7762-0132-583c-987a-0fbc89847c55.pdf",
    "num_pages": 27,
    "abstract": "Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \\url{https://cond-image-leak.github.io/}.",
    "tldr": "Addressing image leakage improves motion in video generated by diffusion models.",
    "tags": [
        "diffusion model",
        "image-to-video diffusion model",
        "video generation",
        "video diffusion model"
    ]
}