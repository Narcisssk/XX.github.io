{
    "uuid": "3f602bc4-14ba-5c76-81e6-89fb1ea38c1b",
    "title": "Learning to Cooperate with Humans using Generative Agents",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nliang2024learning,\ntitle={Learning to Cooperate with Humans using Generative Agents},\nauthor={Yancheng Liang and Daphne Chen and Abhishek Gupta and Simon Shaolei Du and Natasha Jaques},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=v4dXL3LsGX}\n}",
    "authors": [
        "Yancheng Liang",
        "Daphne Chen",
        "Abhishek Gupta",
        "Simon Shaolei Du",
        "Natasha Jaques"
    ],
    "pdf_url": "https://openreview.net/pdf/298dd46c5abad00c26df7b356fb1f3812baaeb74.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/3f602bc4-14ba-5c76-81e6-89fb1ea38c1b.pdf",
    "num_pages": 27,
    "abstract": "Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world.  We show \\emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method---Generative Agent Modeling for Multi-agent Adaptation (GAMMA)---on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.",
    "tldr": "We use generative model to sample partner agents to train a coordinator agent. These agents cooperate well with real human players.",
    "tags": [
        "multi-agent reinforcement learning",
        "human-AI cooperation"
    ]
}