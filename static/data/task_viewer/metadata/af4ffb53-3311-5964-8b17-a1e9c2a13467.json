{
    "uuid": "af4ffb53-3311-5964-8b17-a1e9c2a13467",
    "title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nzhang2024sled,\ntitle={{SLED}: Self Logits Evolution Decoding for Improving Factuality in Large Language Models},\nauthor={Jianyi Zhang and Da-Cheng Juan and Cyrus Rashtchian and Chun-Sung Ferng and Heinrich Jiang and Yiran Chen},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=t7wvJstsiV}\n}",
    "authors": [
        "Jianyi Zhang",
        "Da-Cheng Juan",
        "Cyrus Rashtchian",
        "Chun-Sung Ferng",
        "Heinrich Jiang",
        "Yiran Chen"
    ],
    "pdf_url": "https://openreview.net/pdf/f90243bc5b83369f8668b4ec7f3bcdfc2a7f7f9c.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/af4ffb53-3311-5964-8b17-a1e9c2a13467.pdf",
    "num_pages": 19,
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.",
    "tldr": "Self Logits Evolution Decoding (SLED) improves LLM factuality through better decoding.",
    "tags": [
        "Large Language Models"
    ]
}