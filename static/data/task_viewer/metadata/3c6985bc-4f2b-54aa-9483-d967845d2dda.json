{
    "uuid": "3c6985bc-4f2b-54aa-9483-d967845d2dda",
    "title": "Regularity as Intrinsic Reward for Free Play",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\nsancaktar2023regularity,\ntitle={Regularity as Intrinsic Reward for Free Play},\nauthor={Cansu Sancaktar and Justus Piater and Georg Martius},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=BHHrX3CRE1}\n}",
    "authors": [
        "Cansu Sancaktar",
        "Justus Piater",
        "Georg Martius"
    ],
    "pdf_url": "https://openreview.net/pdf/6d14c7affdc950661caf850922e76e6575fcf49b.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/3c6985bc-4f2b-54aa-9483-d967845d2dda.pdf",
    "num_pages": 29,
    "abstract": "We propose regularity as a novel reward signal for intrinsically-motivated reinforcement learning. Taking inspiration from child development, we postulate that striving for structure and order helps guide exploration towards a subspace of tasks that are not favored by naive uncertainty-based intrinsic rewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR) allows us to operationalize it within model-based reinforcement learning. In a synthetic environment, we showcase the plethora of structured patterns that can emerge from pursuing this regularity objective. We also demonstrate the strength of our method in a multi-object robotic manipulation environment. We incorporate RaIR into free play and use it to complement the modelâ€™s epistemic uncertainty as an intrinsic reward. Doing so, we witness the autonomous construction of towers and other regular structures during free play, which leads to a substantial improvement in zero-shot downstream task performance on assembly tasks.",
    "tldr": "We propose regularity as an intrinsic reward signal, to not bias agents towards chaos by naive novelty seeking objectives and instead favor balance and alignment.",
    "tags": [
        "Intrinsic Motivation",
        "Reinforcement Learning",
        "Model-based Planning",
        "Regularity",
        "Manipulation",
        "Zero-shot Generalization",
        "Unsupervised Exploration"
    ]
}