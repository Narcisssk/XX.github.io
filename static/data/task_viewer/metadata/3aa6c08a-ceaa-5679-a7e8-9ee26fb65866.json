{
    "uuid": "3aa6c08a-ceaa-5679-a7e8-9ee26fb65866",
    "title": "BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 Datasets and Benchmarks Oral",
    "bibtex": "@inproceedings{\nmilani2023bedd,\ntitle={{BEDD}: The Mine{RL} {BASALT} Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks},\nauthor={Stephanie Milani and Anssi Kanervisto and Karolis Ramanauskas and Sander V Schulhoff and Brandon Houghton and Rohin Shah},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2023},\nurl={https://openreview.net/forum?id=D1MOK2t2t2}\n}",
    "authors": [
        "Stephanie Milani",
        "Anssi Kanervisto",
        "Karolis Ramanauskas",
        "Sander V Schulhoff",
        "Brandon Houghton",
        "Rohin Shah"
    ],
    "pdf_url": "https://openreview.net/pdf/4e72d4d2f391475f1704eff68b07b0c5b6b2a40a.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/3aa6c08a-ceaa-5679-a7e8-9ee26fb65866.pdf",
    "num_pages": 12,
    "abstract": "The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These comparisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms.  To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark.",
    "tldr": "To facilitate algorithm development for the BASALT benchmark, we provide a large-scale dataset of human demonstrations and evaluations, along with a streamlined codebase for training, evaluating, and analyzing algorithms.",
    "tags": [
        "learning from human feedback",
        "minecraft",
        "human evaluation",
        "embodied agents",
        "rlhf",
        "demonstrations",
        "benchmark",
        "evaluations"
    ]
}