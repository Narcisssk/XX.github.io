{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: What's the framework proposed in this paper(\"Global Learning with Triplet Relations in Abstractive Summarization\")? Which framework is it similar to? In the source papers of these two frameworks, how many same datasets are they experimented on?\n[Answer Format]: Your answer should be a python list like [\"string1\", \"string2\", integer]. The first element should be a string, representing the name(abbrievation) of the framework proposed in this paper. The second element should be a string, representing the name(abbrievation) of the similar framework. The third element should be an integer, representing the number of common datasets experimented on.\n[Anchor PDF]: '39407fcc-f073-5e75-b8d2-d85cf5b672a0'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to find the framework proposed in the paper \"Global Learning with Triplet Relations in Abstractive Summarization\" and the framework it is similar to. I will use the `metadata` table to get the paper ID and then retrieve the relevant sections or content from the `sections` and `pages` tables.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT paper_id FROM metadata WHERE title = 'Global Learning with Triplet Relations in Abstractive Summarization';\")"}
{"role": "user", "content": "[Observation]:\n{\"paper_id\":\"39407fcc-f073-5e75-b8d2-d85cf5b672a0\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: Now that I have the paper ID, I will retrieve the sections and pages of the paper to find the framework proposed and the framework it is similar to.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"Global Learning with Triplet Relations in Abstractive Summarization\",\"section_content\":\"Fengyu Lu1, Jiaxin Duan1\\\\*, Junfei Liu2\\n1School of Software and Microelectronics, Peking University, Beijing, China 2National Engineering Research Center for Software Engineering, Peking University, Beijing, China {fengyul,duanjx}@stu.pku.edu.cn, liujunfei@pku.edu.cn\"}\n{\"section_title\":\"Abstract\",\"section_content\":\"Abstractive summarization models learned with token-level maximum likelihood estimation suffer from exposure bias, that the condition for predicting the next token is discrepant during training and inference. Existing solutions bridge this gap by learning to estimate semantic or lexical qualities of a candidate summary from the global view, namely global learning (GL), yet ignore maintaining rational tripletrelations among document, reference summary, and candidate summaries, e.g., the candidate and reference summaries should have a similar faithfulness degree judging by a source document. In this paper, we propose an iterative autoregressive summarization paradigm - IARSum, which fuses the learning of triplet relations into a GL framework and further enhances summarization performance. Specifcally, IARSum develops a dual-encoder network to enable the simultaneous input of a document and its candidate (or reference) summary. On this basis, it learns to 1) model the relative semantics defned over tuples (candidate, document) and (reference, document) respectively and balance them; 2) reduce lexical differences between candidate and reference summaries. Furthermore, IARSum iteratively reprocesses a generated candidate at inference time to ground higher quality. We conduct extensive experiments on two widely used datasets to test our method, and IARSum shows the new or matched state-of-the-art on diverse metrics.\"}\n{\"section_title\":\"1 Introduction\",\"section_content\":\"Abstractive summarization is a classical natural language generation (NLG) task, which aims to rewrite a long document into a shorter version, retaining only the salient information (Kumar and Chakkaravarthy, 2023; Xie et al., 2023). In recent years, the advancement of pre-trained language models (PLMs) (Lewis et al., 2020; Zhang\\nFigure 1: Graphicalization of triplet relations among a document, the reference summary, and a candidate summary, where each point of the triangle $\\\\triangle S R C$ presents source document (S), reference summary (R), and candidate summary (C), respectively. Upper right: Traditional GL methods only consider the edge $R C$ and deem it the semantic or lexical gaps between the points $R$ and $C$ , which they learn to minimize. Lower right: SeqCo further considers $S$ and regards each edge as a semantic gap to be minimized. Left: We extend traditional GL methods by treating the side edge $S R\\\\left(S C\\\\right)$ as a relative semantics metric, and we highlight the balance of edges $S R$ and $S C$ .\\net al., 2020a) founded on large-scale corpora boosted abstractive summarization signifcantly, and sequence-to-sequence (Seq2Seq) learning has shown promising results in almost all scenarios. It commonly learns an autoregressive PLM with maximum likelihood estimation (MLE), and the teacher-forcing algorithm (Goyal et al., 2016) is together used to ensure training effciency and stability. However, such a model predicts each token in a summary based on the gold pre-context during training but on its preceding outputs at inference, causing a training-inference discrepancy - exposure bias (Bengio et al., 2015; Goodman et al., 2020), which heavily limits summarization performance.\\nSince exposure bias happens on the token level, existing solutions train models to maximize the global similarity between candidate and reference summaries, namely global learning (GL). Reinforcement and contrastive learning in summarization are the most used GL technologies. For example, in reinforcement learning based GOLD (Pang and He, 2021) and RLEF (Roit et al., 2023), a summarization model is rewarded depending on the quality of candidate summaries it produces, with the reference as the standard. Similarly, contrastive learning methods (Liu et al., 2022; Xie et al., 2023; Zhang et al., 2022) compare candidate summaries with the reference and assign the one closer to the reference a higher probability, and vice versa. On the one hand, all these methods measure candidate-reference similarities without considering the source document conditions. Furthermore, their measurement stands on only the semantic or lexical aspect rather than comprehensive perspectives, resulting in biased learning objectives. SeqCo (Xu et al., 2022) aims to minimize the semantic discrepancies among a source document, its reference, and candidate summaries. Still, it is powerless to learn lexical perception and shows undesirable summarization results.\\n\\nTo address these problems, we highlight rational relations within the triplet (document, candidate summary, reference summary) in abstractive summarization. Take the geometrical triplet in Figure 1 for intuitive perception. Traditional GL methods view the edge $R C$ as lexical or semantic gaps between candidate and reference summaries, aiming to draw points $R$ and close as possible. Similarly, SeqCo considers from the semantics perspective and further aims to condense the triangle $\\\\triangle S R C$ into a single point (draw all three points $S,R$ , and $C$ as close as possible). In this work, we base on the traditional GL assumption and further treat side edges $S R$ and $S C$ as relative semantics metrics between the document and summaries. We propose to balance the edges $S R$ and $S C$ while minimizing the edge $R C$ . Our inspiration is that an ideal model should generate candidate summaries similar to the reference on both semantic and lexical aspects (Sul and Choi, 2023). In particular, the limitations of GL-based methods, which tend to yield summaries with unsatisfactory relative semantics, such as faithfulness and abstractiveness (Dixit et al., 2023) measured by the source document, can be effectively fxed by balancing the two side edges of $\\\\triangle S R C$ .\\nAccording to the above insights, we propose an iterative autoregressive summarization paradigm (IARSum), which facilitates learning the mentioned triplet relations with a standard GL framework to enhance summarization performance. Specifcally, IARSum generates a summary through a series of iterations, during which the model re-inputs and reprocesses the previously generated summary in each iteration to get improved versions. This encourages assessing summaries\\u2019 quality from a global view and effectively prevents exposure bias. We build IARSum on a double encoder-decoder network following Transformer architecture to fulfll the desired properties. It uses two serial encoders to encode the document and reinput summaries, respectively, and uses the second encoder\\u2019s outputs to model summary-document semantics. To learn the IARSum model aware of triplet relations, we reward the model to get similar outputs from the second encoder when provided with candidate and reference summaries as input, respectively. We also reward the model once a candidate achieves higher lexical overlap with the reference after reprocessing. Furthermore, we adopt an offine mini-risk training strategy that enforces the model to maximize the mentioned rewards. In inference, a trained IARSum model can adaptively refne the generated summaries in sequential iterations for increased quality.\\n\\nIn summary, we make three-fold contributions. First, we explore rational relations within the triplet (source document, reference summary, candidate summary) in summarization and propose to balance the relative semantics over tuples (candidate, document) and (reference, document) while reducing the lexical differences within (candidate, reference). Second, we propose IARSum, a novel summarization paradigm that facilitates learning our suggested triplet relations with a GL framework to boost summaries\\u2019 quality. Finally, we conduct extensive experiments on two public datasets to test our methods. Results show that IARSum matches or outperforms previous state-of-the-art (SOTA) approaches in generating high-quality summaries measured by multiple metrics. Furthermore, we transfer IARSum to few-shot settings and show its superior robustness.\"}\n{\"section_title\":\"2 Related Work and Background\",\"section_content\":\"\"}\n{\"section_title\":\"2.1 Abstractive Summarization\",\"section_content\":\"Summarization is always modeled as a Seq2Seq generation task, creating function $f$ that is conditioned on a source document $X$ to output a target summary $Y$ :\\nFor the abstractive paradigm, existing approaches commonly learn an autoregressive language model with parameter $\\\\theta$ to ft $f$ and approximate the conditional probability $\\\\mathrm{P}(Y|X)$ token by token. Maximum likelihood estimation (MLE) is the most used learning schema. It aims to maximize the probability that the model predicts gold reference, following independent and identically distributed conditions, i.e., max\\u03b8 $\\\\mathrm{~\\\\cal~P~}_{\\\\theta}(Y|X)\\\\;\\\\;=\\\\;\\\\;$ $\\\\begin{array}{r}{\\\\operatorname*{max}_{\\\\theta}\\\\prod_{t=1}^{l}\\\\operatorname{P}_{\\\\theta}(y_{t}|Y_{<t},X)}\\\\end{array}$ , where $l$ denotes the length of reference and $Y_{<t}$ refers to sub-sequence $\\\\{y_{1},y_{2},\\\\cdot\\\\cdot\\\\cdot,y_{t-1}\\\\}$ .\\n\\nDuring training, the teacher-forcing mechanism (Goyal et al., 2016) is adopted, which conditions on exact pre-context to predict a target token and minimizes the following negative log-likelihood (NLL) loss:\\nThough this encourages stable MLE learning, such a trained model depends heavily on accurate prediction. Intuitively, it learns to sample the next token at timestep $t$ from the distribution $\\\\operatorname{P}(\\\\cdot|Y_{<t},X)$ , while the case at inference is to sample from $\\\\mathrm{P}(\\\\cdot|Y_{<t}^{\\\\prime},X)$ , where $Y_{<t}^{\\\\prime}$ denotes the previous generation. This gap between training and inference is the so-called exposure bias, causing errors accumulation during inference, especially once any improper token is generated in early steps.\"}\n{\"section_title\":\"2.2 Global Learning\",\"section_content\":\"Reinforcement learning (RL) rewards a model with sequence-level feedback, depending on varying evaluation metrics. Most works (Tan, 2023; Roit et al., 2023) are based on on-policy learning (Paulus et al., 2018), where a model generates a sampled candidate and a greedily searched candidate during training. It requires high computational costs and tends to get stuck in a zero-reward region. As a result, MLE loss is used as an assistant. Richard et al. (Pang and He, 2021) proposed an off-policy learning method that uses reference summary as a demonstrator. Although it averts zero rewards, the exploring ability is reduced.\\nTraditional contrastive learning (CTL) uses positive and negative sample pairs to train a model to distinguish real data labels. For example, CLIFF (Cao and Wang, 2021) builds sample pairs by the back-translation and improves the faithfulness and factuality of the generated summaries. In recent years, ranking-based learning originated from the standard CTL and has shown advanced performance in abstractive summarization. Liu et al. (Liu and Liu, 2021) frst propose a two-stage framework that trains a RoBERTa (Liu et al., 2019) to rank the candidates generated by BART at frst. BRIO (Liu et al., 2022) makes a further optimization, trains BART itself as an evaluation tool, and ranks the conditional probability of candidates. Later, a lot of improved BRIO variants (Xie et al., 2023; Zhao et al., 2023; Zhang et al., 2022) were proposed in succession. Despite performing surprisingly, such methods only focus on maximizing the candidate-reference similarity without considering the source document effects. Noting this point, SeqCo (Xu et al., 2022) contrasts semantics among source documents, candidates, and references. However, SeqCo assumed irrational triplet relations and suffered unstable optimization caused by online learning, the main reason for undesirable performance.\"}\n{\"section_title\":\"3 Method\",\"section_content\":\"In this section, we describe the details of our proposed methods. We introduce the iterative autoregressive text generation paradigm in Section 3.1, describe the IARSum model architecture in Section 3.2, and illustrate the offine global learning strategy in Section 3.3.\"}\n{\"section_title\":\"3.1 Iterative Autoregressive Generation\",\"section_content\":\"The standard autoregressive (AR) text generation illustrated in Figure 2 (a) is widely known as a unidirectional process, where a text is generated sequentially token by token. The major limitation is that each token is predicted depending on its precontext. As a result, a wrongly generated token may mislead the later content and make the generated text entirely deviate from the target due to error accumulation. Calibrating the predicted token distributions on a global view (global learning) is effective in addressing this problem. However, it is hard to involve the source document conditions, i.e., the semantic relations between the document and summaries, in calibration. We propose an iterative autoregressive generation paradigm (IAR) to break these limitations.\\nAs demonstrated in Figure 2, IAR models the generation of target text in the Seq2Seq task as a text-level Markov Chain, where the state transitions from a draft to more refned results. Taking abstractive summarization as an example, at the $i$ -th iteration, IARSum samples a candidate summary $Y^{i-1}$ from the previous iteration\\u2019s outputs, estimates its quality, and produces a new one $Y^{i}$ . This process repeats until the maximum number of iterations $N$ is reached, which formally presents as:\\n\\nwhere $Y^{0}\\\\;\\\\sim\\\\;\\\\mathrm{P}(\\\\cdot|X),\\\\;Y^{i}\\\\;\\\\sim\\\\;\\\\mathrm{P}(\\\\cdot|X,Y^{i-1})$ , and $Y^{N}=Y$ .\"}\n{\"section_title\":\"3.2 Model Architecture\",\"section_content\":\"We implement IARSum with a Transformer-based encoder-decoder model shown in Figure 3. It has two encoders with bidirectional attention and one decoder with unidirectional attention. To speed up convergence, we start our model with a singleencoder Transformer with pre-trained parameters $\\\\theta$ and share initial parameters between the two encoders.\\nThe architecture of IARSum is very similar to GSum (Dou et al., 2021). However, encoders of GSum are independent and connect to the decoder orderly by cross-attention layers (i.e., parallel encoders). IARSum instead adapts serial encoders, where the second encoder relies on the output of the frst to feature the input content, similar to a Transformer decoder without a sequence mask. Besides, GSum uses the second encoder to encode guidance words, while IARSum\\u2019s second encoder is used to encode the candidate summary. Mathematically, IARSum models the following token distributions during the frst and later iterations, respectively:\\n\\nwhere $E_{\\\\theta}^{1},E_{\\\\theta}^{2}$ are the frst and second encoders, and $D_{\\\\theta}$ is the decoder. $\\\\sigma(\\\\cdot)$ is softmax function.\"}\n{\"section_title\":\"3.3 Learning Objective\",\"section_content\":\"According to our intention proposing IAR in Section 3.1, we learn an IARSum model for mainly two objectives. One is to maximize the lexical similarity between candidate and reference summaries, and the other one involves matching the relative semantics of candidates with that of the reference, taking the document as the standard. Both objectives can be attended within a multi-rewards learning framework.\\nSemantics Rewards. The recent study (Dreyer et al., 2023) pointed out that a summary should be logically entailed in the source document to ensure faithfulness. On the other hand, researchers also observed that humans write summaries with hallucinatory words to keep abstractiveness (Maynez et al., 2020) despite contradicting summary-document entailment. Note that faithfulness and abstractiveness are perceived on the source document basis. To bypass their contradictions, we uniformly refer to such perceptions as relative semantics and model them with a neural function $S(\\\\cdot,\\\\cdot)$ . Intuitively, an ideal candidate summary should be at a similar level of relative semantics compared with the reference, and their differences in this attribute are quantitatively observed:\\n\\nwhere $\\\\langle\\\\cdot,\\\\cdot\\\\rangle$ is a distance function, such as Euclidean distance. Furthermore, we will reward the model according to the gain of semantics rewards between two adjacent iterations:\\nLexical Rewards. From the lexical view, we encourage a candidate, after reprocessing, to contain more tokens overlapped with the reference. To this end, we reward the model using the lexical rewards:\\nwhere $\\\\{\\\\cdot\\\\}$ denotes a token set and $\\\\big|\\\\cdot\\\\big|$ means the set size.\\nLearning Objective. Finally, we mix the two types of rewards with a balance coeffcient $\\\\xi\\\\in(0,1)$ :\\nand the overall learning objective for IARSum is unifed to maximize the following expected rewards:\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: From the sections retrieved, I can see that the framework proposed in the paper is IARSum. The framework it is similar to is SeqCo, as mentioned in the introduction. Next, I will retrieve the datasets used in both papers to determine the number of common datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND section_title = '4 Experiments';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The experiments section is empty, so I will use the vectorstore to retrieve the relevant context about the datasets used in the paper. I will use the `text_sentence_transformers_all_minilm_l6_v2` collection to perform the similarity search.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of IARSum', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5568,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.5548,\"pdf_id\":\"39407fcc-f073-5e75-b8d2-d85cf5b672a0\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3543f99f-33eb-5084-98f6-3abc5cb468f8\",\"text\":\"of IARSum with varying \\u03bb shows a similar trend\\non both datasets. It is observed that a too-small\\nweight suppresses global learning efficacy. On the\\ncontrary, once \\u03bb reaches the magnitude above one\\nhundred, varying its value makes inconspicuous ef-\\nfects. We finally set \\u03bb to 100 without distinguishing\\ndatasets.\\n208\"}\n{\"score\":0.5379,\"pdf_id\":\"7806d49c-9169-5309-b688-5c1d81e87497\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"86777b7a-67c1-5ae1-aedc-c1474e198642\",\"text\":\"imum activation in the dataset, similar to the steering approach used by Templeton et al. (2024). We\\n3\"}\n{\"score\":0.5351,\"pdf_id\":\"bf75a7f9-3591-5c2c-81ed-e530cf2bc63e\",\"page_number\":24,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8e79246e-5765-507e-bd2c-22fbab1f9c69\",\"text\":\"and freely available for both non-commercial and commercial use. For experimental data,\\nwe use EMPIAR database which is freely and publicly available to the global community\\nunder the CC0 licence.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\"}\n{\"score\":0.5331,\"pdf_id\":\"61992953-6316-59ce-bf1b-914f513970f2\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cdd46b82-d199-5d86-a4ba-76206e51ff21\",\"text\":\"Links to download or view our dataset can be found on our GitHub homepage at\\nhhttps:\\/\\/github.com\\/autoiac-project\\/iac-eval. The dataset\\u2019s homepage, including all future versions\\nand updates, is hosted at https:\\/\\/huggingface.co\\/datasets\\/autoiac-project\\/iac-eval.\\nDataset Documentation. We offer comprehensive descriptions for the dataset and its usage through\\nthe HugggingFace dataset link above. The page covers the data layout, essential metadata, and ways\\nto load the data for usage.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The top result from the vectorstore indicates that the relevant datasets are summarized in Table 11 of another paper. However, the second result from the paper \"39407fcc-f073-5e75-b8d2-d85cf5b672a0\" mentions that IARSum was tested on two datasets. I will retrieve the datasets used in the SeqCo paper to determine the common datasets.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of SeqCo', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6471,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.6432,\"pdf_id\":\"da238894-aa73-53b4-a4f0-66c21cb8b6db\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f77761ac-35fb-557d-812c-2e8d42d625c3\",\"text\":\"Appendix\\nWe first introduce the details of the datasets and implementation in Sec. A, followed by the supplementary\\nexperiments and analysis in Sec. B. Furthermore, the experimental results are detailed in Sec. C. Lastly,\\nadditional qualitative results are provided in Sec. D.\\nA\\nDetails of Datasets and Implementation\\nA.1\\nPublic datasets details\\nHere, we provide detailed information on the five public datasets used for experiments in Table 11.\"}\n{\"score\":0.6233,\"pdf_id\":\"7ed696bd-4365-5202-924d-f883b0ce73f3\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"5386fb7f-1370-554b-b6bb-d2cb957a6e16\",\"text\":\"We briefly introduce the datasets and baselines in experiments. More details are in Appendix E.\\n7\"}\n{\"score\":0.6081,\"pdf_id\":\"239cad70-2ca8-5b1e-8df1-91a657390dfc\",\"page_number\":32,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"c0da28bf-9f05-5eab-b3d2-f751d9918cfb\",\"text\":\"Justification: We provide the details about the dataset, evaluation protocol, and implementa-\\ntions in the experiments section 5 and the appendix.\\nGuidelines:\\n32\"}\n{\"score\":0.6058,\"pdf_id\":\"a4be6f7b-28f7-550f-9a29-0e342afae074\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"75010488-5210-5069-8be4-3b0f14355d2c\",\"text\":\"A\\nDataset Details and Availability\\nIn this work, we collect 25 open-source datasets for supervised fine-tuning SegVol and some external\\nopen-source datasets specifically for comparative experiments. The detailed information on anatomi-\\ncal categories and dataset scales of these open-source datasets is shown in Table 4. The availability of\\nthese datasets is demonstrated in Table 5. Additionally, to avoid privacy concerns, we collect 90K\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results indicate that the datasets used in the SeqCo paper are detailed in Table 11 of another paper. However, the second result from the SeqCo paper mentions that it uses five public datasets. I will now retrieve the specific datasets used in both papers to determine the common datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND section_title = '5 Experiments';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, so I will try a different approach to find the datasets used in the IARSum paper. I will use the vectorstore again to retrieve the relevant context.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of IARSum', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5568,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.5548,\"pdf_id\":\"39407fcc-f073-5e75-b8d2-d85cf5b672a0\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3543f99f-33eb-5084-98f6-3abc5cb468f8\",\"text\":\"of IARSum with varying \\u03bb shows a similar trend\\non both datasets. It is observed that a too-small\\nweight suppresses global learning efficacy. On the\\ncontrary, once \\u03bb reaches the magnitude above one\\nhundred, varying its value makes inconspicuous ef-\\nfects. We finally set \\u03bb to 100 without distinguishing\\ndatasets.\\n208\"}\n{\"score\":0.5379,\"pdf_id\":\"7806d49c-9169-5309-b688-5c1d81e87497\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"86777b7a-67c1-5ae1-aedc-c1474e198642\",\"text\":\"imum activation in the dataset, similar to the steering approach used by Templeton et al. (2024). We\\n3\"}\n{\"score\":0.5351,\"pdf_id\":\"bf75a7f9-3591-5c2c-81ed-e530cf2bc63e\",\"page_number\":24,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8e79246e-5765-507e-bd2c-22fbab1f9c69\",\"text\":\"and freely available for both non-commercial and commercial use. For experimental data,\\nwe use EMPIAR database which is freely and publicly available to the global community\\nunder the CC0 licence.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\"}\n{\"score\":0.5331,\"pdf_id\":\"61992953-6316-59ce-bf1b-914f513970f2\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cdd46b82-d199-5d86-a4ba-76206e51ff21\",\"text\":\"Links to download or view our dataset can be found on our GitHub homepage at\\nhhttps:\\/\\/github.com\\/autoiac-project\\/iac-eval. The dataset\\u2019s homepage, including all future versions\\nand updates, is hosted at https:\\/\\/huggingface.co\\/datasets\\/autoiac-project\\/iac-eval.\\nDataset Documentation. We offer comprehensive descriptions for the dataset and its usage through\\nthe HugggingFace dataset link above. The page covers the data layout, essential metadata, and ways\\nto load the data for usage.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results indicate that the IARSum paper uses two datasets, but the specific names are not mentioned. I will now retrieve the specific datasets used in the SeqCo paper to determine the common datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id IN (SELECT paper_id FROM metadata WHERE title LIKE '%SeqCo%') AND section_title = '5 Experiments';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, so I will use the vectorstore again to retrieve the specific datasets used in the SeqCo paper.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of SeqCo', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6471,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.6432,\"pdf_id\":\"da238894-aa73-53b4-a4f0-66c21cb8b6db\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f77761ac-35fb-557d-812c-2e8d42d625c3\",\"text\":\"Appendix\\nWe first introduce the details of the datasets and implementation in Sec. A, followed by the supplementary\\nexperiments and analysis in Sec. B. Furthermore, the experimental results are detailed in Sec. C. Lastly,\\nadditional qualitative results are provided in Sec. D.\\nA\\nDetails of Datasets and Implementation\\nA.1\\nPublic datasets details\\nHere, we provide detailed information on the five public datasets used for experiments in Table 11.\"}\n{\"score\":0.6233,\"pdf_id\":\"7ed696bd-4365-5202-924d-f883b0ce73f3\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"5386fb7f-1370-554b-b6bb-d2cb957a6e16\",\"text\":\"We briefly introduce the datasets and baselines in experiments. More details are in Appendix E.\\n7\"}\n{\"score\":0.6081,\"pdf_id\":\"239cad70-2ca8-5b1e-8df1-91a657390dfc\",\"page_number\":32,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"c0da28bf-9f05-5eab-b3d2-f751d9918cfb\",\"text\":\"Justification: We provide the details about the dataset, evaluation protocol, and implementa-\\ntions in the experiments section 5 and the appendix.\\nGuidelines:\\n32\"}\n{\"score\":0.6058,\"pdf_id\":\"a4be6f7b-28f7-550f-9a29-0e342afae074\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"75010488-5210-5069-8be4-3b0f14355d2c\",\"text\":\"A\\nDataset Details and Availability\\nIn this work, we collect 25 open-source datasets for supervised fine-tuning SegVol and some external\\nopen-source datasets specifically for comparative experiments. The detailed information on anatomi-\\ncal categories and dataset scales of these open-source datasets is shown in Table 4. The availability of\\nthese datasets is demonstrated in Table 5. Additionally, to avoid privacy concerns, we collect 90K\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results indicate that the SeqCo paper uses five public datasets, but the specific names are not mentioned. I will now retrieve the specific datasets used in the IARSum paper to determine the common datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND section_title = '4 Experiments';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any useful information, so I will try another approach. I will use the vectorstore to retrieve the specific datasets used in the IARSum paper.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of IARSum', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5568,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.5548,\"pdf_id\":\"39407fcc-f073-5e75-b8d2-d85cf5b672a0\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3543f99f-33eb-5084-98f6-3abc5cb468f8\",\"text\":\"of IARSum with varying \\u03bb shows a similar trend\\non both datasets. It is observed that a too-small\\nweight suppresses global learning efficacy. On the\\ncontrary, once \\u03bb reaches the magnitude above one\\nhundred, varying its value makes inconspicuous ef-\\nfects. We finally set \\u03bb to 100 without distinguishing\\ndatasets.\\n208\"}\n{\"score\":0.5379,\"pdf_id\":\"7806d49c-9169-5309-b688-5c1d81e87497\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"86777b7a-67c1-5ae1-aedc-c1474e198642\",\"text\":\"imum activation in the dataset, similar to the steering approach used by Templeton et al. (2024). We\\n3\"}\n{\"score\":0.5351,\"pdf_id\":\"bf75a7f9-3591-5c2c-81ed-e530cf2bc63e\",\"page_number\":24,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8e79246e-5765-507e-bd2c-22fbab1f9c69\",\"text\":\"and freely available for both non-commercial and commercial use. For experimental data,\\nwe use EMPIAR database which is freely and publicly available to the global community\\nunder the CC0 licence.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\"}\n{\"score\":0.5331,\"pdf_id\":\"61992953-6316-59ce-bf1b-914f513970f2\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cdd46b82-d199-5d86-a4ba-76206e51ff21\",\"text\":\"Links to download or view our dataset can be found on our GitHub homepage at\\nhhttps:\\/\\/github.com\\/autoiac-project\\/iac-eval. The dataset\\u2019s homepage, including all future versions\\nand updates, is hosted at https:\\/\\/huggingface.co\\/datasets\\/autoiac-project\\/iac-eval.\\nDataset Documentation. We offer comprehensive descriptions for the dataset and its usage through\\nthe HugggingFace dataset link above. The page covers the data layout, essential metadata, and ways\\nto load the data for usage.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved results indicate that the IARSum paper uses multiple datasets, but the specific names are not mentioned clearly. I will try to retrieve the specific datasets used in the IARSum paper from the sections table again.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND section_title = '5 Experiments';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, so I will use the vectorstore again to retrieve the specific datasets used in the IARSum paper.\n[Action]:\nClassicRetrieve(query='datasets used in the experiments of IARSum', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5568,\"pdf_id\":\"b3629afc-2785-5da3-b595-291d0f378813\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"87f2178c-7030-5284-b250-70f11ba16856\",\"text\":\"Table 11 summarizes the various datasets used in our experiments, and their characteristics.\\n18\"}\n{\"score\":0.5548,\"pdf_id\":\"39407fcc-f073-5e75-b8d2-d85cf5b672a0\",\"page_number\":11,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3543f99f-33eb-5084-98f6-3abc5cb468f8\",\"text\":\"of IARSum with varying \\u03bb shows a similar trend\\non both datasets. It is observed that a too-small\\nweight suppresses global learning efficacy. On the\\ncontrary, once \\u03bb reaches the magnitude above one\\nhundred, varying its value makes inconspicuous ef-\\nfects. We finally set \\u03bb to 100 without distinguishing\\ndatasets.\\n208\"}\n{\"score\":0.5379,\"pdf_id\":\"7806d49c-9169-5309-b688-5c1d81e87497\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"86777b7a-67c1-5ae1-aedc-c1474e198642\",\"text\":\"imum activation in the dataset, similar to the steering approach used by Templeton et al. (2024). We\\n3\"}\n{\"score\":0.5351,\"pdf_id\":\"bf75a7f9-3591-5c2c-81ed-e530cf2bc63e\",\"page_number\":24,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8e79246e-5765-507e-bd2c-22fbab1f9c69\",\"text\":\"and freely available for both non-commercial and commercial use. For experimental data,\\nwe use EMPIAR database which is freely and publicly available to the global community\\nunder the CC0 licence.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\"}\n{\"score\":0.5331,\"pdf_id\":\"61992953-6316-59ce-bf1b-914f513970f2\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"cdd46b82-d199-5d86-a4ba-76206e51ff21\",\"text\":\"Links to download or view our dataset can be found on our GitHub homepage at\\nhhttps:\\/\\/github.com\\/autoiac-project\\/iac-eval. The dataset\\u2019s homepage, including all future versions\\nand updates, is hosted at https:\\/\\/huggingface.co\\/datasets\\/autoiac-project\\/iac-eval.\\nDataset Documentation. We offer comprehensive descriptions for the dataset and its usage through\\nthe HugggingFace dataset link above. The page covers the data layout, essential metadata, and ways\\nto load the data for usage.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The results are still not providing the specific datasets used in the IARSum paper. I will try to retrieve the datasets from the reference section, as they might be mentioned there.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT reference_content FROM reference WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0';\")"}
{"role": "user", "content": "[Observation]:\n{\"reference_content\":\"Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS 2015, pages 1171\\u20131179.\"}\n{\"reference_content\":\"Shuyang Cao and Lu Wang. 2021. CLIFF: contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, pages 6633\\u2013 6649.\"}\n{\"reference_content\":\"Tanay Dixit, Fei Wang, and Muhao Chen. 2023. Improving factuality of abstractive summarization without sacrifcing summary quality. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, pages 902\\u2013913.\"}\n{\"reference_content\":\"Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2021. Gsum: A general framework for guided neural abstractive summarization. In NAACL-HLT 2021, pages 4830\\u20134842.\"}\n{\"reference_content\":\"Markus Dreyer, Mengwen Liu, Feng Nan, Sandeep Atluri, and Sujith Ravi. 2023. Evaluating the tradeoff between abstractiveness and factuality in abstractive summarization. In Findings of the Association for Computational Linguistics: EACL 2023, pages 2044\\u2013 2060.\"}\n{\"reference_content\":\"Sebastian Goodman, Nan Ding, and Radu Soricut. 2020. Teaforn: Teacher-forcing with n-grams. In EMNLP 2020, pages 8704\\u20138717.\"}\n{\"reference_content\":\"Anirudh Goyal, Alex Lamb, Ying Zhang, Saizheng Zhang, Aaron C. Courville, and Yoshua Bengio. 2016. Professor forcing: A new algorithm for training recurrent networks. In NIPS 2016, pages 4601\\u20134609.\"}\n{\"reference_content\":\"Karl Moritz Hermann, Tom\\u00e1s Kocisk\\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NIPS 2015, pages 1693\\u20131701.\"}\n{\"reference_content\":\"G. Senthil Kumar and Midhun Chakkaravarthy. 2023. A survey on recent text summarization techniques. In MIWAI 2023, volume 14078 of Lecture Notes in Computer Science, pages 496\\u2013502.\"}\n{\"reference_content\":\"Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707\\u2013710. Soviet Union.\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The reference section does not provide the specific datasets used in the IARSum paper. I will try to retrieve the datasets from the sections table again, but this time I will search for the datasets in the experiments section more broadly.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND section_title LIKE '%Experiments%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The experiments section is still not providing the specific datasets used in the IARSum paper. I will try to retrieve the datasets from the tables table, as they might be mentioned in the tables.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT table_content FROM tables WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0';\")"}
{"role": "user", "content": "[Observation]:\n{\"table_content\":\"<html><body><table><tr><td>Model<\\/td><td>R-1<\\/td><td>R-2<\\/td><td>R-L<\\/td><td>BS<\\/td><td>BaS<\\/td><\\/tr><tr><td>BART<\\/td><td>44.16<\\/td><td>21.28<\\/td><td>40.90<\\/td><td>87.95<\\/td><td>-3.91<\\/td><\\/tr><tr><td>PEGASUS<\\/td><td>44.17<\\/td><td>21.47<\\/td><td>41.11<\\/td><td>85.07t<\\/td><td>-3.80t<\\/td><\\/tr><tr><td>GSum<\\/td><td>45.94<\\/td><td>22.32<\\/td><td>42.48<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>GOLD<\\/td><td>45.40<\\/td><td>22.01<\\/td><td>42.25<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>SeqCo<\\/td><td>45.02<\\/td><td>21.80<\\/td><td>41.75<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>BRIO<\\/td><td>47.78<\\/td><td>23.55<\\/td><td>44.57<\\/td><td>89.14t<\\/td><td>-3.62<\\/td><\\/tr><tr><td>SimMCS<\\/td><td>48.16<\\/td><td>24.08<\\/td><td>44.65<\\/td><td>89.20<\\/td><td>-3.58<\\/td><\\/tr><tr><td>SLiC<\\/td><td>47.97<\\/td><td>24.18<\\/td><td>44.88<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>MoCa<\\/td><td>48.88<\\/td><td>24.94<\\/td><td>45.76<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>IARSum<\\/td><td>48.96<\\/td><td>25.14<\\/td><td>45.93<\\/td><td>89.32<\\/td><td>-3.25<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"table_content\":\"<html><body><table><tr><td>Model<\\/td><td>R-1<\\/td><td>R-2<\\/td><td>R-L<\\/td><td>BS<\\/td><td>BaS<\\/td><\\/tr><tr><td>BART<\\/td><td>45.14<\\/td><td>22.27<\\/td><td>37.25<\\/td><td>89.63t<\\/td><td>-3.64<\\/td><\\/tr><tr><td>PEGASUS<\\/td><td>47.21<\\/td><td>24.56<\\/td><td>39.25<\\/td><td>89.68<\\/td><td>-3.89<\\/td><\\/tr><tr><td>GSum<\\/td><td>45.40<\\/td><td>21.89<\\/td><td>36.67<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>GOLD<\\/td><td>45.85<\\/td><td>22.58<\\/td><td>37.65<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>SeqCo<\\/td><td>45.65<\\/td><td>22.41<\\/td><td>37.04<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>BRIO<\\/td><td>49.07<\\/td><td>25.59<\\/td><td>40.40<\\/td><td>89.10t<\\/td><td>-3.79<\\/td><\\/tr><tr><td>SimMCS<\\/td><td>49.39<\\/td><td>25.73<\\/td><td>40.49<\\/td><td>90.23<\\/td><td>-3.77<\\/td><\\/tr><tr><td>SLiC<\\/td><td>49.77<\\/td><td>27.09<\\/td><td>42.08<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>MoCa<\\/td><td>49.32<\\/td><td>25.91<\\/td><td>41.47<\\/td><td><\\/td><td><\\/td><\\/tr><tr><td>IARSum<\\/td><td>49.42<\\/td><td>27.20<\\/td><td>42.50<\\/td><td>92.13<\\/td><td>-3.61<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"table_content\":\"<html><body><table><tr><td>Model<\\/td><td>Iteration<\\/td><td>R-1<\\/td><td>R-2<\\/td><td>R-L<\\/td><td>Dist.<\\/td><\\/tr><tr><td rowspan=\\\"5\\\">IARSum<\\/td><td>i=1<\\/td><td>46.19<\\/td><td>22.27<\\/td><td>43.69<\\/td><td>0.60<\\/td><\\/tr><tr><td>i=2<\\/td><td>47.69<\\/td><td>23.92<\\/td><td>44.68<\\/td><td>0.03<\\/td><\\/tr><tr><td>i=3<\\/td><td>48.96<\\/td><td>25.14<\\/td><td>45.93<\\/td><td>0.01<\\/td><\\/tr><tr><td>i=4<\\/td><td>48.71<\\/td><td>24.12<\\/td><td>44.89<\\/td><td>0.01<\\/td><\\/tr><tr><td>i=5<\\/td><td>48.74<\\/td><td>24.12<\\/td><td>44.91<\\/td><td>0.01<\\/td><\\/tr><tr><td rowspan=\\\"5\\\">-w\\/o Ler<\\/td><td>i=1<\\/td><td>44.16<\\/td><td>21.28<\\/td><td>40.90<\\/td><td>0.62<\\/td><\\/tr><tr><td>i=2<\\/td><td>45.28<\\/td><td>22.63<\\/td><td>40.96<\\/td><td>0.59<\\/td><\\/tr><tr><td>i=3<\\/td><td>44.68<\\/td><td>21.32<\\/td><td>40.57<\\/td><td>0.58<\\/td><\\/tr><tr><td>i=4<\\/td><td>44.30<\\/td><td>22.38<\\/td><td>41.34<\\/td><td>0.59<\\/td><\\/tr><tr><td>i=5<\\/td><td>44.78<\\/td><td>21.06<\\/td><td>40.00<\\/td><td>0.59<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"table_content\":\"<html><body><table><tr><td>System<\\/td><td>Summary<\\/td><\\/tr><tr><td colspan=\\\"2\\\">Case1<\\/td><\\/tr><tr><td>Reference<\\/td><td>RBS has racked up f50billion in losses since it was bailed out by taxpayers. State-back giant warned that it faces'another tough year\\u2019 to come.Lurched toa f446millionlossfor thefirst three months of theyear.<\\/td><\\/tr><tr><td>BART<\\/td><td>state-backed giant lurched to a f446million loss for the first three months of the year. Comes after RBS was forced to put aside f856million to cover lawsuits and fines still being decided.It includes an extra f334million for its part in rigging the foreign exchange marketwithotherbanks.<\\/td><\\/tr><tr><td>IARSum=1<\\/td><td>RBS has racked up f50billion in losses since it was bailed out by taxpayers. The state-backed giant lurched to a f446million loss for the first three months of the year.It warned that it faces another'tough year'in a statement last night.The bank was forced to put aside f856milliontocoverlawsuits andfines.<\\/td><\\/tr><tr><td>IARSum=2<\\/td><td>Since receiving a bailout from taxpayers,RBS has racked up f50 billion in losses. The state-backed giant has issued a warning, stating that it anticipates facing another tough year. It reported a loss of f446 million for the first quarter of this year.<\\/td><\\/tr><tr><td colspan=\\\"2\\\">Case 2<\\/td><\\/tr><tr><td>Reference<\\/td><td>Hungarian national Evelin Mezei, 12, has been found safe and well. She had gone missing from the Stratford area in London last night. Evelinhadbeenseen onCCTVfootagewith an unknownman.<\\/td><\\/tr><tr><td>BART<\\/td><td>Evelin Mezei, a 12-year-old Hungarian national, was spotted with the man at around 10.30pm yesterday. She was last seen by her mother in East London,Scotland Yard said. But the youngster, who came to the UK six months ago,was traced this morning.<\\/td><\\/tr><tr><td>IARSum=1<\\/td><td>Evelin Mezei, 12, went missing in Stratford, London, last night. She was seen on CCTV footage with an unknown man on a city street. The Hungarian national was found safe and well this morning. Her mother was last seen with the man's mother.<\\/td><\\/tr><tr><td>IARSum=2<\\/td><td>Evelin Mezei, a 12-year-old Hungarian girl who went missing from the Stratford area in London last night,has been found safe and well. CCTVfootageshowed Evelinwith an unknown manbeforeherdisappearance.<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n\nIn total, 4 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The tables do not provide the specific datasets used in the IARSum paper. I will try to retrieve the datasets from the chunks table again, but this time I will search for the datasets more broadly.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '39407fcc-f073-5e75-b8d2-d85cf5b672a0' AND text_content LIKE '%datasets%';\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"sis, it learns to 1) model the relative semantics\\ndefined over tuples (candidate, document) and\\n(reference, document) respectively and balance\\nthem; 2) reduce lexical differences between\\ncandidate and reference summaries. Further-\\nmore, IARSum iteratively reprocesses a gen-\\nerated candidate at inference time to ground\\nhigher quality. We conduct extensive exper-\\niments on two widely used datasets to test\\nour method, and IARSum shows the new or\\nmatched state-of-the-art on diverse metrics.\\n1\\nIntroduction\"}\n{\"text_content\":\"erence). Second, we propose IARSum, a novel\\nsummarization paradigm that facilitates learning\\nour suggested triplet relations with a GL framework\\nto boost summaries\\u2019 quality. Finally, we conduct\\nextensive experiments on two public datasets to test\\nour methods. Results show that IARSum matches\\nor outperforms previous state-of-the-art (SOTA)\\napproaches in generating high-quality summaries\\nmeasured by multiple metrics. Furthermore, we\\ntransfer IARSum to few-shot settings and show its\\nsuperior robustness.\\n2\"}\n{\"text_content\":\"the overall loss function is then:\\nL(\\u03b8) = Lnll(\\u03b8) + \\u03bbLer(\\u03b8)\\n(14)\\n4\\nExperiments\\n4.1\\nDatasets\\nTwo public open-domain datasets are used to eval-\\nuate our method. CNN\\/DM (Hermann et al., 2015;\\nNallapati et al., 2016) is a well-known news sum-\\nmarization dataset with the associated highlights\\nas summaries. XSum (Narayan et al., 2018) is\\nan extremely abstractive dataset also in the news\\ndomain that contains a one-sentence summary for\\neach article from BBC.\\n4.2\\nComparison Methods\"}\n{\"text_content\":\"matic evaluation results in Table 1 and Table 2. 1)\\nIARSum outperforms the backbone models by a\\nlarge margin on both datasets, revealing the superi-\\nority of our learning scheme over the traditional su-\\npervised fine-tuning after pre-training. 2) IARSum\\nalso shows superiorities over the similar double-\\nencoder Transformer - GSum. On the one hand,\\nGSum needs an additional system to predict guid-\\nance signals. Besides, it suffers a severe training-\\ninference discrepancy beyond exposure bias as the\"}\n{\"text_content\":\"quality of guidance in training differs from in in-\\nference. In contrast, our IARSum requires no ad-\\nditional systems, and the model behaves identi-\\ncally during both training and inference. 3) Taking\\nROUGE as the measurement, IARSum achieves\\nnew SOTA on CNN\\/DM and matches the currently\\nbest performance on XSum. Moreover, IARSum\\ndemonstrates the best BERTScore and BARTScore\\non both datasets. We note that BRIO, SLiC, and\\nour IARSum employ a similar training schema,\"}\n{\"text_content\":\"40\\n60\\n80\\n100\\nPercentage (%)\\nXSum\\nmodel\\nPEGASUS\\nOurs\\nRefer\\nFigure 4: Novel n-grams on CNN\\/DM (left) and XSum\\n(right) datasets.\\n5\\nMore Analyses\\nAbstractiveness. In our learning framework, we\\nmeasure the increment of novel words using lexis\\nrewards Rl. The case study approves the effective-\\nness of this strategy from a textual aspect. Here, we\\nfurther understand the abstractiveness of IARSum-\\ngenerated summaries through a quantitative analy-\\nsis. According to previous works (Xie et al., 2023)\"}\n{\"text_content\":\"to approximate the expected distribution and, in\\nturn, better summarization performance. To vali-\\ndate this assumption, we train our model on both\\ndatasets and use different beam widths of 4, 8, 16,\\n32, and 64 to sample candidates. Figure 6 displays\\nthe ROUGE-Avg score of each resulting version.\\n207\"}\n{\"text_content\":\"of IARSum with varying \\u03bb shows a similar trend\\non both datasets. It is observed that a too-small\\nweight suppresses global learning efficacy. On the\\ncontrary, once \\u03bb reaches the magnitude above one\\nhundred, varying its value makes inconspicuous ef-\\nfects. We finally set \\u03bb to 100 without distinguishing\\ndatasets.\\n208\"}\n\nIn total, 8 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The chunks table provides the specific datasets used in the IARSum paper. The datasets mentioned are CNN/DM and XSum. I will now retrieve the datasets used in the GSum paper to determine the common datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = (SELECT paper_id FROM metadata WHERE title = 'GSum: A General Framework for Guided Neural Abstractive Summarization') AND text_content LIKE '%datasets%';\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"tion datasets when using highlighted sentences\\nas guidance.\\nIn addition, we show that our\\nguided model can generate more faithful sum-\\nmaries and demonstrate how different types of\\nguidance generate qualitatively different sum-\\nmaries, lending a degree of controllability to\\nthe learned models.1\\n1\\nIntroduction\\nModern techniques for text summarization gener-\\nally can be categorized as either extractive meth-\\nods (Nallapati et al., 2017; Narayan et al., 2018b;\\nZhou et al., 2018), which identify the most suit-\"}\n{\"text_content\":\"lighted sentences as guidance, can achieve state-\\nof-the-art performance on 4 out of the 6 datasets,\\nincluding 1.28\\/0.79\\/1.13 ROUGE-1\\/2\\/L improve-\\nments over previous state-of-the-art model on the\\nwidely-used CNN\\/DM dataset. In addition, we\\nperform in-depth analyses of different guidance\\nsignals and demonstrate that they are complemen-\\ntary to each other in that there is potential to ag-\\ngregate their outputs together and obtain further\\nimprovements. An analysis of the results also re-\"}\n{\"text_content\":\"We experiment on 6 datasets (statistics in Table 2):\\nReddit\\n(Kim et al., 2019) is a highly abstractive\\ndataset and we use its TIFU-long version.\\n2https:\\/\\/github.com\\/elastic\\/\\nelasticsearch\"}\n{\"text_content\":\"Dataset\\nSource\\n#Pairs\\n#Tokens\\n#Ext\\nTrain\\nValid\\nTest\\nDoc.\\nSum.\\nReddit\\nSocial Media\\n41,675\\n645\\n645\\n482.2\\n28.0\\n2\\nXSum\\nNews\\n203,028\\n11,273\\n11,332\\n430.2\\n23.3\\n2\\nCNN\\/DM\\nNews\\n287,084\\n13,367\\n11,489\\n766.1\\n58.2\\n3\\nWikiHow\\nKnowledge Base\\n168,126\\n6,000\\n6,000\\n580.8\\n62.6\\n4\\nNYT\\nNews\\n44,382\\n5,523\\n6,495\\n1183.2\\n110.8\\n4\\nPubMed\\nScienti\\ufb01c Paper\\n83,233\\n4,946\\n5,025\\n444.0\\n209.5\\n6\\nTable 2: Statistics of the datasets. #Ext denotes the number of sentences we extract for extractive summarization.\\nXSum\"}\n{\"text_content\":\"tive model with encoder initialized with BERT and\\ntrained with a different optimizer than its decoder.\\nMatchSum\\n(Zhong et al., 2020) is an extractive\\nmodel that reranks the candidate summaries pro-\\nduced by BertExt and achieves state-of-the-art ex-\\ntractive results on various summarization datasets.\\nBART\\n(Lewis et al., 2020) is an state-of-the-art\\nabstractive summarization model pretrained with a\\ndenoising autoencoding objective.\\n4.3\\nImplementation Details\\nWe build our models based on both BertAbs and\"}\n{\"text_content\":\"evaluate the best guidance on the other \\ufb01ve datasets\\nusing both BertAbs and BART.\\nPerformance of Different Guidance Signals.\\nAs shown in Table 3, if we feed the model with\\nautomatically constructed signals, feeding either\\nhighlighted sentences or keywords can outperform\\nthe abstractive summarization baseline by a large\\nmargin. Especially, feeding highlighted sentences\\ncan outperform the best baseline by more than 1\"}\n{\"text_content\":\"we use MatchSum to predict the guidance at test\\ntime. From Table 4, we can see that our model can\\nachieve over 1 ROUGE-1\\/L point improvements\\ncompared with the state-of-the-art models, indicat-\\ning the effectiveness of the proposed methods.\\nPerformance on Other Datasets.\\nWe report the\\nperformance of the highlighted sentence model on\\nall the other \\ufb01ve datasets in Table 5. Generally,\\nthe model works better when the dataset is more\\nextractive. For abstractive datasets such as Reddit\"}\n{\"text_content\":\"and XSum, our model cannot achieve performance\\nincreases when the abstractive summarization base-\\n1-grams\\n2-grams\\n3-grams\\n0\\n10\\n20\\n30\\nNovel n-grams %\\n1-grams\\n2-grams\\n3-grams\\n0\\n1\\n2\\nRecall of Novel n-grams %\\nBertAbs\\nSentence\\nKeyword\\nRelation\\nRetrieve\\nFigure 3: Our model can generate more novel words\\nand achieve higher recall of novel words in the gold\\nreference compared with baseline.\\nline is already rather strong. For extractive datasets\\nsuch as PubMed and NYT, on the other hand, our\"}\n{\"text_content\":\"Ours (BertAbs + MatchSum) 26.89 6.75 20.35 38.77 16.14 30.96 38.29 15.10 34.80 37.82 12.32 30.53 50.50 31.57 47.24\\nBART-Based\\nBART\\n35.00 12.89 27.96 45.51 21.94 36.75 41.46 17.80 39.89 44.72 16.48 41.00 54.13 35.15 47.00\\nOurs (BART + MatchSum)\\n34.52 12.71 27.58 45.40 21.89 36.67 41.74 17.73 40.09 45.09 16.72 41.32 54.27 35.37 47.63\\nTable 5: Results of our model guided with highlighted sentences on \\ufb01ve datasets. Highest numbers in each section\"}\n{\"text_content\":\"summarization, using which we investigate four\\ntypes of guidance signals and achieve state-of-the-\\nart performance on various popular datasets. We\\nTrain\\nTest\\nR-1\\nR-2\\nR-L\\nOracle\\nAuto\\n43.78\\n20.66\\n40.66\\nOracle\\n55.18\\n32.54\\n52.06\\nAuto\\nAuto\\n41.61\\n19.04\\n38.65\\nOracle\\n43.07\\n20.79\\n40.13\\nTable 10:\\nUsing automatically constructed guidance\\nduring training degrades the performance signi\\ufb01cantly.\\ndemonstrate the complementarity of the four guid-\\nance signals, and \\ufb01nd that our models can generate\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The datasets used in the GSum paper include CNN/DM, XSum, Reddit, WikiHow, NYT, and PubMed. Both IARSum and GSum use CNN/DM and XSum. Therefore, the number of common datasets is 2. I can now generate the final answer.\n[Action]:\nGenerateAnswer(answer=['IARSum', 'GSum', 2])"}
{"role": "user", "content": "[Observation]: ['IARSum', 'GSum', 2]"}
