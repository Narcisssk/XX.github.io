{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: Using only task-level prompts or using only example-specific prompts, which is better on the Multi-Domain test set?\n[Answer Format]: Your answer should be a single string, either \"task-level\" or \"example-specific\".\n[Anchor PDF]: '5a2b95c1-12d6-5b77-82a1-ee24180d27ae'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: To answer the question, I need to find the relevant content in the specified PDF. I will first retrieve the sections of the paper to see if there is any information about the performance of task-level prompts versus example-specific prompts on the Multi-Domain test set.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '5a2b95c1-12d6-5b77-82a1-ee24180d27ae';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"In-context Examples Selection for Machine Translation\",\"section_content\":\"Sweta Agrawal1,\\u2217 Chunting Zhou2, Mike Lewis2, Luke Zettlemoyer2, Marjan Ghazvininejad2\\n1 University of Maryland 2 Meta AI sweagraw@umd.edu {chuntinz,mikelewis,lsz,ghazvini}@meta.com\"}\n{\"section_title\":\"Abstract\",\"section_content\":\"Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source signifcantly and consistently improves the translation quality of the outputs, outperforming a strong $k\\\\mathsf{N N-M T}$ baseline in 2 out of 4 out-of-domain datasets.\"}\n{\"section_title\":\"1 Introduction\",\"section_content\":\"In-context learning (Brown et al., 2020) has recently received a lot of attention from the NLP research community due to its remarkable ability to utilize only a few input-output examples to perform many NLP tasks (Liu et al., 2021). For example, Lin et al. (2021) demonstrate that a 7.5B multilingual generative model, XGLM, outperforms a supervised sequence-to-sequence baseline in 45 translation directions on the FLORES-101 machine translation benchmark (Goyal et al., 2022) using just 32 randomly sampled translation examples as demonstrations. While these results are compelling, recent work has also shown that the performance and capability of a pre-trained language model (PLM) can be highly sensitive to many factors, such as the choice of in-context examples (Liu et al., 2022b), their ordering (Lu et al., 2022) and the template (Jiang et al., 2020).\\n\\nTypically, in-context learning for MT uses examples that are randomly sampled from a small development set that resembles the domain of the test dataset. The effect of the aforementioned factors (such as the choice of the examples) on the translation quality of the PLM hence remains unclear and unexplored. Yet another crucial gap in using in-context learning for MT in the current literature is the effect of the domain of in-context examples on translation quality since out-of-domain generalization is a known and important challenge in MT (Koehn and Knowles, 2017).\\nIn this work, we systematically analyze how factors such as the choice and the number of few-shot in-context examples and their ordering impact MT output quality. We show that while noisy unrelated 1-shot example can have a signifcantly adverse effect on translation quality, a single prompt optimized to maximize the translation quality on a development set can suffciently elicit task-based information from the PLM. Our analysis thus demonstrates the importance of selecting good examples for MT and raises the question: What are the properties of good in-context examples for MT? In that direction, our fndings suggest that a well-formed meaning-equivalent translation example results in higher quality translation than randomly selected in-context examples.\\nMotivated by the use of Translation Memory in Computer-Aided Translation (Yamada, 2011) and its usage in computational approaches to Machine Translation (Somers, 1999; Koehn and Senellart, 2010; Khandelwal et al., 2020, inter alia), we retrieve similar examples to the test source from a datastore that includes pairs of the source text and their corresponding translations via BM25, an unsupervised effcient retriever to provide additional context to the model. We propose a novel incontext example selection and re-ranking strategy to maximize the coverage of the source n-grams in the retrieved examples. Experiments on WMT\\u201919 English $\\\\leftrightarrow$ German and English $\\\\leftrightarrow$ Russian datasets show that our proposed strategy can consistently improve the translation quality over the outputs generated using BM25 retrieved examples. Combining optimized 1-shot task-level with examplespecifc in-context examples using a simple concatenation strategy further improves translation quality, outperforming state-of-the-art inferenceadapted nearest-neighbor MT models (kNN-MT) on two out-of-domain datasets (Medical and IT) while being memory and compute effcient as our approach does not require constructing and querying a dense token-level datastore.\"}\n{\"section_title\":\"2 Background: In-context Learning\",\"section_content\":\"Generating translations from large-scale multilingual language models like mGPT (Shliazhko et al., 2022), XGLM (Lin et al., 2021) or AlexaTM 20B (Soltan et al., 2022) requires conditioning the decoder-only language model with in-context parallel examples. These examples serve two purposes: a) providing the model with the format and knowledge of the task (task-level) and b) guiding the output generation via providing useful information about the unseen source sentence (example-specifc). This is different from the standard sequence-to-sequence models, where the task is always known, and the model learns generalizable patterns from the input-output examples to perform the task (in this case, translation) for the unseen source text.\\nSource: Welche Risiken sind mit Poulvac FluFend H5N3 RG verbunden?\\nTemplate: {Source text} $=$ {Target text}.\\nFormally, given $k$ in-context examples $\\\\{x_{i},y_{i}\\\\}_{1}^{k}$ the prefx input or the prompt, $\\\\boldsymbol{x}_{j}^{p}$ is generated by concatenating the demonstration examples $\\\\{(x_{i},y_{i})\\\\}_{1}^{k}$ to the test input, $\\\\boldsymbol{x}_{j}^{s}$ according to a template, $P$ (see Table 1). The output, $\\\\hat{y}$ , is then generated via the PLM with parameters $\\\\theta$ via greedy decoding as follows:\"}\n{\"section_title\":\"3 Prompt Selection\",\"section_content\":\"Ideally, good in-context examples can trigger the pre-trained language model to generate the desired output and also elicit the information learned during pre-training (Jiang et al., 2020). Min et al. (2022) show that, for classifcation tasks, the incontext examples provide information about the task (the distribution of the input text, the label space, and the format of the task) and that the model does not rely on these examples to generate the fnal output. However, their analysis is limited to a) classifcation tasks and 2) randomly sampled in-context examples. Prior work has also shown that the order of these in-context examples can also lead to high variance in downstream performance (Zhang et al., 2022). However, less is understood about how these factors impact text generation tasks like MT. Do we need multiple incontext examples? What makes good in-context examples for MT? How sensitive is the model to the order of the prompts?\\nIn this work, we aim to better understand the impact of prompt selection on the translation quality of the outputs. Given a training dataset consisting of $n$ parallel examples $D=\\\\{x_{i},y_{i}\\\\}_{i=1}^{n}$ , and a test source $x_{j}$ , we select a subset of $m$ informative samples to form a prompt which either provides task-level and\\/or example-specifc information as discussed below.\"}\n{\"section_title\":\"3.1 Task-level In-context Examples\",\"section_content\":\"A good task-level in-context example should be able to elicit information learned during pretraining from the PLM. One way to measure the effcacy of an example as a prompt is via computing the translation quality of the outputs generated when prompting the PLM given an example. Hence, we select the task-level prompt as follow: For a given example sampled from the training dataset, $(x_{i},y_{i})\\\\in D^{S}$ , we create a prompt, $\\\\boldsymbol{x}_{i}^{p}$ by concatenating the example $\\\\{(x_{i},y_{i})\\\\}$ to each source in the development set. The system outputs are then generated using equation 1. We then rank examples from $D^{S}$ as task-level prompts based on the BLEU of the generated outputs against the references on this held-out development set, $D^{d e v}=\\\\{X,Y\\\\}$ :\"}\n{\"section_title\":\"3.2 Example-specifc In-context Examples\",\"section_content\":\"Prior work on retrieving good in-context examplespecifc prompts for tasks other than MT (like question answering or knowledge retrieval) either trains a dense-retriever (Rubin et al., 2021) or utilizes samples that are closer to the test source in the embedding space of a PLM like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), or XLNET models (Liu et al., 2022b). While contextual models can generate a global sentence representation, they overlook rare lexicons which can be important for generating translations in unseen domains like medical or IT (Wrzalik and Krechel, 2021).\\nHowever, for MT, overlapping n-grams between the source and the retrieved sentences ensures informativeness as the target associated with the retrieved sentence is likely to include partial translations of the source. We can thus use BM25 as an effcient unsupervised retrieval method to retrieve similar examples. However, as the examples are scored independently and BM25 favors rare word matches (Robertson et al., 2009), the top retrieved candidates might not cover all the terms in the source text (Figure 1). Given that the context window of the PLM is usually limited $\\\\sim3096$ tokens, $16-20$ examples), maximizing the coverage of all the terms found in the test input might be favorable. Hence, we propose to re-rank the top 100 candidates retrieved from BM25 using our algorithm outlined in 1. We extract all the word $\\\\mathbf{n}$ -grams, and their counts from the test source, $\\\\boldsymbol{x}_{j}^{s}$ and source of the BM25 retrieved examples, $\\\\{P_{j}(x_{i})\\\\}_{1}^{k}$ (lines 2-4). Let $S$ and $Q$ denote the set of the source n-grams and the n-grams from a BM25 retrieved example, respectively. We compute a recall-based (R) n-gram overlap score (line 7):\\n\\nThe example with the maximum score is then added to the set of selected prompts, and the found n-grams from the test source are then downweighted by a factor, $\\\\lambda$ , for the next iteration of selection (line 14). For example, setting $\\\\lambda=0$ will select the example that covers the n-grams from the test source in the subsequent iteration that has not already been encountered. This process is then repeated over the retrieved pool until a set threshold of the score is reached.\\nFigure 1 shows the top-100 candidates retrieved via BM25 for the input: \\u201cWelche Risiken sind mit Poulvac FluFend H5N3 RG verbunden?\\u201d. The top few candidates provide the same information to the PLM, i.e., translation of the phrase \\u201cPoulvac FluFend H5N3 RG\\u201d. The examples including the other terms (\\u201cWelche Risiken sind mit verbunden ?\\u201d) from the input text, are ranked lower. On the other hand, our proposed re-ranking strategy can cover all the terms from the input text, in this case, with just the top-2 examples.\"}\n{\"section_title\":\"4 Evaluation Settings\",\"section_content\":\"\"}\n{\"section_title\":\"4.1 Datasets and Evaluation Metric\",\"section_content\":\"We perform our in-domain evaluation on the WMT19 German (de) $\\\\Leftrightarrow$ English (en) and WMT-19 Russian (ru) $\\\\Leftrightarrow$ English (en) datasets (Barrault et al., 2019). For the out-of-domain evaluation, we use the multi-domain dataset from Aharoni and Goldberg (2020) for the following domains: Medical, Law, IT, and Koran. The dataset statistics are reported in the Appendix (Table 8). Following $\\\\mathrm{Ng}$ et al. (2019), we normalize punctuation using Moses (Koehn et al., 2007) and remove sentences longer than 250 tokens and sentence pairs with a source\\/target length ratio exceeding 1.5 from the in-domain datasets. The detokenized length truncated model-generated outputs are evaluated using sacreBLEU (Papineni et al., 2002; Post, 2018).1 The PLM outputs are truncated to twice the source length, as preliminary analysis suggested degeneration in a few $(\\\\sim\\\\!10\\\\!-\\\\!20)$ examples.\"}\n{\"section_title\":\"4.2 Experimental Conditions\",\"section_content\":\"Language Model We use the publicly available checkpoint of the $\\\\mathbf{XGLM_{7.5B}}$ , a decoder-only multilingual language model (Lin et al., 2021) for all our experiments, which has 32 layers and a hidden dimension of 4096.\\n\\nBaselines and Comparisons We consider the following comparisons:\\n\\u2022 Random: $p$ random few-shot examples sampled from the training dataset (number of trials $\\\\mathrel{\\\\mathop:}$ ).\\n\\u2022 Task-level: top- $\\\\cdot p$ examples that achieve the highest BLEU on the development set $(\\\\S\\\\,3.1)$ .\\n\\u2022 Retrieved In-context (BM25): $q_{\\\\mathrm{max}}$ examples retrieved via BM25, since, unlike task-level examples, there is no guarantee that exactly $q$ similar examples will be found in the training dataset for each input.\\n\\u2022 Retrieved Re-ranked In-context (R-BM25): $q_{\\\\mathrm{max}}$ re-ranked examples using our proposed approach as detailed in $\\\\S\\\\ 3.2$ .\\nWe also compare our results with the state-ofthe-art nearest neighbor-based approach for out-ofdomain evaluation, kNN-MT (Khandelwal et al., 2020). We use $\\\\lambda=0.1$ , threshold $\\\\leftrightharpoons\\\\rightmoon$ and order the examples according to their similarity to the source, with the most similar examples on the left in all our experiments (Appendix Tables 9,10).\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The sections retrieved from the database contain information about the performance of different types of prompts. However, the specific comparison between task-level prompts and example-specific prompts on the Multi-Domain test set is not explicitly stated. I will perform a similarity search in the vectorstore to find more detailed information.\n[Action]:\nClassicRetrieve(query='Which is better on the Multi-Domain test set: task-level prompts or example-specific prompts?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6459,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"222f7b8f-8ddf-57f9-9fad-8a193b109c46\",\"text\":\"target domain1) and show the results in Figure 1.\\nInterestingly, these optimized prompts exhibit\\n(i) great performance drop in general (by an\\naverage of 8.49%) on target domain, validating\\nthe existence of research gap mentioned above,\\n(ii) different domain generalization ability in\\nparticular (Acc. drops by 2.61% in best case\\nand by 18.64% in worst case), indicating the\\nexistence of distinct prompt \\u201cnature\\u201d that contributes to its generalizability.\"}\n{\"score\":0.6203,\"pdf_id\":\"0e57b18a-c261-582f-8527-2337f0aeda90\",\"page_number\":16,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"deee226f-fb49-5e4d-b324-7748ce7fde10\",\"text\":\"requiring test cases, we use the two remaining test cases. Overall execution performance is relatively poor, with accuracy\\nnever exceeding 29% for any prompt type. Results indicate that including test cases in the prompt seems to help more\\nthan any other individual component. Including test cases and natural language descriptions in the prompt lead to\\nthe highest overall performance\\u2014higher than using the code itself. Because the code unambiguously describes the\"}\n{\"score\":0.6185,\"pdf_id\":\"fb01ed88-e68e-547c-b44b-f5289090bbf3\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9f0afce9-490d-5d90-99ba-dc5f87ce7403\",\"text\":\"dataset, we used the same question prompt for all the models across the various scientific tasks. It\\u2019s\\nworth noting that each model may perform differently with different prompts. However, for the sake\\nof simplicity in our evaluation, we opted for a consistent prompt for all the models. The prompts\\nspecific to each task are displayed in Figure 7.\\n20\"}\n{\"score\":0.6065,\"pdf_id\":\"524bd91c-e8d1-540e-9f1e-da460111ba4c\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"e62743df-e5a9-565c-a2d9-23f4a6c4b3d7\",\"text\":\"changes in the prompt can drastically affect the\\nperformance when evaluated on the entire test set.\\nThis is particularly apparent in the zero-shot set-\\nting, where the in-context examples shown to the\\nmodel have a substantial influence.\\nOur prompt engineering process was highly ex-\\nperimental. We evaluated our prompts on a compa-\\nrable small subset of samples and chose the best-\\nperforming ones with minor variations. Further-\\nmore, only a few prompts were evaluated on the\"}\n{\"score\":0.6064,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4e0552c7-f71a-5c2f-800d-9d4bc311f0d9\",\"text\":\"Recognizing the problems mentioned above, we\\nfocus on improving the domain generalization\\nability of prompts with three constraints: (i) do\\nso with no knowledge about the target domain,\\n(ii) do so with little training cost, (iii) do so with\\neasy adaptation on both soft prompt and hard\\nprompt optimization. To get started, we test the\\npopular prompt optimization methods on cross-\\ndomain setting (i.e., training prompts on one\\ndomain and testing them on out-of-distribution\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks do not provide a clear answer to the question. I will try a more specific query to find the exact comparison between task-level prompts and example-specific prompts on the Multi-Domain test set.\n[Action]:\nClassicRetrieve(query='Which performs better on the Multi-Domain test set: task-level prompts or example-specific prompts?', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.662,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"222f7b8f-8ddf-57f9-9fad-8a193b109c46\",\"text\":\"target domain1) and show the results in Figure 1.\\nInterestingly, these optimized prompts exhibit\\n(i) great performance drop in general (by an\\naverage of 8.49%) on target domain, validating\\nthe existence of research gap mentioned above,\\n(ii) different domain generalization ability in\\nparticular (Acc. drops by 2.61% in best case\\nand by 18.64% in worst case), indicating the\\nexistence of distinct prompt \\u201cnature\\u201d that contributes to its generalizability.\"}\n{\"score\":0.6265,\"pdf_id\":\"0e57b18a-c261-582f-8527-2337f0aeda90\",\"page_number\":16,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"deee226f-fb49-5e4d-b324-7748ce7fde10\",\"text\":\"requiring test cases, we use the two remaining test cases. Overall execution performance is relatively poor, with accuracy\\nnever exceeding 29% for any prompt type. Results indicate that including test cases in the prompt seems to help more\\nthan any other individual component. Including test cases and natural language descriptions in the prompt lead to\\nthe highest overall performance\\u2014higher than using the code itself. Because the code unambiguously describes the\"}\n{\"score\":0.6134,\"pdf_id\":\"524bd91c-e8d1-540e-9f1e-da460111ba4c\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"e62743df-e5a9-565c-a2d9-23f4a6c4b3d7\",\"text\":\"changes in the prompt can drastically affect the\\nperformance when evaluated on the entire test set.\\nThis is particularly apparent in the zero-shot set-\\nting, where the in-context examples shown to the\\nmodel have a substantial influence.\\nOur prompt engineering process was highly ex-\\nperimental. We evaluated our prompts on a compa-\\nrable small subset of samples and chose the best-\\nperforming ones with minor variations. Further-\\nmore, only a few prompts were evaluated on the\"}\n{\"score\":0.6077,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4e0552c7-f71a-5c2f-800d-9d4bc311f0d9\",\"text\":\"Recognizing the problems mentioned above, we\\nfocus on improving the domain generalization\\nability of prompts with three constraints: (i) do\\nso with no knowledge about the target domain,\\n(ii) do so with little training cost, (iii) do so with\\neasy adaptation on both soft prompt and hard\\nprompt optimization. To get started, we test the\\npopular prompt optimization methods on cross-\\ndomain setting (i.e., training prompts on one\\ndomain and testing them on out-of-distribution\"}\n{\"score\":0.6046,\"pdf_id\":\"fb01ed88-e68e-547c-b44b-f5289090bbf3\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9f0afce9-490d-5d90-99ba-dc5f87ce7403\",\"text\":\"dataset, we used the same question prompt for all the models across the various scientific tasks. It\\u2019s\\nworth noting that each model may perform differently with different prompts. However, for the sake\\nof simplicity in our evaluation, we opted for a consistent prompt for all the models. The prompts\\nspecific to each task are displayed in Figure 7.\\n20\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide a clear answer to the question. I will try a more focused query to find the exact comparison between task-level prompts and example-specific prompts on the Multi-Domain test set.\n[Action]:\nClassicRetrieve(query='Comparison of task-level prompts and example-specific prompts on the Multi-Domain test set', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7175,\"pdf_id\":\"524bd91c-e8d1-540e-9f1e-da460111ba4c\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"e62743df-e5a9-565c-a2d9-23f4a6c4b3d7\",\"text\":\"changes in the prompt can drastically affect the\\nperformance when evaluated on the entire test set.\\nThis is particularly apparent in the zero-shot set-\\nting, where the in-context examples shown to the\\nmodel have a substantial influence.\\nOur prompt engineering process was highly ex-\\nperimental. We evaluated our prompts on a compa-\\nrable small subset of samples and chose the best-\\nperforming ones with minor variations. Further-\\nmore, only a few prompts were evaluated on the\"}\n{\"score\":0.709,\"pdf_id\":\"0e57b18a-c261-582f-8527-2337f0aeda90\",\"page_number\":16,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"deee226f-fb49-5e4d-b324-7748ce7fde10\",\"text\":\"requiring test cases, we use the two remaining test cases. Overall execution performance is relatively poor, with accuracy\\nnever exceeding 29% for any prompt type. Results indicate that including test cases in the prompt seems to help more\\nthan any other individual component. Including test cases and natural language descriptions in the prompt lead to\\nthe highest overall performance\\u2014higher than using the code itself. Because the code unambiguously describes the\"}\n{\"score\":0.6898,\"pdf_id\":\"0439a3cf-0730-5414-8418-06e525b53520\",\"page_number\":3,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"3f2a3af3-6316-5bbf-a185-b0d23a92ef47\",\"text\":\"performance on downstream tasks. This process can be iterative. Although these approaches can\\nproficiently pinpoint prompts that result in better performance for a specific task, they all necessitate\\na test set for prompt scoring. However, in real-world scenarios, such test sets are often absent. Gonen\\net al. (2022) suggests that prompt perplexity can be an effective data-free criterion. However, our\\nexperiment results tell a different story, indicating that prompt perplexity is not a reliable metric.\\n3\"}\n{\"score\":0.6802,\"pdf_id\":\"faa5110d-0ad7-5f57-abc6-a643c0800b57\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"d2870cb7-f637-5585-820f-7515ecc083c5\",\"text\":\"prompts), Koala (Geng et al., 2023) (156 prompts), HH_RLHF (Bai et al., 2022a) (129 prompts),\\nLIMA (Zhou et al., 2023) (300 prompts), crowdsourced from authors (64 prompts). In total there are\\n1130 unique prompts, providing a good coverage on a variety of task categories, e.g. writing, coding,\\nmathematical reasoning, information seeking, advice, roleplay, safety, etc. We sample 256 prompts\\nfrom them excluding those in the AlpacaEval test set as a dev set. We ran both automatic evaluation\"}\n{\"score\":0.6755,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"222f7b8f-8ddf-57f9-9fad-8a193b109c46\",\"text\":\"target domain1) and show the results in Figure 1.\\nInterestingly, these optimized prompts exhibit\\n(i) great performance drop in general (by an\\naverage of 8.49%) on target domain, validating\\nthe existence of research gap mentioned above,\\n(ii) different domain generalization ability in\\nparticular (Acc. drops by 2.61% in best case\\nand by 18.64% in worst case), indicating the\\nexistence of distinct prompt \\u201cnature\\u201d that contributes to its generalizability.\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide a clear answer to the question. I will try a more specific query to find the exact comparison between task-level prompts and example-specific prompts on the Multi-Domain test set, focusing on the specified PDF.\n[Action]:\nClassicRetrieve(query='Comparison of task-level prompts and example-specific prompts on the Multi-Domain test set in the paper with ID 5a2b95c1-12d6-5b77-82a1-ee24180d27ae', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6543,\"pdf_id\":\"df8d2d04-f907-5e26-a5a1-c549d376cca0\",\"page_number\":15,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"00a01389-94b4-5be2-a2cf-d27fabd257fd\",\"text\":\"The full user prompt for the ARC domain is shown in Fig. 8. It contains the domain-specific language,\\nfour in-context examples and the query for the test task.\\n15\"}\n{\"score\":0.6503,\"pdf_id\":\"76f35116-3d44-55a8-ae89-62637f68d1df\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"222f7b8f-8ddf-57f9-9fad-8a193b109c46\",\"text\":\"target domain1) and show the results in Figure 1.\\nInterestingly, these optimized prompts exhibit\\n(i) great performance drop in general (by an\\naverage of 8.49%) on target domain, validating\\nthe existence of research gap mentioned above,\\n(ii) different domain generalization ability in\\nparticular (Acc. drops by 2.61% in best case\\nand by 18.64% in worst case), indicating the\\nexistence of distinct prompt \\u201cnature\\u201d that contributes to its generalizability.\"}\n{\"score\":0.6489,\"pdf_id\":\"b5621481-f1f2-5609-b8f9-61202856062f\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"c7fd2f34-31e1-50f9-b357-1458e4eeff7f\",\"text\":\"value of feature B\\u201d). This results in 3 \\u00b7 4 = 12 composite prompts. For each domain, we run each\\nprompt on the 15 RMAB instances from the three datasets.\\n7\"}\n{\"score\":0.6456,\"pdf_id\":\"0e57b18a-c261-582f-8527-2337f0aeda90\",\"page_number\":16,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"deee226f-fb49-5e4d-b324-7748ce7fde10\",\"text\":\"requiring test cases, we use the two remaining test cases. Overall execution performance is relatively poor, with accuracy\\nnever exceeding 29% for any prompt type. Results indicate that including test cases in the prompt seems to help more\\nthan any other individual component. Including test cases and natural language descriptions in the prompt lead to\\nthe highest overall performance\\u2014higher than using the code itself. Because the code unambiguously describes the\"}\n{\"score\":0.6402,\"pdf_id\":\"babe92f9-bb43-5240-acd3-22bd724c150d\",\"page_number\":16,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"ed11902a-e1a8-5472-a096-4f32c7b054ad\",\"text\":\"3In this section, we expand upon the initial set of 80 prompts specified in Section A.3, incorporating additional\\nprompts provided by Allingham et al. (2023). This results in a diverse set of 320 hard prompts.\\n16\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide a clear answer to the question. I will try to retrieve the specific section that discusses the results of the experiments on the Multi-Domain test set.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '5a2b95c1-12d6-5b77-82a1-ee24180d27ae' AND section_title LIKE '%Results%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"5 Results\",\"section_content\":\"Table 2 and 3 summarize the main results for the in-domain and the out-of-domain evaluations.\"}\n{\"section_title\":\"C Results using Second Metric: Comet\",\"section_content\":\"We report translation quality using Comet (Rei et al., 2020) in Tables 14 and 15. We use the eamt22-cometinho-da model (Rei et al., 2022) to generate the scores as it was shown to achieve higher correlations with human judgments than lexical overlap metrics while being computationally effcient. Our re-ranking strategy (with $q_{m a x}=16$ ) consistently performs the best across the board except for Koran, outperforming strong $k\\\\mathsf{N N-M T}$ baselines on the multi-domain test set in 3 out of 4 settings. Adding a task-level prompt to $16\\\\ \\\\mathbf{R}\\\\cdot$ - BM25 prompts via concatenation further improves quality in 5 out of 8 settings.\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections do not provide a clear answer to the question. I will try to retrieve the specific part of the paper that discusses the comparison between task-level prompts and example-specific prompts on the Multi-Domain test set.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = '5a2b95c1-12d6-5b77-82a1-ee24180d27ae' AND section_content LIKE '%task-level prompts%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"3.1 Task-level In-context Examples\",\"section_content\":\"A good task-level in-context example should be able to elicit information learned during pretraining from the PLM. One way to measure the effcacy of an example as a prompt is via computing the translation quality of the outputs generated when prompting the PLM given an example. Hence, we select the task-level prompt as follow: For a given example sampled from the training dataset, $(x_{i},y_{i})\\\\in D^{S}$ , we create a prompt, $\\\\boldsymbol{x}_{i}^{p}$ by concatenating the example $\\\\{(x_{i},y_{i})\\\\}$ to each source in the development set. The system outputs are then generated using equation 1. We then rank examples from $D^{S}$ as task-level prompts based on the BLEU of the generated outputs against the references on this held-out development set, $D^{d e v}=\\\\{X,Y\\\\}$ :\"}\n{\"section_title\":\"5.1 In-domain Evaluation\",\"section_content\":\"A single task-level prompt is competitive with 16 random few-shot examples. Our experiment suggests that it is possible to elicit the task-level knowledge from the large-scale language model using a single prompt as opposed to using 16 random few-shot examples when translating into English (Table 2). Using a single task-level prompt (optimized on the development set) improves BLEU over using 16 random few-shot examples for 2 out of 4 translation directions (De-En, Ru-En). We hypothesize that when translating out of English, the model still benefts from getting exposed to multiple and diverse random few-shot examples as the target language model is relatively weaker.\\nMultiple example-specifc prompts are required to improve translation quality over a single tasklevel prompt. Using a single task-level $(p=1)$ ) prompt attains higher BLEU over using a single example-specifc prompt ( $\\\\chi=1$ ; BM25, RBM25) across the board. By contrast, using up to 16 BM25 prompts $(g_{\\\\mathrm{max}}\\\\,=\\\\,16)$ signifcantly improves output quality over using task-level prompts, with an average gain of 1.41 in BLEU.\\nRe-ranking BM25 retreived examples improves BLEU. Our proposed re-ranking strategy consistently improves BLEU across the board over BM25 for both values of $q_{\\\\mathrm{max}}\\\\,=\\\\,\\\\{1,16\\\\}$ showing that both the order and the choice of the in-context examples matters.\\nBoth task-level and R-BM25 examples provide complementary advantages, as combining them using a simple concatenation strategy improves output quality over task-level or R-BM25 examples. We leave the exploration of optimizing the number and the joint order of task-level and examplespecifc prompts to future work.\"}\n{\"section_title\":\"5.2 Out-of-domain Evaluation\",\"section_content\":\"As XGLM is trained on monolingual Common Crawl snapshots, translation in any domain and language could be considered an out-of-domain task. However, we hypothesize that translation in specifc domains like medical, law, or IT could still be challenging for the PLM as the model is less likely to have observed suffcient monolingual datasets for these specialized domains, in contrast to the news text found in WMT. Examples from these domains will require translating rare terminology and carry domain-specifc idiosyncrasies, which is known to pose a challenge even for a well-trained supervised neural MT model (Koehn and Knowles, 2017). Hence, we also evaluate PLM under these specialized out-of-domain scenarios.\\nDomain of few-shot in-context examples matter. Task-level in-context examples drawn from the domain of evaluation, i.e., domain-specifc, obtain on an average higher BLEU scores across the board than using examples from a distant WMT corpus as expected (Table 3) in both 1-shot $(p=1;+1.4)$ and 16-shot $(p=16;+2.7)$ settings.\\nExample-specifc prompts signifcantly improve translation quality over task-level prompts. Unlike the in-domain evaluation, retrieved and reranked example-specifc prompts (R-BM25) improve the translation quality signifcantly across the board with up to 23 BLEU gain in the Law domain using just a single example as a prompt over a task-level prompt. This can be attributed to the high lexical overlap in the examples retrieved from the training data for these domains (Table 6).\\n\\nTask-level and R-BM25 prompts are complementary. Both task-level and R-BM25 provide supporting information for a given test source sentence as concatenating these set of prompts improves output quality over using these methods independently, outperforming a strong $k\\\\mathsf{N N-M T}$ baseline on 2 out of 4 domains (Medical and IT). Where $k\\\\mathrm{NN-MT}$ utilizes token-level nearestneighbor inference with representations extracted for bitext using and in combination with a strong supervised MT model to reach the reported translation quality, our approach only uses a sentencelevel unsupervised retrieval (BM25) to provide additional context to the unseen source with a multilingual PLM that has not been trained with any known parallel supervision to reach better or comparable translation quality. Hence, our results provide support for further analysis of the translation abilities of retrieval-augmented PLM on new domains and language pairs.\\nOur manual analysis suggests that the higher gain obtained in the IT domain $\\\\left(+0.86\\\\right)$ with both task-level and example-specifc prompts can be explained by the observation that for 100 test source sentences, there are no training examples with any lexical overlap with the test source. The task-level prompt can still elicit learned information from the PLM over using no examples for these inputs.\"}\n{\"section_title\":\"6.1 Task-level Example Selection\",\"section_content\":\"Choice of Few-shot Examples We show the distribution of output quality as measured by BLEU when using 100 different examples as prompts in Figure 2. Across all four language pairs, there is a large variation in BLEU scores (up to 20 BLEU), where noisy or unrelated prompts can lead to signifcantly worse output quality. Given that most existing parallel corpora are web-crawled and the quality of bitext can vary signifcantly across different language pairs (Kreutzer et al., 2022), randomly sampled examples can under-estimate the translation quality attainable by prompting the PLM.\\nImpact of Pool Size on Task-level Prompt Selection We select the best task-level prompt based on the translation quality on the development set from a random sample of 100 examples (pool) as detailed in Section 3.1. However, one concern regarding selecting the best task-level prompt in this fashion could be that we might still be underestimating the PLM (s) performance, as a larger pool size could result in better output quality. We study the impact of using a larger pool size in Table 4 where increasing the number of examples from 100 to 1000 only leads to a gain of 0.5 points in the maximum BLEU. From the same table, we can also observe that for any subset of random 100 fewshot examples, we can extract a task-level prompt (BLEU: 36) with a small standard deviation in overall output quality (0.18).\\n\\nTable 5: Average scores obtained by top-10 1-best prompts and 10 Random 1-shot prompts (averaged across 3 seeds) on features quantifying semantic equivalence\\/translation quality (higher is better).\\nProperties of good Task-level prompts Our manual analysis on the best task-level prompts suggests that any well-formed and meaning-equivalent translation (Vyas et al., 2018; Briakou and Carpuat, 2020) could make a good task-level prompt (see examples in Appendix Table 11). To quantify the meaning equivalence of the 1-best task-level prompt against random 1-shot examples, we report the percentage of aligned words between the source and reference translation ( ${}\\\\cdot\\\\%$ Aligned words\\u201d) using fastAlign (Dyer et al., 2013) and the log probability of generating the reference translation conditioned on the source using a pre-trained multilingual NMT model, Prism-src (Thompson and Post, 2020; Agrawal et al., 2021) in Table 5.2 Across all language pairs and both metrics, task-level examples achieve higher semantic similarity scores than random 1-shot examples suggesting that task-level examples are relatively more equivalent in meaning than random examples.\\n\\nImpact of Ordering To investigate the sensitivity of the few-shot prompts ordering on MT quality, we use all possible order permutations of four randomly sampled examples and the top four task-level examples as prompts and report BLEU in Table 7. Task-level prompts are less sensitive to prompt order, as suggested by the lower standard deviation achieved in all settings, and result in higher translation quality than randomly selected examples. Across the three different runs of randomly sampled examples, there is a signifcant difference in BLEU, further corroborating that the\"}\n{\"section_title\":\"8 Conclusion\",\"section_content\":\"We investigate the choice of in-context examples selection for MT in both in-domain and out-ofdomain settings. We propose a novel recall-based re-ranking approach to utilize similar training examples as prompts and show their effcacy across multiple datasets and domains. Our fndings show that task-level prompts can provide a complementary advantage to example-specifc prompts, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets while being memory and compute effcient. Our manual analysis of the generated outputs reveals that the PLM can mimic the style of the in-context examples provided and can be used for template-based translation synthesis. These results allow future research to evaluate the potential of generating diverse and style-specifc outputs for MT.\"}\n{\"section_title\":\"9 Limitations\",\"section_content\":\"We note a few limitations of our work: a) while we systematically investigate the choice of in-context examples for both in- and out-of-domain settings for higher-resource language pairs (EnglishGerman, English-Russian), it is unclear how this in-context ability of the PLM varies for the lowerresourced language pairs; b) We only experimented with one pre-trained language model, XGLM. Our preliminary experiments suggested XGLM-7.5B to result in better translation quality than Bloom7B (Scao et al., 2022) under the same settings. However, further investigation is required to understand how these results vary across different model scales; c) We analyze different orderings for the few-shot task-level prompts but only examine limited sets of ordering (most similar to the left or right) for the example-specifc prompts. As the PLM is shown to be sensitive to the ordering of these in-context examples, it remains an open question to study how to best combine the information from multiple example-specifc prompts, with prompt ensembling being a viable option, which we leave to future work.\"}\n\nIn total, 6 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections provide the necessary context to answer the question. The paper discusses the performance of task-level prompts and example-specific prompts on the Multi-Domain test set. Based on the content, it is clear that example-specific prompts perform better in the out-of-domain evaluation.\n[Action]:\nGenerateAnswer(answer='example-specific')"}
{"role": "user", "content": "[Observation]: example-specific"}
