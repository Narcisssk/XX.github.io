{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: In the paper named\"MBIAS: Mitigating Bias in Large Language Models While Retaining Context\", to develop MBIAS, which PEFT(Parameter Efficient Fine-tuning) technique is used to finetune the model?In the paper where this technique is proposed, what's the innovations introduced to save memory?\n[Answer Format]: Your answer should be a single python list, containing two strings. The first string is the name(abbrievation) of the PEFT technique. The second string is the innovations introduced to save memory in the relevant paper.\n[Anchor PDF]: 'e49150c3-c433-515e-be7d-d3dc87048029'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to find the specific PEFT technique used in the paper \"MBIAS: Mitigating Bias in Large Language Models While Retaining Context\". I will start by querying the `metadata` table to get the `paper_id` for this paper, and then I will search for the relevant information in the `sections` and `chunks` tables.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT paper_id FROM metadata WHERE title = 'MBIAS: Mitigating Bias in Large Language Models While Retaining Context';\")"}
{"role": "user", "content": "[Observation]:\n{\"paper_id\":\"e49150c3-c433-515e-be7d-d3dc87048029\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: Now that I have the `paper_id`, I will search the `sections` and `chunks` tables to find the specific PEFT technique used in the paper.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_title, section_content FROM sections WHERE ref_paper_id = 'e49150c3-c433-515e-be7d-d3dc87048029';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"MBIAS: Mitigating Bias in Large Language Models While Retaining Context\",\"section_content\":\"Shaina Raza1,\\\\*, Anaya Raval1, Veronica Chatrath1 1Vector Institute, Toronto, ON, Canada.\\n{shaina.raza, ananya.raval, veronica.chatrath} @vectorinstitute.ai\"}\n{\"section_title\":\"Abstract\",\"section_content\":\"The deployment of Large Language Models (LLMs) in diverse applications necessitates an assurance of safety without compromising the contextual integrity of the generated content. Traditional approaches, including safetyspecifc fne-tuning or adversarial testing, often yield safe outputs at the expense of contextual meaning. This can result in a diminished capacity to handle nuanced aspects of bias and toxicity, such as underrepresentation or negative portrayals across various demographics. To address these challenges, we introduce MBIAS, an LLM framework carefully instruction fnetuned on a custom dataset designed specifcally for safety interventions. MBIAS is designed to signifcantly reduce biases and toxic elements in LLM outputs while preserving the main information. This work also details our further use of LLMs: as annotator under human supervision and as evaluator of generated content. Empirical analysis reveals that MBIAS achieves a reduction in bias and toxicity by over $30\\\\%$ in standard evaluations, and by more than $90\\\\%$ in diverse demographic tests, highlighting the robustness of our approach. We make the dataset and the fne-tuned MBIAS model available to the research community for further investigation and to ensure reproducibility. The code for this project can be accessed here 1.\"}\n{\"section_title\":\"1 Introduction\",\"section_content\":\"The rise of generative artifcial intelligence (AI) has brought forth substantial ethical and safety challenges, raising concerns of misuse, misinformation (Raza and Ding, 2022), and bias of the generated information (Wach et al., 2023). Recent initiatives in this line of research for safety in LLMs aim at aligning large language models (LLMs) with ethical norms. These efforts prioritize mitigating biases and enhancing values such as inclusivity, fairness, and non-discrimination (Guo et al., 2023). To address harmful, biased, or misleading content (referred to as \\u2018biased texts\\u2019 herein), foundational strategies include implementing safety guardrails in the outputs generated by LLMs. These guardrails provide guidelines and boundaries to ensure AI applications align with ethical standards and societal expectations (Att, 2024).\\n\\nMethods such as red-teaming (Perez et al., 2022), human feedback during pre-training (Casper et al., 2023), and data augmentation methods (e.g., balanced sampling, paraphrasing, or counterfactual data generation) (Sadasivan et al., 2023) are often used to reduce biases in LLMs, with the goal of making them safer and more aligned with human values (Ganguli et al., 2022; Korbak et al., 2023). In the fne-tuning phase, techniques like instruction tuning, reinforcement learning from human feedback (RLHF), and safety context distillation (Ouyang et al., 2022; Qi et al., 2023; Bai et al., 2022) are also used to address unsafe behaviors and improve the models\\u2019 ability to generalize.\\nDespite advancements in the implementation of LLM safety measures, one concern is the loss of actual context or meaning in the original text (Schlicht et al., 2024). This delicate balance between two competing goals \\u2014 reducing biases in the text and preserving informational integrity (Raza and Ding, 2020) \\u2014 highlights a central paradox in bias reduction efforts. Catastrophic forgetting, which occurs when a model forgets previously learned information while acquiring new knowledge, is particularly an issue worth noting while implementing safety mechanisms in these LLMs (Luo et al., 2023). Demonstrating the understanding of language models post-safety interventions has thus become a topic of research and discussion (Nadeem et al., 2021; Schick et al., 2021).\\nRecent studies indicate that incorporating even a modest number of safety-focused examples during the instruction-tuning stage can effectively mitigate certain safety concerns (Bianchi et al., 2023; Inan et al., 2023; Bai et al., 2022). State-of-the-art LLMs such as GPT-4 (OpenAI, 2023), Mistral (Jiang et al., 2023), PaLM (Anil et al., 2023), LLaMA-2 (Inan et al., 2023), LLaMA-3 (AI $@$ Meta, 2024), and Claude (Anthropic, 2024) have been further fnetuned using high-quality safety demonstrations, including perturbations and adversarial prompts, to enhance safety. While not entirely foolproof, this safety-tuning enables LLMs to reduce biases in their outputs. Advancing beyond mere demonstrations, we propose instruction fne-tuning a LLM on safety mechanisms. This approach aligns with prior research that emphasizes the importance of the quality and breadth of instruction-tuning data for developing profcient and reliable instructionfollowing models (Touvron et al., 2023; Wang et al., 2024).\\n\\nOur primary objective in this research is to create a safe LLM that can address bias and toxicity in the outputs while retaining the original content of the message. The primary contributions of this study are as follows:\\n\\u2022 We curated an instruction-tuning dataset with a focus on safety considerations. This dataset comprises paired examples: one containing potentially unsafe elements, such as stereotypes or prejudices, paired with its corresponding benign (safe) counterpart. The dataset was carefully annotated by human experts for reliable ground truth labels (safe counterparts).\\n\\u2022 We propose MBIAS (Mitigating Bias through LLM), an instruction-fne tuned model, built on Mistral2-7B-instruct (Jiang et al., 2023). The fne-tuning process involves training the model with our custom dataset that contains examples of both unsafe and safe instances, guiding the model to recognize biases and generate safe responses that can also retain the meaning of the original text.\\n\\u2022 MBIAS can be adapted for use with other LLMs. We utilize parameter-effcient fnetuning (Ding et al., 2023) to train the model, making implementation simple and straightforward. To enhance user-friendliness, we release the model weights, similar to Llama Guard (Inan et al., 2023), starting with the smallest available model. This approach ensures that researchers can easily integrate\\nMBIAS into their own projects and beneft from its bias reduction capabilities.\\n\\u2022 In this study, we further explore the versatile roles of LLMs as both an annotator and a judge or evaluator, using GPT-4. Initially, GPT-4 generates accurate ground truth labels for each unsafe input, under human oversight. Later, we employ it as an evaluator, alongside human evaluation, to assess the results of our\"}\n{\"section_title\":\"MBIAS model.\",\"section_content\":\"Experimental results on our test set and an outof-distribution test set across various demographics demonstrate the robustness of our safety interventions in LLMs. We are aware of the ethical implications of modifying user content. However, our aim is to establish a method for fair LLM generations that respects copyright boundaries, while maintaining user trust and autonomy.\"}\n{\"section_title\":\"2 Related Works\",\"section_content\":\"Safety in LLMs Establishing safety measures and protocols is crucial to upholding trust in generative AI and LLMs. Many methods have been proposed to address specifc biases (that are threats to safe model outputs) in language models. Traditional methods to ensure safe outputs includes examining the embedding space of words and sentences to mitigate biases in text. Embedding-space-based methods are often applied after training, requiring littleto-no fne-tuning of the model. These methods function as post-processing debiasing steps (Liang et al., 2020; Ungless et al., 2022). Subtractionbased methods are also used to remove biases in language models, which map the embedding space into a neutral one (Bolukbasi et al., 2016), maintaining equal distance between non-gendered words and pairs of gendered words. Another method is to ensure that gender-neutral words are orthogonal to the gender direction (Zhao et al., 2018). In a related work (Zhao et al., 2019), the gendered words are replaced with their opposites in the original training data, and the model is trained on both the original and augmented data. Other methods include effcient fne-tuning for debiasing (Gira et al., 2022) and fne-tuning only a small portion of parameters on debiasing data (Gira et al., 2022).\\nPrompt-based debiasing, ranging from prompttuning using continuous prompts (Yang et al., 2023), to self-supervised zero-shot prompting, is also used to detect and reduce bias by controlling model behavior during generation. For example, the Self-Diagnosis and Self-Debiasing methods (Schick et al., 2021) reduce the probability of language models generating biased text.\\n\\nDebiasing practicality and reliability also depend on dataset selection, evaluation methods, and demographic coverage. Datasets like RedditBias (Barikeri et al., 2021), WinoBias (Zhao et al., 2019), HolisticBias (Smith et al., 2022), RealToxicityPrompts (Gehman et al., 2020), and others discussed in Chang et al. (2024) are frequently used for evaluating bias reduction in models. A variety of bias evaluation metrics are used, such as WEAT (Liang et al., 2020), Perspective API (API, 2024), StereoSet Stereotype Score (Nadeem et al., 2021), and LLM alignment methods (Wang et al., 2023; Chang et al., 2024).\\nDespite rapid adoption, safety concerns remain (Zhou et al., 2024; Gudibande et al., 2023), particularly around production integration. Most recent LLMs, such as GPT-4, Mistral, PaLM, and Llama-2 and 3, have been instruction fne-tuned with high-quality instruction-following data. While debates persist regarding the competitiveness of fnely-tuned instruction models (Zhou et al., 2024; Gudibande et al., 2023), there has been rapid adoption of these models among the NLP community.\\nNumerous considerations on their adoption and integration into production settings are currently under observation. Our focus lies in safety implications for instruction-tuned models: specifcally, how these models respond safely to biased instructions, while retaining context. Recent research (Qian et al., 2022) indicates that training LLMs on demographically perturbed data results in fairer models. We investigate whether training on unsafe-benign text pairs can achieve better fairness in LLMs while retaining their knowledge.\\nLLM as Annotator Recent advances have showcased LLMs like GPT-3.5 and GPT-4 as promising alternative annotators (Tan et al., 2024). These models are capable of annotating data for tasks such as classifcation and entity recognition (AI, 2024b), through prompting methods. To maximize the utility of LLMs and leverage vast amounts of unlabeled data, they can be deployed as annotators within an active learning loop (Zhang et al., 2023). Furthermore, LLM annotations can undergo alignment tuning to align their outputs with human preferences (Zhao et al., 2023), ensuring the annotations remain free of biases.\\nLLM-based annotations are also shown to achieve or even exceed the performance of human annotators across various NLP tasks, including tweet annotation (Huang et al., 2023), computational science (Ziems et al., 2024), and medical information extraction (Goel et al., 2023). Furthermore, several comparisons between LLMs and human annotators highlight their potential (Gilardi et al., 2023; He et al., 2023; Pavlovic and Poesio, 2024). Motivated by these fndings, our work also employs the role of LLMs in producing benign variations for biased texts, with the goal to mitigate bias within linguistic contexts while retaining knowledge \\u2014 an area that remains relatively unexplored.\\n\\nLLM as Judge LLMs such as GPT-3.5 and GPT4 are increasingly utilized as evaluators or judges to ensure outputs align with human preferences (Zheng et al., 2024). In this capacity, LLMs can assess and judge the quality of model outputs against specifc criteria such as accuracy, toxicity, and relevance (Dubois et al., 2024; Zhou et al., 2024), utilizing methods like asking for correctness, relevance or agreement with human annotations in a controlled experiment. In a LLM evaluation 2, OpenAI\\u2019s GPT-4 led with $82\\\\%$ of the questions answered correctly, followed by Llama3 and Mixtral. Google\\u2019s Gemini Pro 1.5 managed to answer only half of the questions correctly, while GPT-3.5 ranked last. Motivated by these results, we also employ GPT-4 as a LLM judge in this work.\"}\n{\"section_title\":\"3 Method\",\"section_content\":\"For the purposes of this study, we defne bias as any textual content that portrays unfair, inaccurate, or derogatory assumptions about individuals or groups based on their inherent characteristics such as race, gender, ethnicity, or age (Raza et al., 2024). The MBIAS architecture can be seen in Figure 1.\"}\n{\"section_title\":\"3.1 Dataset Preparation\",\"section_content\":\"The dataset for this study is extracted from our vast collection of about 3.7M records3 that spans diverse content from news and social media platforms. This dataset is entirely in English and covers many aspects of bias. We carefully chose a subset of $8.5\\\\mathrm{k}$ records for this work (records with various aspects of bias and text lengths), balancing diversity with computational effciency.\\nData Labeling and Annotation Procedure: During labeling, each instance of biased text (unsafe) is transformed into its benign (safe) and debiased counterpart. In the initial step, we leverage GPT-4 from OpenAI (OpenAI) as an annotator to generate the benign versions of the data. An illustrative prompt (without demonstration) is presented below, while demonstrations are provided in Appendix A:\\nPlease rewrite the following text to create a benign variation that eliminates all forms of bias, toxicity, stereotyping, and harm, such as gender, age, race, political, and socio-economic biases. Ensure the revised text promotes inclusivity and neutrality without sacrifcing the original message essence.   \\nExample 1: \\u201cMen are naturally better leaders than women.\\u201d   \\nResponse 1: Leadership skills are not determined by gender. Anyone can be a great leader.   \\nExample 2: \\u201cYoung people are too inexperienced to lead effectively.\\u201d   \\nResponse 2: Leadership effectiveness is not limited by age. People of any age can lead well.   \\nExample 3: \\u201cPeople from certain socio-economic backgrounds are more likely to be successful.\\u201d   \\nResponse 3: Your Turn\\nAfter GPT-4\\u2019s initial labels (the benign version of each text), a human review process is performed. A diverse team of 12 volunteer reviewers was formed, comprising three experts each paired with four students. The team varies in demographics and subject-level expertise. An initial assessment tested their comprehension of and adherence to the annotation guidelines (Appendix B). The reviewers were tasked with evaluating each biased input alongside GPT-4\\u2019s benign output, making corrections if they identifed any discrepancies. Majority voting was used to fnalize the gold label for the benign text in the case of discrepancies. Some of the examples are given in Table 1.\\n\\nThe dataset schema4 is given in Listing 1 below:\\nTo ensure seamless interoperability and integration across computational environments, our dataset follows the Alpaca dataset format (Dubois et al., 2023), comprised of \\u2018instructions\\u2019, \\u2018input\\u2019, and \\u2018output\\u2019.\"}\n{\"section_title\":\"3.2 Model Training\",\"section_content\":\"MBIAS is fne-tuned on the pre-trained Mistral (Jiang et al., 2023) model using instruction fnetuning. Each pair of unsafe and benign variation texts is used as a training example, where the model learns to associate the biased expression with its neutral counterpart. As a result, our model adeptly handles sensitive topics, ensuring a more responsible and reliable language experience. We selected the smallest variant, Mistral-7B-Instructv0.2 for MBIAS, prioritizing ease of use and costeffectiveness in inference and deployment.\\nThis instruction format follows the following structure:\"}\n{\"section_title\":\"$<\\\\leq>$ [INST] {sys_message} {instruction} [\\/INST] {user_prompt}<\\/s>\",\"section_content\":\"Here, the [INST] strings mark the beginning and end of the instruction, sys_message provides context for the LLM, instruction is the specifc task we want the model to perform, and user_prompt is the user\\u2019s input or query.\\n$<\\\\leq>$ [INST] You are a text debiasing bot. You take as input a text and you output its debiased version by rephrasing it to be free from any age, gender, political, social or socio-economic biases, without any extra outputs: [INST] \\u201cHow dumb can the school system get. Almost every day there is a column about the stupidity of the system.\\u201d [\\/INST] The school system could use some improvement. It is concerning to see frequent criticisms of its effectiveness. <\\/s>\\nEffcient Fine-Tuning with QLoRA To develop MBIAS, we employ QLoRA (Quantized Language Model Optimization with Low Rank Adapters) (Belkada et al., 2023), a ParameterEffcient Fine-tuning (PEFT) technique using bitsandbytes (Dettmers et al., 2022), alongside the HuggingFace transformers Trainer class, to fnetune the Mistral-7B-instruct- $.\\\\mathrm{v}0.2^{5}$ model with our specialized instruction dataset. QLoRA effectively reduces the memory demands for achieving robust fne-tuning outcomes. It balances accuracy with resource effciency through a 4-bit NormalFloat (NF4) representation, double quantization, and paged optimizers. We release our model weights on HuggingFace. 6\"}\n{\"section_title\":\"4 Experiments\",\"section_content\":\"\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The PEFT technique used in the paper is QLoRA (Quantized Language Model Optimization with Low Rank Adapters). Next, I need to find the innovations introduced to save memory in the relevant paper where QLoRA is proposed. I will use the vectorstore to retrieve this information.\n[Action]:\nClassicRetrieve(query='QLoRA innovations to save memory', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6472,\"pdf_id\":\"5758e627-a060-5d43-a950-86915bd9ddcf\",\"page_number\":2,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2811dad5-16f4-5012-9959-4f3ac75c6969\",\"text\":\"the original parameters are frozen, this method is non-destructive, and the adapters can be switched\\nor combined.\\nQLoRA\\nQLoRA, like LoRA, freezes the pre-trained model\\u2019s weights and inserts low-rank matrices\\nas adapters into each layer of the Transformer architecture. However, QLoRA further reduces memory\\nrequirements by quantizing the model to 4-bit precision during fine-tuning. This significantly lowers\\nthe memory footprint while retaining the model\\u2019s performance. By freezing the original parameters,\\n2\"}\n{\"score\":0.6247,\"pdf_id\":\"c7b3bb82-aadc-5ff3-80e5-6a87188c8e20\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"0c696f41-ece0-50dc-9602-4266a63c85b0\",\"text\":\"prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORA has one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORA weight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\"}\n{\"score\":0.6159,\"pdf_id\":\"c7b3bb82-aadc-5ff3-80e5-6a87188c8e20\",\"page_number\":5,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"8b89dd17-6bdd-5a03-acb6-5fb1c3169459\",\"text\":\"fine-grained control over the memory footprint to fit\\nmodels of certain size (33B\\/65B) into certain GPUs\\n(24\\/48GB).\\nWe have discussed how QLoRA works and how\\nit can significantly reduce the required memory\\nfor finetuning models. The main question now is\\nwhether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the\\ncomponents of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The follow-\\ning sections will discuss the experiments that aimed\"}\n{\"score\":0.6062,\"pdf_id\":\"c7b3bb82-aadc-5ff3-80e5-6a87188c8e20\",\"page_number\":10,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2e76133b-3c68-5374-a0ee-96c3036d5c9f\",\"text\":\"can finetune 3 million tokens per night while the phone is charging. QLORA can help enable\\nprivacy-preserving usage of LLMs, where users can own and manage their own data and models,\\nwhile simultaneously making LLMs easier to deploy.\\nFurthermore, because of the increased inference efficiency of 4-bit models, if QLoRA models are\\ndeployed they reduce the environmental impact that LLMs have when deployed for personal use.\"}\n{\"score\":0.604,\"pdf_id\":\"7a99573f-284a-542e-bf03-6acf46264ab3\",\"page_number\":8,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"a6b6dc76-525f-503a-aa6b-51fef5bf6c35\",\"text\":\"memory. In particular, we reduce the memory for 0.89GB, a relative improvement of 15.4% from\\nQLoRA. We observe that this performance improvement is maintained on the larger model of\\n13B parameters, where again our method outperforms QLoRA by 0.5pp and lowers the memory\\nrequirements by 1.43GB, a relative improvement of 14.4%.\\n4.5\\nPre-training on C4\\nWe now perform an experiment where we use VeLoRA to train language models from scratch in\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The innovations introduced in the QLoRA paper to save memory include quantizing the model to 4-bit precision during fine-tuning, which significantly lowers the memory footprint while retaining the model's performance. Additionally, QLoRA uses a 4-bit NormalFloat (NF4) representation, double quantization, and paged optimizers to balance accuracy with resource efficiency.\n[Action]:\nGenerateAnswer(answer=['QLoRA', 'Quantizing the model to 4-bit precision during fine-tuning, which significantly lowers the memory footprint while retaining the modelâ€™s performance. By freezing the original parameters, QLoRA uses a 4-bit NormalFloat (NF4) representation, double quantization, and paged optimizers to balance accuracy with resource efficiency.'])"}
{"role": "user", "content": "[Observation]: ['QLoRA', 'Quantizing the model to 4-bit precision during fine-tuning, which significantly lowers the memory footprint while retaining the modelâ€™s performance. By freezing the original parameters, QLoRA uses a 4-bit NormalFloat (NF4) representation, double quantization, and paged optimizers to balance accuracy with resource efficiency.']"}
