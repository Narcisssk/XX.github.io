{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: In the previous work of \"Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks\" which uses the same game, how many training hours does it take in average for the agent trained with 128 CPU cores to beat the behavior-tree AI?\n[Answer Format]: Your answer should be a float, rounding to 2 decimal places.\n[Anchor PDF]: '1caab0d7-f86a-52d7-9c89-8e4506d76b1f'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: The question asks for the average training hours for the agent trained with 128 CPU cores to beat the behavior-tree AI in the paper \"Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks\". The paper ID is '1caab0d7-f86a-52d7-9c89-8e4506d76b1f'. I will first retrieve the relevant sections or pages from the database that might contain this information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '1caab0d7-f86a-52d7-9c89-8e4506d76b1f' AND section_title IN ('Experimental Setup', 'Results', 'Training Details');\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the sections I specified might not exist in the paper. I will try a more general query to retrieve all sections and pages that might contain the relevant information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content, section_title FROM sections WHERE ref_paper_id = '1caab0d7-f86a-52d7-9c89-8e4506d76b1f';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Introduction\\nOnline Reinforcement Learning (Online RL) relies on the interaction between the training policy\\nand the environment for data collection and policy optimization [33, 18, 7]. However, this paradigm\\nmakes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous\\ndriving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19].\\nIn contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a\\nfixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This\\ncharacteristic alleviates the aforementioned issue, making offline RL potentially more suitable for\\ncertain real-world scenarios compared to online RL [23].\\nThe research on offline RL has attracted significant attention in recent years and has made substantial\\nprogress in both theoretical analysis and practical performance. The core challenge of offline RL is\\nthe value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this\\nproblem by constraining the learning policy to closely resemble the behavior policy induced by the\\ndataset [39, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be\\nlargely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL\\nUnplugged [12]. These datasets offer standardized and diverse pre-collected data for the development\\nof new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons\\nbetween different algorithms. However, despite their benefits, tasks contained in these datasets (such\\n* Authors contributed equally\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\nas Atari 2600 [3] or Mujoco [35]) are often overly simplistic or purely academic, failing to simulate\\nthe complexity of real-world scenarios and lacking practical applications. Thus, there is a significant\\ngap between offline RL research and its practical application in real-world settings. This disparity\\nhinders the usefulness of offline RL in addressing real-world problems, and thus, it is indispensable\\nto create datasets that reflect a realistic level of complexity and practicality in real-world applications.\\nOffline Multi-Agent Reinforcement Learning (Offline MARL) [28, 43] is gaining increasing attention\\ndue to its close relationship to real scenarios, such as games [31, 38], sensor networks [49] and\\nautonomous vehicle control [42]. However, the lack of standardized datasets restricts the development\\n40 of offline MARL. Existing works only rely on self-made datasets, which hampers fairness and\\nreproducibility. Moreover, the settings they focus on are typically limited to toy examples (e.g.,\\nMulti-agent Particle Environment [26]) or simplified versions of classic games (e.g., StarCraft Multi-\\nAgent Challenge [31]), facing the same impractical issues encountered in offline RL. Thus, there is\\nan urgent need for open-access datasets to further the progress of offline MARL.\\nThe connections between offline RL and offline MARL are closely linked due to similar challenges\\npertaining to offline learning. While, offline MARL also introduces distinct algorithm development\\nrequirements, as it involves unique characteristics like multiple agents and intra-team cooperation.\\nThe current offline MARL algorithms [28, 43] are mainly adaptations of offline RL algorithms,\\nnecessitating the availability of standardized offline datasets that cater to both single-agent and\\nmulti-agent settings. Thus, to enhance versatility and practicality, it is crucial to propose datasets that\\nencompass both single-agent settings and multi-agent settings.\\nIn this paper, we present Hokoff, a suite of pre-collected datasets for both offline RL and offline\\nMARL, along with a comprehensive framework for conducting corresponding research. Our paper\\nmakes several novel contributions, which are shown below:\\n\\u2022\\nThe tasks we adopt are based on one of the world\\u2019s most popular Multiplayer Online Battle\\nArena (MOBA) games, Honor of Kings (HoK), which has over 100 million daily active players[38],\\nensuring the practicality of our datasets. The complexity of this environment dramatically surpasses\\nthose of its counterparts, demonstrating the potential for simulating real-world scenarios.\\n\\u2022 We present an open-source, easy-to-use framework 1 under Apache License V2.0. This framework\\nincludes comprehensive processes for offline RL (sampling, training, and evaluation), and some\\nuseful tools. Based on the framework, We release a rich and diverse set of datasets 2 which are\\ngenerated using a series of pre-trained models featuring distinct design factors. These datasets cater\\nnot only to offline RL but also offline MARL.\\n\\u2022 Building on the framework, we reproduce various offline RL and offline MARL algorithms and\\npropose a novel baseline algorithm tailored for the inherent hierarchical structured action space of\\nHonor of Kings. We fully validate and compare these baselines on our datasets. The results indicate\\nthat current offline RL and offline MARL approaches are unable to effectively address complex\\ntasks with discrete action space. Additionally, these methods exhibit shortcomings in terms of their\\ngeneralization capabilities and their ability to facilitate multi-task learning.\\n2\",\"section_title\":\"Introduction\"}\n{\"section_content\":\"Related Works\\n2.1\\nOffline RL and Offline MARL\\nOffline RL gains significant attention in recent years, primarily due to the inherent difficulties of\\ndirectly applying online RL algorithms to offline environments. The main hurdles encountered is\\nthe issue of erroneous value overestimation, which arises from the distributional shift between the\\ndataset and the learning policy [9]. Theoretical studies have demonstrated that the overestimation\\nissue can be alleviated by pessimism, which results in satisfactory performance even with imperfect\\ndata coverage [4, 5, 14, 20, 25, 30, 41, 48]. In practice, certain studies [1, 2, 40, 1, 2, 40] employ\\nuncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic\\ndynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18,\\n9, 39, 16, 19, 7] adopt behavior regularization-based approaches by imposing constraints on the\\n1https:\\/\\/github.com\\/tencent-ailab\\/hokoff\\n2https:\\/\\/sites.google.com\\/view\\/hok-offline\\n2\\nlearned policy to align closely with the behavior policy, either explicitly or implicitly, which offers\\nbetter computational efficiency and memory consumption compared to uncertainty-based methods.\\nOffline MARL, combining offline RL and MARL, has emerged in recent years to address safety and\\ntraining efficiency concerns in practical multi-agent scenarios. Most studies in this domain adopt a\\nmulti-agent paradigm, such as independent learning [34] or centralized training with decentralized\\nexecution (CTDE) [26]. These investigations also incorporat offline methods, similar to those\\nemployed in single-agent settings, to mitigate distributional shift. Moreover, innovative treatments\\nare introduced for cooperation, such as zeroth-order optimization in OMAR [28] or decomposing\\nthe joint-policy in MAICQ [43]. In addition, Jiang & Lu [13] specifically focuses on decentralized\\nlearning using BCQ [9], while Tseng et al. [36] regards offline MARL as a sequence modeling\\nproblem, utilizing supervised learning and knowledge distillation to tackle the challenges it presents.\\n2.2\\nOffline Datasets\\nThe availability of large-scale pre-collected datasets has greatly facilitated the progress of deep\\nsupervised learning [11]. Offline RL, which is regarded as a bridge between RL and supervised\\nlearning, also requires learning policies from pre-collected datasets [6]. Therefore, high-quality\\npre-collected offline datasets play a significant role in the development of offline RL. To meet this\\ndemand, some datasets have been published and widely adopted. D4RL [6] is designed to address\\nkey challenges often faced in practical applications where datasets may have limited and biased\\ndistributions, incomplete observations, and suboptimal data. To tackle these issues, D4RL offers\\na range of datasets that enjoy these characteristics. Similarly, RL Unplugged [12] introduces a\\nbenchmark to evaluate and compare offline RL methods with various settings, such as partially or\\nfully observable and continuous or discrete actions. These offline datasets play a significant role\\nin offline RL research, and many previous works train and evaluate their methods based on these\\ndatasets [16, 2, 15, 28, 43].\\nHowever, both D4RL and RL Unplugged primarily focus on relatively simple tasks and lack high-\\ndimensional, practical and multi-agent tasks that closely resemble real-world scenarios. StarCraft\\nII Unplugged [27] introduces a benchmark for StarCraft II, a complex simulated environment with\\nseveral practical properties. However, they only utilize a dataset derived from human replays, which\\nlacks diversity in design for offline RL, and they did not evaluate existing offline RL methods. To\\naddress this research gap, we propose Hokoff, a benchmark based on HoK, which aims to provide\\ndiverse offline datasets for high-dimensional, practical tasks, and present a comprehensive evaluation\\nof previous offline RL and offline MARL methods with a general, easy-to-use framework.\\n3\",\"section_title\":\"Related Works\"}\n{\"section_content\":\"Background\\nHonor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million\\ndaily active players [38]. The game involves two teams, each consisting of several players who have\\nthe option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes\\nare expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience.\\nThe primary objective is to destroy the enemies\\u2019 turrets and crystal while defending their own. To\\nsucceed in MOBA games, players must learn how to choose the appropriate hero combination, master\\ncomplex information processing and action control, plan for long-term decision-making, cooperate\\nwith allies, and balance multiple interests. The complex rules and properties of HoK make it be\\nmore in line with the complex decision-making behavior of human society. Thus, HoK has attracted\\nnumerous researchers interest [45, 44, 38, 37, 10].\\nThe underlying system dynamic of HoK can be characterized by a Partially Observable Markov\\nDecision Process (POMDP [33]), denoted by M = (S, O, A, P, r, \\u03b3, d). Due to the fog of war and\\nprivate features, each agent has access to only local observations o rather than the global state s.\\nSpecifically, the agents are limited to perceiving information about game units within their field\\nof view, as well as certain global features. Due to the intricate nature of control, the action space\\nA is organized in a hierarchically structured manner, rather than being flattened, which avoids the\\nrepresentation of millions of discretized actions. Randomness is added into the transition distribution\\nP in the form of critical hit rate. The reward r is decomposed into multi-head form and each hero\\u2019s\\nreward is a weighted sum of different reward items and is designed to be zero-sum. Details of\\nobservation space, action space and reward are presented in Appendix D.\\n3\\n(a) HoK1v1\\n(b) HoK3v3\\nFigure 1: (a) The Game replay user interface (UI) in HoK1v1. (b) The UI in HoK3v3. Important\\ninformation and units of the game are highlighted using orange boxes.\\n4\",\"section_title\":\"Background\"}\n\n... # only display 3 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved sections do not contain the specific information about the training hours. I will try to retrieve the relevant content from the `chunks` table, which contains more granular text content.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT text_content FROM chunks WHERE ref_paper_id = '1caab0d7-f86a-52d7-9c89-8e4506d76b1f';\")"}
{"role": "user", "content": "[Observation]:\n{\"text_content\":\"Hokoff: Real Game Dataset from Honor of Kings and\\nits Offline Reinforcement Learning Benchmarks\\nYun Qu\\u2217\\u2020, Boyuan Wang\\u2217\\u2020, Jianzhun Shao\\u2217\\u2020, Yuhang Jiang\\u2020, Chen Chen\\u2020, Zhenbin Ye\\u266e, Lin\\nLiu\\u266e, Junfeng Yang\\u266e, Lin Lai\\u266e, Hongyang Qin\\u00a7, Minwen Deng\\u00a7, Juchao Zhuo\\u00a7, Deheng Ye\\u00a7,\\nQiang Fu\\u00a7, Wei Yang\\u00a7, Guang Yang\\u266e, Lanxiao Huang\\u266e, Xiangyang Ji\\u2020\\n\\u2020Tsinghua University, \\u266eTencent Timi Studio, \\u00a7Tencent AI Lab\\n{qy22,wangby22,sjz18,jiangyh19}@mails.tsinghua.edu.cn,chenchen.peach@gmail.com,\"}\n{\"text_content\":\"{zhenbinye,lincliu,fengjunyang,linlai,hongyangqin,danierdeng,jojozhuo,dericye,leonfu,\\nwillyang,mikoyang,jackiehuang}@tencent.com,xyji@tsinghua.edu.cn\\nAbstract\\nThe advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent\\nReinforcement Learning (MARL) critically depends on the availability of high-\\nquality, pre-collected offline datasets that represent real-world complexities and\\npractical applications. However, existing datasets often fall short in their simplicity\"}\n{\"text_content\":\"and lack of realism. To address this gap, we propose Hokoff, a comprehensive set\\nof pre-collected datasets that covers both offline RL and offline MARL, accompa-\\nnied by a robust framework, to facilitate further research. This data is derived from\\nHonor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game\\nknown for its intricate nature, closely resembling real-life situations. Utilizing this\\nframework, we benchmark a variety of offline RL and offline MARL algorithms.\"}\n{\"text_content\":\"We also introduce a novel baseline algorithm tailored for the inherent hierarchi-\\ncal action space of the game. We reveal the incompetency of current offline RL\\napproaches in handling task complexity, generalization and multi-task learning.\\n1\\nIntroduction\\nOnline Reinforcement Learning (Online RL) relies on the interaction between the training policy\\nand the environment for data collection and policy optimization [33, 18, 7]. However, this paradigm\"}\n{\"text_content\":\"makes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous\\ndriving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19].\\nIn contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a\\nfixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This\\ncharacteristic alleviates the aforementioned issue, making offline RL potentially more suitable for\"}\n{\"text_content\":\"certain real-world scenarios compared to online RL [23].\\nThe research on offline RL has attracted significant attention in recent years and has made substantial\\nprogress in both theoretical analysis and practical performance. The core challenge of offline RL is\\nthe value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this\\nproblem by constraining the learning policy to closely resemble the behavior policy induced by the\"}\n{\"text_content\":\"dataset [39, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be\\nlargely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL\\nUnplugged [12]. These datasets offer standardized and diverse pre-collected data for the development\\nof new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons\\nbetween different algorithms. However, despite their benefits, tasks contained in these datasets (such\"}\n{\"text_content\":\"* Authors contributed equally\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\"}\n{\"text_content\":\"as Atari 2600 [3] or Mujoco [35]) are often overly simplistic or purely academic, failing to simulate\\nthe complexity of real-world scenarios and lacking practical applications. Thus, there is a significant\\ngap between offline RL research and its practical application in real-world settings. This disparity\\nhinders the usefulness of offline RL in addressing real-world problems, and thus, it is indispensable\"}\n{\"text_content\":\"to create datasets that reflect a realistic level of complexity and practicality in real-world applications.\\nOffline Multi-Agent Reinforcement Learning (Offline MARL) [28, 43] is gaining increasing attention\\ndue to its close relationship to real scenarios, such as games [31, 38], sensor networks [49] and\\nautonomous vehicle control [42]. However, the lack of standardized datasets restricts the development\\n40 of offline MARL. Existing works only rely on self-made datasets, which hampers fairness and\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The retrieved chunks do not contain the specific information about the training hours either. I will try to retrieve the relevant content from the `sections` table with a more specific section title that might contain the experimental details.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content FROM sections WHERE ref_paper_id = '1caab0d7-f86a-52d7-9c89-8e4506d76b1f' AND section_title IN ('Experiments', 'Experimental Results', 'Results', 'Training Details', 'Evaluation');\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the sections I specified might not exist in the paper. I will try to retrieve the relevant content from the `sections` table with a broader query to get more sections and then filter out the relevant information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT section_content, section_title FROM sections WHERE ref_paper_id = '1caab0d7-f86a-52d7-9c89-8e4506d76b1f';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Introduction\\nOnline Reinforcement Learning (Online RL) relies on the interaction between the training policy\\nand the environment for data collection and policy optimization [33, 18, 7]. However, this paradigm\\nmakes online RL unsuitable for certain real-world scenarios, such as robotics and autonomous\\ndriving [23, 18], as deploying untested policies to the environment can be costly and dangerous [19].\\nIn contrast, Offline Reinforcement Learning (Offline RL) can learn satisfactory policies using a\\nfixed dataset without the need for further interaction with the environment [23, 18, 7, 19]. This\\ncharacteristic alleviates the aforementioned issue, making offline RL potentially more suitable for\\ncertain real-world scenarios compared to online RL [23].\\nThe research on offline RL has attracted significant attention in recent years and has made substantial\\nprogress in both theoretical analysis and practical performance. The core challenge of offline RL is\\nthe value overestimation issue induced by distributional shift [9, 21]. Existing studies mitigate this\\nproblem by constraining the learning policy to closely resemble the behavior policy induced by the\\ndataset [39, 18, 7], or adopting conservative value iteration [19, 16]. The success of offline RL can be\\nlargely attributed to the availability of widely-adopted open-access datasets, such as D4RL [6] and RL\\nUnplugged [12]. These datasets offer standardized and diverse pre-collected data for the development\\nof new algorithms, while also offering proper evaluation protocols that facilitate fair comparisons\\nbetween different algorithms. However, despite their benefits, tasks contained in these datasets (such\\n* Authors contributed equally\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\nas Atari 2600 [3] or Mujoco [35]) are often overly simplistic or purely academic, failing to simulate\\nthe complexity of real-world scenarios and lacking practical applications. Thus, there is a significant\\ngap between offline RL research and its practical application in real-world settings. This disparity\\nhinders the usefulness of offline RL in addressing real-world problems, and thus, it is indispensable\\nto create datasets that reflect a realistic level of complexity and practicality in real-world applications.\\nOffline Multi-Agent Reinforcement Learning (Offline MARL) [28, 43] is gaining increasing attention\\ndue to its close relationship to real scenarios, such as games [31, 38], sensor networks [49] and\\nautonomous vehicle control [42]. However, the lack of standardized datasets restricts the development\\n40 of offline MARL. Existing works only rely on self-made datasets, which hampers fairness and\\nreproducibility. Moreover, the settings they focus on are typically limited to toy examples (e.g.,\\nMulti-agent Particle Environment [26]) or simplified versions of classic games (e.g., StarCraft Multi-\\nAgent Challenge [31]), facing the same impractical issues encountered in offline RL. Thus, there is\\nan urgent need for open-access datasets to further the progress of offline MARL.\\nThe connections between offline RL and offline MARL are closely linked due to similar challenges\\npertaining to offline learning. While, offline MARL also introduces distinct algorithm development\\nrequirements, as it involves unique characteristics like multiple agents and intra-team cooperation.\\nThe current offline MARL algorithms [28, 43] are mainly adaptations of offline RL algorithms,\\nnecessitating the availability of standardized offline datasets that cater to both single-agent and\\nmulti-agent settings. Thus, to enhance versatility and practicality, it is crucial to propose datasets that\\nencompass both single-agent settings and multi-agent settings.\\nIn this paper, we present Hokoff, a suite of pre-collected datasets for both offline RL and offline\\nMARL, along with a comprehensive framework for conducting corresponding research. Our paper\\nmakes several novel contributions, which are shown below:\\n\\u2022\\nThe tasks we adopt are based on one of the world\\u2019s most popular Multiplayer Online Battle\\nArena (MOBA) games, Honor of Kings (HoK), which has over 100 million daily active players[38],\\nensuring the practicality of our datasets. The complexity of this environment dramatically surpasses\\nthose of its counterparts, demonstrating the potential for simulating real-world scenarios.\\n\\u2022 We present an open-source, easy-to-use framework 1 under Apache License V2.0. This framework\\nincludes comprehensive processes for offline RL (sampling, training, and evaluation), and some\\nuseful tools. Based on the framework, We release a rich and diverse set of datasets 2 which are\\ngenerated using a series of pre-trained models featuring distinct design factors. These datasets cater\\nnot only to offline RL but also offline MARL.\\n\\u2022 Building on the framework, we reproduce various offline RL and offline MARL algorithms and\\npropose a novel baseline algorithm tailored for the inherent hierarchical structured action space of\\nHonor of Kings. We fully validate and compare these baselines on our datasets. The results indicate\\nthat current offline RL and offline MARL approaches are unable to effectively address complex\\ntasks with discrete action space. Additionally, these methods exhibit shortcomings in terms of their\\ngeneralization capabilities and their ability to facilitate multi-task learning.\\n2\",\"section_title\":\"Introduction\"}\n{\"section_content\":\"Related Works\\n2.1\\nOffline RL and Offline MARL\\nOffline RL gains significant attention in recent years, primarily due to the inherent difficulties of\\ndirectly applying online RL algorithms to offline environments. The main hurdles encountered is\\nthe issue of erroneous value overestimation, which arises from the distributional shift between the\\ndataset and the learning policy [9]. Theoretical studies have demonstrated that the overestimation\\nissue can be alleviated by pessimism, which results in satisfactory performance even with imperfect\\ndata coverage [4, 5, 14, 20, 25, 30, 41, 48]. In practice, certain studies [1, 2, 40, 1, 2, 40] employ\\nuncertainty-based methods to estimate Q-values pessimistically or to perform learning on pessimistic\\ndynamic models by estimating the epistemic uncertainty of Q-values or dynamics. Some studies [18,\\n9, 39, 16, 19, 7] adopt behavior regularization-based approaches by imposing constraints on the\\n1https:\\/\\/github.com\\/tencent-ailab\\/hokoff\\n2https:\\/\\/sites.google.com\\/view\\/hok-offline\\n2\\nlearned policy to align closely with the behavior policy, either explicitly or implicitly, which offers\\nbetter computational efficiency and memory consumption compared to uncertainty-based methods.\\nOffline MARL, combining offline RL and MARL, has emerged in recent years to address safety and\\ntraining efficiency concerns in practical multi-agent scenarios. Most studies in this domain adopt a\\nmulti-agent paradigm, such as independent learning [34] or centralized training with decentralized\\nexecution (CTDE) [26]. These investigations also incorporat offline methods, similar to those\\nemployed in single-agent settings, to mitigate distributional shift. Moreover, innovative treatments\\nare introduced for cooperation, such as zeroth-order optimization in OMAR [28] or decomposing\\nthe joint-policy in MAICQ [43]. In addition, Jiang & Lu [13] specifically focuses on decentralized\\nlearning using BCQ [9], while Tseng et al. [36] regards offline MARL as a sequence modeling\\nproblem, utilizing supervised learning and knowledge distillation to tackle the challenges it presents.\\n2.2\\nOffline Datasets\\nThe availability of large-scale pre-collected datasets has greatly facilitated the progress of deep\\nsupervised learning [11]. Offline RL, which is regarded as a bridge between RL and supervised\\nlearning, also requires learning policies from pre-collected datasets [6]. Therefore, high-quality\\npre-collected offline datasets play a significant role in the development of offline RL. To meet this\\ndemand, some datasets have been published and widely adopted. D4RL [6] is designed to address\\nkey challenges often faced in practical applications where datasets may have limited and biased\\ndistributions, incomplete observations, and suboptimal data. To tackle these issues, D4RL offers\\na range of datasets that enjoy these characteristics. Similarly, RL Unplugged [12] introduces a\\nbenchmark to evaluate and compare offline RL methods with various settings, such as partially or\\nfully observable and continuous or discrete actions. These offline datasets play a significant role\\nin offline RL research, and many previous works train and evaluate their methods based on these\\ndatasets [16, 2, 15, 28, 43].\\nHowever, both D4RL and RL Unplugged primarily focus on relatively simple tasks and lack high-\\ndimensional, practical and multi-agent tasks that closely resemble real-world scenarios. StarCraft\\nII Unplugged [27] introduces a benchmark for StarCraft II, a complex simulated environment with\\nseveral practical properties. However, they only utilize a dataset derived from human replays, which\\nlacks diversity in design for offline RL, and they did not evaluate existing offline RL methods. To\\naddress this research gap, we propose Hokoff, a benchmark based on HoK, which aims to provide\\ndiverse offline datasets for high-dimensional, practical tasks, and present a comprehensive evaluation\\nof previous offline RL and offline MARL methods with a general, easy-to-use framework.\\n3\",\"section_title\":\"Related Works\"}\n{\"section_content\":\"Background\\nHonor of Kings (HoK) is one of the most popular MOBA games worldwide, boasting over 100 million\\ndaily active players [38]. The game involves two teams, each consisting of several players who have\\nthe option to select from a wide range of heroes with diverse roles and abilities. In the game, heroes\\nare expected to eliminate enemy units, such as heroes, creeps, and turrets, to gain gold and experience.\\nThe primary objective is to destroy the enemies\\u2019 turrets and crystal while defending their own. To\\nsucceed in MOBA games, players must learn how to choose the appropriate hero combination, master\\ncomplex information processing and action control, plan for long-term decision-making, cooperate\\nwith allies, and balance multiple interests. The complex rules and properties of HoK make it be\\nmore in line with the complex decision-making behavior of human society. Thus, HoK has attracted\\nnumerous researchers interest [45, 44, 38, 37, 10].\\nThe underlying system dynamic of HoK can be characterized by a Partially Observable Markov\\nDecision Process (POMDP [33]), denoted by M = (S, O, A, P, r, \\u03b3, d). Due to the fog of war and\\nprivate features, each agent has access to only local observations o rather than the global state s.\\nSpecifically, the agents are limited to perceiving information about game units within their field\\nof view, as well as certain global features. Due to the intricate nature of control, the action space\\nA is organized in a hierarchically structured manner, rather than being flattened, which avoids the\\nrepresentation of millions of discretized actions. Randomness is added into the transition distribution\\nP in the form of critical hit rate. The reward r is decomposed into multi-head form and each hero\\u2019s\\nreward is a weighted sum of different reward items and is designed to be zero-sum. Details of\\nobservation space, action space and reward are presented in Appendix D.\\n3\\n(a) HoK1v1\\n(b) HoK3v3\\nFigure 1: (a) The Game replay user interface (UI) in HoK1v1. (b) The UI in HoK3v3. Important\\ninformation and units of the game are highlighted using orange boxes.\\n4\",\"section_title\":\"Background\"}\n\n... # only display 3 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved sections do not contain the specific information about the training hours. I will now perform a similarity search in the vectorstore to find the relevant context.\n[Action]:\nClassicRetrieve(query='average training hours 128 CPU cores beat behavior-tree AI', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6014,\"pdf_id\":\"61e6162d-74ba-57f7-8721-9545b4843b13\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"b3f8eafa-05d9-5074-aede-7e2a72bc7a8d\",\"text\":\"Table 1: Training time to beat BT under different computing resources (standard deviation, each experiment\\nrun 3 times). With limited CPU cores, the self-play agent is able to beat BT using around 6 hours, and the time\\nusing additional CPU cores over 512 becomes about one hour.\\nCPU Cores\\nTraining hours\\nConsumption freq.\\n128\\n6.16\\u00b10.009\\n23.80 \\u00b10.87\\n256\\n1.67\\u00b10.021\\n10.60\\u00b10.64\\n512\\n1.08\\u00b10.025\\n5.21\\u00b10.07\\n1204\\n0.90\\u00b10.06\\n2.69\\u00b10.03\\n2048\\n0.89\\u00b10.06\\n1.30\\u00b10.008\\nPerformance of Different Models\"}\n{\"score\":0.5837,\"pdf_id\":\"f28be1e4-358a-505c-9803-c13c6117a736\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"002d62eb-a631-581d-b392-1055fa0790ec\",\"text\":\"D\\nExperimental Details\\nWe used Google Cloud Platform (GCP) nodes to run all experiments. Figure 1 experiments were run\\non a node with 360 AMD Genoa CPU cores with runtime totaling approximately 90 minutes including\\naveraging over seeds as described below. Neural network training and NTK calculation for Figure 5\\nwas performed on single A100 GPU nodes. Runtime was approximately 20 hours for Figure 5(a),\\nfour hours for 5(b), 12 hours for 5(c) (with individual runs ranging from five to 30 minutes depending\"}\n{\"score\":0.5715,\"pdf_id\":\"d1abb254-da54-595c-9e2a-cb662d3a5841\",\"page_number\":21,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"960b2918-4438-5e45-8a74-d9bb8363042d\",\"text\":\"Our experiments are conducted on a high-performance cluster with 2.6Ghz CPUs with 56 threads\\nand 64gb RAM per node. The algorithms are coded in Python 3 and we use PyTorch to construct\\nneural network architectures (Paszke et al., 2019). For the maze environment, having relatively long\\nepisodes, the average training time was 21 CPU hours. The inventory and job-shop environments\\ntook on average 12 and 10 CPU hours to train, respectively.\\nH\\nCOMPLEMENTARY RESULTS\"}\n{\"score\":0.5633,\"pdf_id\":\"5ef6c2fa-a827-5217-8a3c-26b70e4c5512\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"d60cf0bb-abae-5e9f-9137-77fa4dcc7905\",\"text\":\"consumed more compute, which we estimate would have added 10 CPU and GPU hours, to account\\nfor experimentation.\\nTable 4: Compute resources\\nResource\\nValue (1 Run)\\nValue (Training Total)\\nValue (Evaluation & Analysis)\\nNodes\\n1\\n8\\n1\\nCPU\\n16 cores\\n128 cores\\n64 cores\\nGPU\\n1\\n8\\n1\\nMemory\\n50 GB\\n400 GB\\n120 GB\\nStorage\\n1 GB\\n32 GB\\n32 GB\\nWall time\\n2 hours\\n240 hours\\n24 hours\\nTable 5: Hyperparameters\\nParameter\\nValue\\nEpochs\\n1000\\nOptimizer\\nAdam\\nLearning Rate \\u03b1\\n0.001\\nGumbel-Softmax Temperature\\n[1.0]\\nTraining Dataset Size\\n200k\"}\n{\"score\":0.5611,\"pdf_id\":\"8df84cec-9c1e-5d52-8e50-ad0afd0a6066\",\"page_number\":9,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4f3c0037-a334-5682-8dbe-773d5ac64077\",\"text\":\"8\\n10\\nReward\\n(d) Training on O2, testing on O1.\\nFigure 4: Rewards for nonrobust training of GUMTREES. The horizontal line indicates the median.\\nA.2.1\\nRUNTIME RESULTS FOR EXPERIMENT IN THE MAIN PAPER.\\nTable 3: Extended average computation time per trial of different algorithms for experiment in the\\nmain paper. We report the mean \\u00b1 standard deviation across 30 trials. All units are in seconds.\\nTiming experiments was performed on 4 cores of a Intel(R) Xeon(R) CPU E5-2667 v4.\\nAlgorithm\\nTrain O1,\\ntest O1\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The top-ranked result from the vectorstore contains the specific information about the training hours for the agent trained with 128 CPU cores to beat the behavior-tree AI.\n[Action]:\nGenerateAnswer(answer=6.16)"}
{"role": "user", "content": "[Observation]: 6.16"}
