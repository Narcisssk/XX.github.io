{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: On the dataset proposed by this work, how much does the GPT-3.5-turbo model improve its GPT4score after using Graph-CoT?\n[Answer Format]: Your answer should be a single float number ranging from 0 to 100, representing the subtraction result.\n[Anchor PDF]: 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: The question is asking for the improvement in the GPT4score of the GPT-3.5-turbo model after using Graph-CoT on the dataset proposed by the specified paper. We need to find the relevant scores in the paper and compute the difference. First, let's retrieve the relevant sections or tables from the paper with the given `paper_id`.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_title, sections.section_content, tables.table_caption, tables.table_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id JOIN tables ON tables.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_title\":\"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs\",\"section_content\":\"Bowen $\\\\mathbf{Jin}^{1}$ , Chulin $\\\\mathbf{X}\\\\mathbf{i}\\\\mathbf{e}^{1}$ , Jiawei Zhang1, Kashob Kumar Roy1, Yu Zhang1   \\nZheng $\\\\mathbf{Li^{2}}$ , Ruirui $\\\\mathbf{L}\\\\mathbf{i}^{2}$ , Xianfeng $\\\\mathbf{Tang^{2}}$ , Suhang Wang3, Yu Meng4, Jiawei Han1\\n1University of Illinois at Urbana-Champaign, 2Amazon 3Pennsylvania State University, 4University of Virginia bowenj4@illinois.edu\",\"table_caption\":\"Table 5: Detailed Dataset Statistics of GRBENCH.\",\"table_content\":\"<html><body><table><tr><td rowspan=\\\"2\\\">Domain<\\/td><td rowspan=\\\"2\\\">Topic<\\/td><td colspan=\\\"2\\\">Graph Statistics<\\/td><td colspan=\\\"2\\\">Data<\\/td><\\/tr><tr><td>Nodes<\\/td><td>Edges<\\/td><td>#Templates#Questions<\\/td><td><\\/td><\\/tr><tr><td rowspan=\\\"8\\\">Academic<\\/td><td>CS<\\/td><td>Paper (~5M) Author (~2M) Venue (~55K)<\\/td><td>Written-by (~14M) Publish-in (~5M) Cited-by (~32M)<\\/td><td>15<\\/td><td>150<\\/td><\\/tr><tr><td>Biology<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~8M) publish-in (~1M) cited-by (~29M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Chemistry<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~7M) publish-in (~1M) cited-by (~20M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Material Science<\\/td><td>Paper (~1M) Author (~1M) Venue (99)<\\/td><td>written-by (~6M) publish-in (~1M) cited-by (~14M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Medicine<\\/td><td>Paper (~2M) Author (~4M) Venue (100)<\\/td><td>written-by (~14M) publish-in (~2M) cited-by (~12M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">Physics<\\/td><td>Paper (~1M)<\\/td><td>written-by (~13M) publish-in (~1M)<\\/td><td rowspan=\\\"2\\\">14<\\/td><td rowspan=\\\"2\\\">140<\\/td><\\/tr><tr><td>Author (~1M) Venue (91)<\\/td><td>cited-by (~18M)<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">E-commerce<\\/td><td rowspan=\\\"2\\\">Amazon<\\/td><td>Item (~9M)<\\/td><td>also-viewed (~125M) buy-after-viewing (~9M)<\\/td><td rowspan=\\\"2\\\">20<\\/td><td rowspan=\\\"2\\\">200<\\/td><\\/tr><tr><td>Brand (~110K)<\\/td><td>also-bought (~170M) bought-together (~6M) item-brand (~1M)<\\/td><\\/tr><tr><td>Literature<\\/td><td>Goodreads<\\/td><td>Book (~2M) Author (~829K) Publisher (~193K) Series (~400K)<\\/td><td>written-by (~3M) published-in (~1M) book-series (~822K) similar-book (~16M)<\\/td><td>24<\\/td><td>240<\\/td><\\/tr><tr><td>Healthcare<\\/td><td>Disease<\\/td><td>11 nodes types See Sec A<\\/td><td>24 edge types See Sec A<\\/td><td>27<\\/td><td>270<\\/td><\\/tr><tr><td>Legal<\\/td><td>Freelaw<\\/td><td>Opinion (~9M) Opinion-cluster (~8M) Docket (~66M) Court (~3K)<\\/td><td>opinion-cluster (~9M) opinion-citation (~29M) cluster-docket (~8M) docket-court (~66M)<\\/td><td>18<\\/td><td>180<\\/td><\\/tr><tr><td>SUM<\\/td><td><\\/td><td><\\/td><td><\\/td><td>174<\\/td><td>1740<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"section_title\":\"Abstract\",\"section_content\":\"Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts\\/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBENCH, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each GRAPH-COT iteration consists of three substeps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBENCH, where GRAPH-COT outperforms the baselines consistently. The code is available at https:\\/\\/github.com\\/PeterGriffinJin\\/ Graph-CoT.\",\"table_caption\":\"Table 5: Detailed Dataset Statistics of GRBENCH.\",\"table_content\":\"<html><body><table><tr><td rowspan=\\\"2\\\">Domain<\\/td><td rowspan=\\\"2\\\">Topic<\\/td><td colspan=\\\"2\\\">Graph Statistics<\\/td><td colspan=\\\"2\\\">Data<\\/td><\\/tr><tr><td>Nodes<\\/td><td>Edges<\\/td><td>#Templates#Questions<\\/td><td><\\/td><\\/tr><tr><td rowspan=\\\"8\\\">Academic<\\/td><td>CS<\\/td><td>Paper (~5M) Author (~2M) Venue (~55K)<\\/td><td>Written-by (~14M) Publish-in (~5M) Cited-by (~32M)<\\/td><td>15<\\/td><td>150<\\/td><\\/tr><tr><td>Biology<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~8M) publish-in (~1M) cited-by (~29M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Chemistry<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~7M) publish-in (~1M) cited-by (~20M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Material Science<\\/td><td>Paper (~1M) Author (~1M) Venue (99)<\\/td><td>written-by (~6M) publish-in (~1M) cited-by (~14M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Medicine<\\/td><td>Paper (~2M) Author (~4M) Venue (100)<\\/td><td>written-by (~14M) publish-in (~2M) cited-by (~12M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">Physics<\\/td><td>Paper (~1M)<\\/td><td>written-by (~13M) publish-in (~1M)<\\/td><td rowspan=\\\"2\\\">14<\\/td><td rowspan=\\\"2\\\">140<\\/td><\\/tr><tr><td>Author (~1M) Venue (91)<\\/td><td>cited-by (~18M)<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">E-commerce<\\/td><td rowspan=\\\"2\\\">Amazon<\\/td><td>Item (~9M)<\\/td><td>also-viewed (~125M) buy-after-viewing (~9M)<\\/td><td rowspan=\\\"2\\\">20<\\/td><td rowspan=\\\"2\\\">200<\\/td><\\/tr><tr><td>Brand (~110K)<\\/td><td>also-bought (~170M) bought-together (~6M) item-brand (~1M)<\\/td><\\/tr><tr><td>Literature<\\/td><td>Goodreads<\\/td><td>Book (~2M) Author (~829K) Publisher (~193K) Series (~400K)<\\/td><td>written-by (~3M) published-in (~1M) book-series (~822K) similar-book (~16M)<\\/td><td>24<\\/td><td>240<\\/td><\\/tr><tr><td>Healthcare<\\/td><td>Disease<\\/td><td>11 nodes types See Sec A<\\/td><td>24 edge types See Sec A<\\/td><td>27<\\/td><td>270<\\/td><\\/tr><tr><td>Legal<\\/td><td>Freelaw<\\/td><td>Opinion (~9M) Opinion-cluster (~8M) Docket (~66M) Court (~3K)<\\/td><td>opinion-cluster (~9M) opinion-citation (~29M) cluster-docket (~8M) docket-court (~66M)<\\/td><td>18<\\/td><td>180<\\/td><\\/tr><tr><td>SUM<\\/td><td><\\/td><td><\\/td><td><\\/td><td>174<\\/td><td>1740<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"section_title\":\"1 Introduction\",\"section_content\":\"Large language models (LLMs) (Touvron et al., 2023; Jiang et al., 2024) have demonstrated their exceptional language understanding and text generation capability in real-world scenarios (Zhao et al., 2023). However, LLMs suffer from hallucination problems and sometimes tend to generate content that appears plausible but is ungrounded (Tonmoy et al., 2024). This is because they memorize world knowledge parametrically and fail to refer to concrete knowledge sources (Zhang et al.,\\n2023b). To alleviate the hallucination issues, existing works propose to augment LLMs with external text corpora as knowledge sources (Shuster et al., 2021; Wu et al., 2023) and treat every single document as a knowledge unit. Retrieval augmentation (RAG) (Lewis et al., 2020; Gao et al., 2023) is then proposed to enable LLMs to interact with external knowledge sources, where relevant texts are retrieved and serve as contexts to improve the factuality of LLMs (shown in Figure 1 (a)). However, retrieval augmentation assumes that knowledge is well represented in individual text units and ignores the correlations among multiple text units.\\nIn real-world scenarios, text units are generally interconnected, forming a (text-attributed) graph. The knowledge of such graphs is refected not only in the form of texts but also in the structure of their connections. For example, academic papers in a bibliographic graph are linked by citation links (Wang et al., 2020). We can trace the source of a research direction (Bai et al., 2019) by traversing such a graph. Cases and opinions in a legal graph are interconnected by reference edges (Sadeghian et al., 2018). We can verify the judgment for a case by looking up its citations on such a graph (Chen et al., 2019).\\nAlthough widely used for text corpora as external knowledge sources, retrieval-augmentation cannot be readily used to augment LLMs with graphs for two reasons: 1) Structure Context: Retrieval augmentation can fnd individual nodes\\/texts from the graphs which can serve as context to augment the LLMs. However, knowledge on the graph also lies in the structure which can not be captured by single nodes\\/texts. 2) Graph Size Explosion: Although it is feasible to convert local subgraph structures into text descriptions as the input contexts to LLMs, the size of the local subgraph increases exponentially as the hop number increases, resulting in an excessively long context sequence. This could cause LLMs to be lost in the middle (Liu\\nFigure 1: Augmenting LLMs with external text corpora or external text-attributed graph.\\net al., 2023) given a plethora of irrelevant information in the context. In addition, the long sequence could potentially exceed the input length limitations of LLMs (Zhao et al., 2023).\\nTherefore, it is an important research topic to augment LLMs with such graph information. Unfortunately, there has been a lack of benchmark datasets to support the development of methodology and facilitate the evaluation of the proposed models. To this end, we frst construct a Graph Reasoning benchmark dataset called GRBENCH. GRBENCH includes ten real-world graphs that can serve as external knowledge sources for LLMs from fve domains including academic, ecommerce, literature, healthcare, and legal domains. Each sample in GRBENCH consists of a manually designed question and an answer, which can be directly answered by referring to the graphs or retrieving the information from the graphs as context. To make the dataset comprehensive, we include samples of different diffculty levels: easy questions (which can be answered with single-hop reasoning on graphs), medium questions (which necessitate multi-hop reasoning on graphs), and hard questions (which call for inductive reasoning with information on graphs as context).\\nWe propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT). The main idea is to enable LLMs to traverse the graph step-by-step to fgure out the key information needed, rather than directly feeding the whole subgraph as context into the LLMs (shown in Figure 1 (b)). GRAPH-COT is an iterative framework, where one iteration corresponds to one step on the graph. Each iteration in GRAPH-COT consists of three sub-steps: 1) Reasoning: LLMs propose what conclusion we can make with the current information and what further information is needed from the graph; 2) Interaction: LLMs generate the interactions needed to fetch information from the graph (e.g., fnding the nodes, checking the neighbors, etc); 3) Execution: The requests from the interaction step are executed on the graph and the corresponding information is returned. In this way, LLMs can conduct chain-based reasoning on the graph and fnd the key information on the graph. This process will be iterated until LLMs conclude the fnal answer in the reasoning sub-step.\\n\\nIn summary, our contributions are as follows: \\u2022 We propose the problem of augmenting LLMs with external graphs and introduce a comprehensive benchmark dataset called GRBENCH. \\u2022 We develop a straightforward and effective framework GRAPH-COT to encourage the LLMs to reason on the graph iteratively. \\u2022 We conduct extensive experiments on GRBENCH to demonstrate the effectiveness of GRAPH-COT and analyze its performance across different demonstration settings, backbone LLMs, and questions diffculties. Furthermore, we explore its failure cases with future directions outlined.\",\"table_caption\":\"Table 5: Detailed Dataset Statistics of GRBENCH.\",\"table_content\":\"<html><body><table><tr><td rowspan=\\\"2\\\">Domain<\\/td><td rowspan=\\\"2\\\">Topic<\\/td><td colspan=\\\"2\\\">Graph Statistics<\\/td><td colspan=\\\"2\\\">Data<\\/td><\\/tr><tr><td>Nodes<\\/td><td>Edges<\\/td><td>#Templates#Questions<\\/td><td><\\/td><\\/tr><tr><td rowspan=\\\"8\\\">Academic<\\/td><td>CS<\\/td><td>Paper (~5M) Author (~2M) Venue (~55K)<\\/td><td>Written-by (~14M) Publish-in (~5M) Cited-by (~32M)<\\/td><td>15<\\/td><td>150<\\/td><\\/tr><tr><td>Biology<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~8M) publish-in (~1M) cited-by (~29M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Chemistry<\\/td><td>Paper (~1M) Author (~2M) Venue (100)<\\/td><td>written-by (~7M) publish-in (~1M) cited-by (~20M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Material Science<\\/td><td>Paper (~1M) Author (~1M) Venue (99)<\\/td><td>written-by (~6M) publish-in (~1M) cited-by (~14M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td>Medicine<\\/td><td>Paper (~2M) Author (~4M) Venue (100)<\\/td><td>written-by (~14M) publish-in (~2M) cited-by (~12M)<\\/td><td>14<\\/td><td>140<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">Physics<\\/td><td>Paper (~1M)<\\/td><td>written-by (~13M) publish-in (~1M)<\\/td><td rowspan=\\\"2\\\">14<\\/td><td rowspan=\\\"2\\\">140<\\/td><\\/tr><tr><td>Author (~1M) Venue (91)<\\/td><td>cited-by (~18M)<\\/td><\\/tr><tr><td rowspan=\\\"2\\\">E-commerce<\\/td><td rowspan=\\\"2\\\">Amazon<\\/td><td>Item (~9M)<\\/td><td>also-viewed (~125M) buy-after-viewing (~9M)<\\/td><td rowspan=\\\"2\\\">20<\\/td><td rowspan=\\\"2\\\">200<\\/td><\\/tr><tr><td>Brand (~110K)<\\/td><td>also-bought (~170M) bought-together (~6M) item-brand (~1M)<\\/td><\\/tr><tr><td>Literature<\\/td><td>Goodreads<\\/td><td>Book (~2M) Author (~829K) Publisher (~193K) Series (~400K)<\\/td><td>written-by (~3M) published-in (~1M) book-series (~822K) similar-book (~16M)<\\/td><td>24<\\/td><td>240<\\/td><\\/tr><tr><td>Healthcare<\\/td><td>Disease<\\/td><td>11 nodes types See Sec A<\\/td><td>24 edge types See Sec A<\\/td><td>27<\\/td><td>270<\\/td><\\/tr><tr><td>Legal<\\/td><td>Freelaw<\\/td><td>Opinion (~9M) Opinion-cluster (~8M) Docket (~66M) Court (~3K)<\\/td><td>opinion-cluster (~9M) opinion-citation (~29M) cluster-docket (~8M) docket-court (~66M)<\\/td><td>18<\\/td><td>180<\\/td><\\/tr><tr><td>SUM<\\/td><td><\\/td><td><\\/td><td><\\/td><td>174<\\/td><td>1740<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n\n... # only display 3 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The sections and tables retrieved do not contain the specific scores for the GPT-3.5-turbo model. We need to perform a similarity search in the vectorstore to find the relevant context that mentions the GPT-3.5-turbo model and its performance on the GRBENCH dataset.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo performance on GRBENCH dataset using Graph-CoT', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7221,\"pdf_id\":\"02e5d154-bea5-572b-abd9-c8880583ee7e\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"20e4b2ff-2338-597e-8a85-2701ac9e9040\",\"text\":\"reaches up to 90.0% of GPT-3.5-turbo\\u2019s performance and 77.6% of GPT-4, illustrating the competi-\\ntiveness of open-source models in the creation and use of tools despite the model size gap.\\n6\"}\n{\"score\":0.7081,\"pdf_id\":\"b7c43a2c-11c4-5c63-8986-49097ff6e18d\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4b8d71c2-9243-5c21-a761-51b5bfc3ab94\",\"text\":\"27.0% with adjacency.\\nA.6\\nEXPERIMENTING WITH GPT-3.5-TURBO\\nIn this section, we conduct experiments using gpt-3.5-turbo (Brown et al., 2020a) and report the\\nresults in Table 10. This additional evaluation provides further insights into the performance of\\n18\"}\n{\"score\":0.7077,\"pdf_id\":\"8f68adfa-9710-5c56-804f-a79376f4e6c4\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"685a8231-0262-580a-9dcb-6278ec993171\",\"text\":\"of several models approaching zero. Consequently, we report only the performance of GPT-4 Turbo\\nusing least-to-most prompting on this dataset, employing DARG across varying levels of complexity.\\nAs illustrated in Figure 8, the performance of GPT-4 Turbo exhibits a decreasing trend as the number\\nof nodes in the input increases. Additionally, as the number of brackets in the label increases, GPT-4\\nTurbo\\u2019s performance also declines, dropping from 22.8 to 15.6. These results are consistent with\"}\n{\"score\":0.6962,\"pdf_id\":\"ab9d93a7-d639-5790-9ac0-b28eeafcb9d8\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"df4cf116-a586-57e8-89ca-55cfeba5f6af\",\"text\":\"abstract for the provided paper title. and test our methods using gpt-3.5-turbo. We observed a slight\\n19\"}\n{\"score\":0.6881,\"pdf_id\":\"38d860db-0353-54e7-9123-156425612d17\",\"page_number\":25,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"fff9da59-b4e7-5073-8c8f-7b5ea51c2872\",\"text\":\"Under review as a conference paper at ICLR 2024\\nTable 10: Variation of the accuracy of refined GPT-3.5-Turbo on BIG-Bench-F and PVR datasets when increasing\\nthe refinement steps. The step yielding the best result for each dataset and model is indicated in bold. Experiments\\nperformed with the latest version of GPT-3.5 (gpt-3.5-turbo-0613).\\nBIG-Bench-F\\nPVR\\n2 steps\\n4 steps\\n8 steps\\n2 steps\\n4 steps\\n8 steps\\nGPT-3.5-Turbo-code-filtering\\n0.320\\n0.352\\n0.380\\n0.200\\n0.200\\n0.208\\nGPT-3.5-Turbo-code-refinement\\n0.320\\n0.336\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks contain some relevant information, but none of them directly provide the GPT4score improvement for the GPT-3.5-turbo model after using Graph-CoT. We need to refine our query to specifically look for the improvement in performance.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo GPT4score improvement using Graph-CoT', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7332,\"pdf_id\":\"84cc7138-cde6-59b4-b672-0d654c6796d5\",\"page_number\":23,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"77aa5558-6e73-5767-94c9-b0747b57116a\",\"text\":\"GPT-4 calls while optimizing performance of weaker models like GPT-3.5-turbo, will cost an estimated total of\\n55 USD. Pairwise analysis of a 100 node graph using GPT-3.5-turbo on the other hand will lead to an estimated\\ncost of 10 USD. Therefore while our proposed triplet pipeline uses more calls, but is able to use GPT-4 more\\noptimally because of inbuilt error correction thus leading to cost optimization while improving performance\\nsignificantly over pairwise orientation using GPT-3.5-turbo.\\nI\"}\n{\"score\":0.7251,\"pdf_id\":\"8f68adfa-9710-5c56-804f-a79376f4e6c4\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"685a8231-0262-580a-9dcb-6278ec993171\",\"text\":\"of several models approaching zero. Consequently, we report only the performance of GPT-4 Turbo\\nusing least-to-most prompting on this dataset, employing DARG across varying levels of complexity.\\nAs illustrated in Figure 8, the performance of GPT-4 Turbo exhibits a decreasing trend as the number\\nof nodes in the input increases. Additionally, as the number of brackets in the label increases, GPT-4\\nTurbo\\u2019s performance also declines, dropping from 22.8 to 15.6. These results are consistent with\"}\n{\"score\":0.7074,\"pdf_id\":\"92a9ad28-3849-5a0e-a3ec-4dedd72d10eb\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"daa5c5a5-80f3-54a6-bc71-01af198b3325\",\"text\":\"GPT-3.5-TURBO are instruction-tuned models, we use the chat style prompt for them while using the\\ncode completion prompt from the CODELLAMA models. We elaborate further on our setup, models,\\nand prompts in Appendix E.\\nContamination. GPT-4 and GPT-3.5-TURBO have a cut-off date of 2021 and are therefore not\\ncontaminated on our benchmark since we curate our problems from repositories created after August\\n2022 (see Section 3.1). Given a problem instance I = {D, R, T } in our benchmark, we need to use\"}\n{\"score\":0.6979,\"pdf_id\":\"15c24ac9-8bd0-52e5-9a73-c97fb440404b\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f989cd6b-a778-5c0e-b6d5-6b2b758abe42\",\"text\":\"despite navigating the graph, the LLM fails to self-correct inaccuracies due to inherent reasoning\\nlimitations.\\nWe emphasize that the results of GraphSearch with GPT-4-turbo, tend to \\u201cmaintaining\\u201d cases. Under\\nsuch conditions, even a few failures can degrade overall performance, explaining why GraphSearch\\ndoes not consistently enhance performance for GPT-4-turbo.\\n37\"}\n{\"score\":0.6976,\"pdf_id\":\"2de84909-9563-57fb-acf9-3c3f953e498b\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4047a478-e6fb-5c2b-bc11-3d5459c01905\",\"text\":\"6After several rounds of evaluation with both GPT-4 and GPT-3.5-turbo without observing significant\\ndifferences, we chose GPT-3.5-turbo to minimize cost.\\n6\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. We need to refine our query further to focus on the specific performance metrics.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo GPT4score improvement on GRBENCH dataset using Graph-CoT', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6899,\"pdf_id\":\"8f68adfa-9710-5c56-804f-a79376f4e6c4\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"685a8231-0262-580a-9dcb-6278ec993171\",\"text\":\"of several models approaching zero. Consequently, we report only the performance of GPT-4 Turbo\\nusing least-to-most prompting on this dataset, employing DARG across varying levels of complexity.\\nAs illustrated in Figure 8, the performance of GPT-4 Turbo exhibits a decreasing trend as the number\\nof nodes in the input increases. Additionally, as the number of brackets in the label increases, GPT-4\\nTurbo\\u2019s performance also declines, dropping from 22.8 to 15.6. These results are consistent with\"}\n{\"score\":0.6898,\"pdf_id\":\"15c24ac9-8bd0-52e5-9a73-c97fb440404b\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f989cd6b-a778-5c0e-b6d5-6b2b758abe42\",\"text\":\"despite navigating the graph, the LLM fails to self-correct inaccuracies due to inherent reasoning\\nlimitations.\\nWe emphasize that the results of GraphSearch with GPT-4-turbo, tend to \\u201cmaintaining\\u201d cases. Under\\nsuch conditions, even a few failures can degrade overall performance, explaining why GraphSearch\\ndoes not consistently enhance performance for GPT-4-turbo.\\n37\"}\n{\"score\":0.6689,\"pdf_id\":\"8c548725-cab3-523e-8adf-10fbd65a600a\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2ca48efd-a15b-5f98-a4ee-ec55c9b6c6a6\",\"text\":\"GPT-3.5-turbo-1106 which represent three different versions of the GPT-3.5. For GPT-4 models, we\\nexperiment with GPT-4-turbo and GPT-4 (GPT-4-0613). For the claude-3 model, we evaluate the\\nsonnet and haiku versions. For each LLM, we first collect the code that is correctly generated for\\neach coding problem (i.e., they can pass all test cases provided by the dataset), then execute these\\ncorrect code and calculate the efficiency metrics (See Section 3.4).\\nPrompt:\"}\n{\"score\":0.6649,\"pdf_id\":\"15c24ac9-8bd0-52e5-9a73-c97fb440404b\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"9e074bf8-c06a-51c1-9dda-d0dcb6724567\",\"text\":\"Diagnosing GraphSearch with GPT-4-turbo: In our experiments, GraphSearch brings marginal or\\neven decreased performance for GPT-4-turbo across most experimental datasets, contradicting current\\nresearch [33, 32] which suggests that exhaustive search strategies can enhance the performance of\\nmore powerful LLMs. To provide a detailed analysis, we show three types of cases in Figure 11:\\nSuccessful cases, Failure cases, as well as Maintaining cases:\"}\n{\"score\":0.6593,\"pdf_id\":\"a147cd64-999d-5ce8-bac0-bf3a7badf94b\",\"page_number\":23,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"7088d88e-5341-5dae-a2bf-174b5d66570c\",\"text\":\"An unusual finding in InfiBench is that the performance of recent GPT-4o (API version: May 13,\\n2024) is slightly inferior to that of GPT-4 (API version: Jun 13, 2024). Indeed, as shown in Table 7,\\nwe benchmarked three models in the GPT-4 family, GPT-4 with a score of 70.64%, GPT-4-turbo\\nwith a score of 68.42%, and GPT-4o with a score of 66.19%. These are the top three models in our\\nleaderboard, and the score difference is small. We deem this as small fluctuations among different\\nmodel versions.\\nF.3\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. We need to try a more direct query to find the exact improvement value.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo GPT4score improvement Graph-CoT GRBENCH', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7024,\"pdf_id\":\"84cc7138-cde6-59b4-b672-0d654c6796d5\",\"page_number\":23,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"77aa5558-6e73-5767-94c9-b0747b57116a\",\"text\":\"GPT-4 calls while optimizing performance of weaker models like GPT-3.5-turbo, will cost an estimated total of\\n55 USD. Pairwise analysis of a 100 node graph using GPT-3.5-turbo on the other hand will lead to an estimated\\ncost of 10 USD. Therefore while our proposed triplet pipeline uses more calls, but is able to use GPT-4 more\\noptimally because of inbuilt error correction thus leading to cost optimization while improving performance\\nsignificantly over pairwise orientation using GPT-3.5-turbo.\\nI\"}\n{\"score\":0.7024,\"pdf_id\":\"8c548725-cab3-523e-8adf-10fbd65a600a\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"2ca48efd-a15b-5f98-a4ee-ec55c9b6c6a6\",\"text\":\"GPT-3.5-turbo-1106 which represent three different versions of the GPT-3.5. For GPT-4 models, we\\nexperiment with GPT-4-turbo and GPT-4 (GPT-4-0613). For the claude-3 model, we evaluate the\\nsonnet and haiku versions. For each LLM, we first collect the code that is correctly generated for\\neach coding problem (i.e., they can pass all test cases provided by the dataset), then execute these\\ncorrect code and calculate the efficiency metrics (See Section 3.4).\\nPrompt:\"}\n{\"score\":0.7001,\"pdf_id\":\"8f68adfa-9710-5c56-804f-a79376f4e6c4\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"685a8231-0262-580a-9dcb-6278ec993171\",\"text\":\"of several models approaching zero. Consequently, we report only the performance of GPT-4 Turbo\\nusing least-to-most prompting on this dataset, employing DARG across varying levels of complexity.\\nAs illustrated in Figure 8, the performance of GPT-4 Turbo exhibits a decreasing trend as the number\\nof nodes in the input increases. Additionally, as the number of brackets in the label increases, GPT-4\\nTurbo\\u2019s performance also declines, dropping from 22.8 to 15.6. These results are consistent with\"}\n{\"score\":0.6927,\"pdf_id\":\"b7c43a2c-11c4-5c63-8986-49097ff6e18d\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4b8d71c2-9243-5c21-a761-51b5bfc3ab94\",\"text\":\"27.0% with adjacency.\\nA.6\\nEXPERIMENTING WITH GPT-3.5-TURBO\\nIn this section, we conduct experiments using gpt-3.5-turbo (Brown et al., 2020a) and report the\\nresults in Table 10. This additional evaluation provides further insights into the performance of\\n18\"}\n{\"score\":0.6926,\"pdf_id\":\"ab9d93a7-d639-5790-9ac0-b28eeafcb9d8\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"df4cf116-a586-57e8-89ca-55cfeba5f6af\",\"text\":\"abstract for the provided paper title. and test our methods using gpt-3.5-turbo. We observed a slight\\n19\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. Let's try a more targeted SQL query to find the relevant scores in the paper.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND sections.section_title IN ('Experiments', 'Results', 'Discussion');\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the relevant sections might not be titled exactly as 'Experiments', 'Results', or 'Discussion'. Let's try a more general query to retrieve all sections and then filter the relevant content.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content, sections.section_title FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Bowen $\\\\mathbf{Jin}^{1}$ , Chulin $\\\\mathbf{X}\\\\mathbf{i}\\\\mathbf{e}^{1}$ , Jiawei Zhang1, Kashob Kumar Roy1, Yu Zhang1   \\nZheng $\\\\mathbf{Li^{2}}$ , Ruirui $\\\\mathbf{L}\\\\mathbf{i}^{2}$ , Xianfeng $\\\\mathbf{Tang^{2}}$ , Suhang Wang3, Yu Meng4, Jiawei Han1\\n1University of Illinois at Urbana-Champaign, 2Amazon 3Pennsylvania State University, 4University of Virginia bowenj4@illinois.edu\",\"section_title\":\"Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs\"}\n{\"section_content\":\"Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts\\/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBENCH, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each GRAPH-COT iteration consists of three substeps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBENCH, where GRAPH-COT outperforms the baselines consistently. The code is available at https:\\/\\/github.com\\/PeterGriffinJin\\/ Graph-CoT.\",\"section_title\":\"Abstract\"}\n{\"section_content\":\"Large language models (LLMs) (Touvron et al., 2023; Jiang et al., 2024) have demonstrated their exceptional language understanding and text generation capability in real-world scenarios (Zhao et al., 2023). However, LLMs suffer from hallucination problems and sometimes tend to generate content that appears plausible but is ungrounded (Tonmoy et al., 2024). This is because they memorize world knowledge parametrically and fail to refer to concrete knowledge sources (Zhang et al.,\\n2023b). To alleviate the hallucination issues, existing works propose to augment LLMs with external text corpora as knowledge sources (Shuster et al., 2021; Wu et al., 2023) and treat every single document as a knowledge unit. Retrieval augmentation (RAG) (Lewis et al., 2020; Gao et al., 2023) is then proposed to enable LLMs to interact with external knowledge sources, where relevant texts are retrieved and serve as contexts to improve the factuality of LLMs (shown in Figure 1 (a)). However, retrieval augmentation assumes that knowledge is well represented in individual text units and ignores the correlations among multiple text units.\\nIn real-world scenarios, text units are generally interconnected, forming a (text-attributed) graph. The knowledge of such graphs is refected not only in the form of texts but also in the structure of their connections. For example, academic papers in a bibliographic graph are linked by citation links (Wang et al., 2020). We can trace the source of a research direction (Bai et al., 2019) by traversing such a graph. Cases and opinions in a legal graph are interconnected by reference edges (Sadeghian et al., 2018). We can verify the judgment for a case by looking up its citations on such a graph (Chen et al., 2019).\\nAlthough widely used for text corpora as external knowledge sources, retrieval-augmentation cannot be readily used to augment LLMs with graphs for two reasons: 1) Structure Context: Retrieval augmentation can fnd individual nodes\\/texts from the graphs which can serve as context to augment the LLMs. However, knowledge on the graph also lies in the structure which can not be captured by single nodes\\/texts. 2) Graph Size Explosion: Although it is feasible to convert local subgraph structures into text descriptions as the input contexts to LLMs, the size of the local subgraph increases exponentially as the hop number increases, resulting in an excessively long context sequence. This could cause LLMs to be lost in the middle (Liu\\nFigure 1: Augmenting LLMs with external text corpora or external text-attributed graph.\\net al., 2023) given a plethora of irrelevant information in the context. In addition, the long sequence could potentially exceed the input length limitations of LLMs (Zhao et al., 2023).\\nTherefore, it is an important research topic to augment LLMs with such graph information. Unfortunately, there has been a lack of benchmark datasets to support the development of methodology and facilitate the evaluation of the proposed models. To this end, we frst construct a Graph Reasoning benchmark dataset called GRBENCH. GRBENCH includes ten real-world graphs that can serve as external knowledge sources for LLMs from fve domains including academic, ecommerce, literature, healthcare, and legal domains. Each sample in GRBENCH consists of a manually designed question and an answer, which can be directly answered by referring to the graphs or retrieving the information from the graphs as context. To make the dataset comprehensive, we include samples of different diffculty levels: easy questions (which can be answered with single-hop reasoning on graphs), medium questions (which necessitate multi-hop reasoning on graphs), and hard questions (which call for inductive reasoning with information on graphs as context).\\nWe propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT). The main idea is to enable LLMs to traverse the graph step-by-step to fgure out the key information needed, rather than directly feeding the whole subgraph as context into the LLMs (shown in Figure 1 (b)). GRAPH-COT is an iterative framework, where one iteration corresponds to one step on the graph. Each iteration in GRAPH-COT consists of three sub-steps: 1) Reasoning: LLMs propose what conclusion we can make with the current information and what further information is needed from the graph; 2) Interaction: LLMs generate the interactions needed to fetch information from the graph (e.g., fnding the nodes, checking the neighbors, etc); 3) Execution: The requests from the interaction step are executed on the graph and the corresponding information is returned. In this way, LLMs can conduct chain-based reasoning on the graph and fnd the key information on the graph. This process will be iterated until LLMs conclude the fnal answer in the reasoning sub-step.\\n\\nIn summary, our contributions are as follows: \\u2022 We propose the problem of augmenting LLMs with external graphs and introduce a comprehensive benchmark dataset called GRBENCH. \\u2022 We develop a straightforward and effective framework GRAPH-COT to encourage the LLMs to reason on the graph iteratively. \\u2022 We conduct extensive experiments on GRBENCH to demonstrate the effectiveness of GRAPH-COT and analyze its performance across different demonstration settings, backbone LLMs, and questions diffculties. Furthermore, we explore its failure cases with future directions outlined.\",\"section_title\":\"1 Introduction\"}\n{\"section_content\":\"Defnition 2.1. Graph. A graph can be denoted as $\\\\mathcal{G}=(\\\\mathcal{V},\\\\mathcal{E})$ , where $\\\\mathcal{V}$ and $\\\\mathcal{E}$ are node set and edge set, respectively. Each $v_{i}\\\\in\\\\mathcal{V}$ can be associated with some feature information $\\\\boldsymbol{\\\\chi}_{v_{i}}$ . For example, in an e-commerce item graph, $v\\\\in\\\\mathcal{V}$ are items, $e\\\\in\\\\mathcal{E}$ are co-purchase edges, and $\\\\scriptstyle{\\\\mathcal{X}}_{v}$ include features such as item title, description, price, and category. In this work, we formulate all the features as texts and the graph is also called a text-attributed graph. Defnition 2.2. Neighbors and Degree. The neighbors of a node $v_{i}$ refer to nodes which are linked to $v_{i}$ on the graph, denoted as $N(v_{i})=\\\\{v_{j}|e_{v_{i},v_{j}}\\\\in$ $\\\\mathcal{E}\\\\}$ . The degree of a node $v_{i}$ refers to the number of $v_{i}$ \\u2019s neighbors, denoted as $D(v_{i})=|N(v_{i})|$ .\",\"section_title\":\"2 Preliminaries\"}\n{\"section_content\":\"\",\"section_title\":\"3 GRBENCH Dataset\"}\n{\"section_content\":\"We create the GRBENCH dataset to evaluate how effectively LLMs can interact with domain-specifc graphs containing rich knowledge to solve the desired problem. GRBENCH contains 10 graphs from 5 general domains (academia, e-commerce, literature, healthcare, and legal). Each data sample in GRBENCH is a question-answer pair. The questions are designed to simulate the real-world use cases in specifc domains. However, it is hard for LLMs to answer those questions directly with their internal knowledge stored in model parameters;\\nthey need to interact with external domain-specifc graphs. The overall statistics of GRBENCH are in Table 1.\\nTo curate high-quality and diverse data without heavy human effort, the construction of GRBENCH contains four steps: 1) We frst collect large reference graph data from real-world scenarios which can serve as the context for data generation. 2) Then, we manually design question templates which can be answered on the reference graph data. 3) After that, we call GPT-4 to generate diverse question expressions for each question template. 4) Finally, we automatically generate ground truth answers from the domain-specifc graphs.\",\"section_title\":\"3.1 Dataset Overview\"}\n{\"section_content\":\"We collect data from fve domains where the knowledge lies in the format of graphs: academia, ecommerce, literature, healthcare, and legal. The detailed statistics of the graphs can be found in Appendix Table 5.\\nIn the academic domain, papers, authors, and venues are naturally interconnected by citation, \\u201cwritten-by\\u201d, and \\u201cpublish-in\\u201d relations. We obtain academic graphs across six disciplines including Biology, Computer Science, Chemistry, Material Science, Medicine, and Physics from DBLP1 (Tang et al., 2008) and Microsoft Academic Graph $({\\\\bf M A G})^{2}$ (Wang et al., 2020; Zhang et al., 2023a). Nodes on such graphs are papers, authors, and venues, while edges include citation edges, authorship edges, and venueship edges.\\nIn the e-commerce domain, a single product is assigned a brand, and different products are interlinked through \\u201calso-viewed\\u201d or \\u201calso-bought\\u201d relationships, which naturally embody graph-like structures. We use Amazon product datasets3 (He and McAuley, 2016), which provides the metadata information of items across a myriad of product categories. Nodes on this graph are items and brands, while edges include \\u201calso-viewed\\u201d, \\u201calso-bought\\u201d, \\u201cbuy-after-viewing\\u201d, \\u201cbought-together\\u201d, and \\u201citembrand\\u201d.\\n\\nIn the literature domain, the inherent graph structure exists with interconnections between books, authors, publishers, and series. The Goodreads dataset4 (Wan and McAuley, 2018) offers an extensive collection of books with their metadata. Nodes on this graph are books, authors, publishers, and series, while edges include \\u201cwrittenby\\u201d, \\u201cpublish-in\\u201d, \\u201cbook-series\\u201d and so on.\\nIn the healthcare domain, we can construct a graph by considering the diseases with their associated properties. We adopt the biological disease graph Hetionet5 (Himmelstein et al., 2017), which comprehensively summarizes existing disease and their symptoms, with the aim of repurposing drugs. Nodes on this graph include diseases, symptoms, side effects, compounds, and so on, while edges include \\u201cdisease-present-symptom\\u201d, \\u201ccompoundcause-side effect\\u201d and so on.\\nIn the legal domain, there are rich citation links between cases and opinions (since judges rely on citing opinions from previous cases to write for the current case) which naturally form a graph. We use the data from CourtListener6. Nodes on this graph are opinion, opinion-cluster, docket, and court, while edges include \\u201copinion-citation\\u201d, \\u201copinioncluster\\u201d, \\u201ccluster-docket\\u201d, and \\u201cdocket-court\\u201d.\",\"section_title\":\"3.2 Reference Graph Data\"}\n{\"section_content\":\"The question generation phase aims to generate questions that can be answered by LLMs after referring to the domain graphs. Considering that the generated questions should be accurate and meaningful, we ask four well-trained computer science Ph.D. students to write potential questions that can be answered given the graphs as context.\\nTo comprehensively evaluate the LLMs and their capability to interact with graphs, we ask the annotators to design question templates of three different diffculties:\\n\\u2022 Easy: These questions can be answered by looking up the feature\\/degree of only one node or travel on the graph within one hop. For example, \\u201cWhat is the price of the {item}?\\u201d or \\u201cWho are the authors of {paper}?\\u201d\\n\\n\\u2022 Medium: These questions require reasoning on the graphs for more than one hop and involve returning the feature\\/degree of nodes. For example, \\u201cWho is the closest collaborator with {author} in {year}?\\u201d. \\u2022 Hard: These questions cannot be directly answered by looking up the graph, but the graph can be useful by providing informative context. For example, \\u201cWhat is the complementary item given this {query}?\\u201d\\nIt is worth noting that the easy-level and mediumlevel questions can be answered from the given graph, while the ground truth for hard questions cannot be directly found in the graph. All the question templates can be found in Appendix B.\\nOnce the question templates are manually designed, we extract values from the graph to transform the templates into actual questions. For example, given the question template \\u201cHow many citations did {paper} have in {year}?\\u201d, we can refer to the academic graphs and sample \\u201cLanguage Models are Unsupervised Multitask Learners\\u201d as the \\u201cpaper\\u201d value and $^{\\\\bullet\\\\bullet}2021^{\\\\circ}$ as the \\u201cyear\\u201d value. This will result in a real question: \\u201cHow many citations did Language Models are Unsupervised Multitask Learners have in 2021?\\u201d\",\"section_title\":\"3.3 Manually Designed Question Templates\"}\n{\"section_content\":\"Following the previous steps, we obtain question samples for each graph. However, all samples pertaining to the same template will share the same expressions. For example, inquiring about the price of an item will consistently yield the question \\u201cWhat is the price of the {item}?\\u201d. This limits the diversity of the data samples and may lead to a partially comprehensive evaluation.\\nTo this end, we propose to use GPT-4 to paraphrase each question template into fve different expressions so that we can have more diverse question samples regarding the same type of question. The prompts for paraphrasing can be found in Appendix C.\",\"section_title\":\"3.4 Diverse Question Expression with GPT-4\"}\n{\"section_content\":\"The fnal step is to obtain the ground truth answer from the graph for each generated question. To achieve this goal, we frst implement graph functions (e.g. neighbor check, degree check), which can be utilized to reason on the graph. Then we implement function chains which can serve as a combination of graph functions in order to fetch the ground truth answer from the graph. The function chains are manually written by annotators for each type of question. Examples can be found in Appendix D.\",\"section_title\":\"3.5 Automatic Answer Generation\"}\n\n... # only display 10 rows in JSON format, more are truncated due to length constraint based on max_rows (10)"}
{"role": "assistant", "content": "[Thought]: The retrieved sections do not directly provide the GPT4score improvement for the GPT-3.5-turbo model after using Graph-CoT. We need to refine our SQL query to focus on the specific section that discusses the experimental results or performance metrics.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND sections.section_title LIKE '%Results%' OR sections.section_title LIKE '%Performance%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"This section presents the dialogue performance results for CDial-GPT and EVA2.0.\\nTable 7: Examples of posts labeled as biased.\\nTable 8: Dialogue performance of EVA2.0-base and its variations on gender bias.\"}\n{\"section_content\":\"This section presents the human evaluation results of dialogue performance for CDial-GPT and EVA2.0.\"}\n{\"section_content\":\"Performance Evaluation\\nWe compare the following tests: distance-based statistic dCor [39], the original HSIC QHSIC [14],\\nthe copula-based method RDC [26], the three variants of HSIC NyHSIC [44], FHSIC [44], BH-\\nSIC [44] and HSICAgg [32], NFSIC [18] as introduced in Sec. 1. Among them, dCor and QHSIC are\\nO(n2) tests. RDC is calculated in O(n log n) time and the rest are O(n) tests. A detailed description\\nof the comparing methods is given in the Appendix. For our methods, We provide two variants as\\nmentioned in Sec. 4.3. We name the Gaussian class case as LFHSIC-G, and name the Mahalanobis\\nclass case (and set \\u03a3 as a diagonal matrix) LFHSIC-M. Additionally, for the comparative meth-\\nods [30] that are relevant to us, due to their high time overhead and therefore inability to handle some\\nsettings of evaluation, we separately provide a comparison with our method under certain feasible\\nexperimental settings, the results are given in the Appendix.\\nExperimental setup. The significance level \\u03b1 is set to 0.05. We use Gaussian kernels for both X and\\nY in all kernel-based methods. And QHSIC, RDC, NyHSIC, FHSIC, BHSIC are all with the kernel\\nwidth being set to the Euclidean distance median of the samples. The number of random features D\\nfor FHSIC, LFHSIC-G\\/M, the number of induced variables for NyHSIC, the block size for BHSIC\\nas well as the number of sub-diagonals R for HSICAgg are all kept consistent as recommended\\nin [44, 32] for fair evaluation. Parameter settings for the rest of the methods follow the defaults in the\\ncode. More details of the setups are given in the Appendix.\\nEvaluation protocol. We evaluate on four synthetic datasets [18, 30] and two real datasets [44, 30].\\nSynthetic datasets consist of Sine Dependency (SD), Sinusoid (Sin), Gaussian Sign (GSign), and\\nindependent subspace analysis (ISA) dataset [14]. On real data, we introduce high-dimensional image\\ndata and another music dataset to evaluate the capability of all methods in different data scenarios.\\nUnless otherwise specified, we perform 100 repeated randomized experiments and report the average\\nresult of test power as default. More details of the generating process of each dataset and the details\\nof the evaluation (including running time) are provided in the Appendix.\\n6.1\\nResults on Synthetic Datasets\\nSettings of SD, Sin, and GSign Dataset. The Sin data corresponds to the example in Sec. 3 that\\nrequires the method to focus on differences in specific frequencies. In SD, Y is dependent solely on\\nthe first two dimensions of X. In contrast, in GSign, Y is independent of any proper subset of X\\n8\\nSD\\nSin\\nGSign\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\nType I error rate( \\u2193)\\nAverage Results\\n2000\\n2500\\n3000\\n3500\\n4000\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSine Dependency (SD)\\n1000\\n1500\\n2000\\n2500\\n3000\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSinusoid (Sin)\\n2000\\n3000\\n4000\\n5000\\n6000\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nGaussian Sign (GSign)\\nSD\\nSin\\nGSign\\nDatasets\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\nType I error rate( \\u2193)\\n2000\\n2500\\n3000\\n3500\\n4000\\nSample size n\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\n1000\\n1500\\n2000\\n2500\\n3000\\nSample size n\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\ndCor\\nQHSIC\\nRDC\\nNyHSIC\\nFHSIC\\nBHSIC\\nHSICAgg\\nNFSIC\\nLFHSIC-G\\nLFHSIC-M\\n2000\\n3000\\n4000\\n5000\\n6000\\nSample size n\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nFigure 2: Top: (D = 100). Below: (D = 500). Left: The average Type I error rate on SD, Sin, and\\nGSign datasets. The other three plots: The results of average test power on these three datasets.\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nAngle(\\u00d7\\u03c0\\/4)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSamp:10000, Dim:16, D:300\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nAngle(\\u00d7\\u03c0\\/4)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSamp:60000, Dim:32, D:300\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nAngle(\\u00d7\\u03c0\\/4)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSamp:10000, Dim:16, D:500\\nRDC\\nNyHSIC\\nBHSIC\\nFHSIC\\nHSICAgg\\nNFSIC\\nLFHSIC-G\\nLFHSIC-M\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\nAngle(\\u00d7\\u03c0\\/4)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSamp:60000, Dim:32, D:500\\nFigure 3: The average test power v.s. the rotation angle of each method on the ISA dataset.\\nbut dependent on X as a whole. Therefore, it requires the method to learn important local\\/global\\nfeatures based on the characteristics of the data to improve the test power. For SD and GSign, we\\nset the dimension of X as 4 and 5, respectively, and the dimension of Y is 1 for both. For Sin, we\\nset the frequency parameter \\u03c9 = 5. For calculating the Type I error rate, we evaluate using samples\\n(n = 2000) obtained by permutation for all three datasets.\\nPerformance. The results for D = 100 and D = 500 are shown in Fig. 2. Except for NFSIC and\\nBHSIC, all the other methods succeed in controlling the Type I error rate \\u22640.05. LFHSIC-M\\/G,\\nNFHSIC, and HSICAgg perform much better than other methods due to their ability to obtain more\\nappropriate kernels\\/features for testing. LFHSIC-G\\/M performs on both settings of D and has a\\nmore significant advantage over the others when D is small, implying the optimization objective can\\nstill be successfully optimized and the criterion is still powerful under high-speed requirements. In\\naddition, as the sample size increases the test power of LFHSIC-G\\/M is gradually converging to 1 in\\nboth settings, which corroborates the results of Theorem 3.\\nSettings of ISA Dataset (Large Scale). We set dimension (of both X, Y ) and sample size as\\nd = 16, n = 10000 and d = 32, n = 60000, then evaluate the average test power with angle\\nparameter \\u03b8 \\u2208[0, \\u03c0\\/4]. Note that a larger angle signifies stronger dependency. The quadratic-time\\nmethods are not involved in the evaluation due to their inability to handle large-scale settings. For\\nHSICAgg under the challenging setting n = 60000, d = 32, the memory space required for parallel\\nimplementation leads to memory overflow and hence the results are not given.\\nPerformance. The results for D = 300 and D = 500 are shown in Fig. 3. The results obtained\\nat \\u03b8 = 0 reflect the Type I error rate. All methods successfully control the Type I error rate \\u2264\\n0.05. LFHSIC-M stably outperforms other methods significantly as the angle increases. Method\\n(LFHSIC-G) that simply optimizes the global bandwidth performs worse as d increases, corroborating\\nthe need for more flexible kernel designs for more challenging tasks. Furthermore, comparing the\\n9\\nT\\nype I error\\nT\\nype II error\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage error rate ( \\u2193)\\n0.05\\nResults on 3Dshapes\\ndCor\\nQHSIC\\nRDC\\nNyHSIC\\nFHSIC\\nBHSIC\\nHSICAgg\\nNFSIC\\nLFHSIC-G\\nLFHSIC-M\\ndCor\\nQHSIC\\nRDC\\nNyHSIC\\nFHSIC\\nBHSIC\\nHSICAgg\\nNFSIC\\nLFHSIC-G\\nLFHSIC-M\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSample size: 500\\ndCor\\nQHSIC\\nRDC\\nNyHSIC\\nFHSIC\\nBHSIC\\nHSICAgg\\nNFSIC\\nLFHSIC-G\\nLFHSIC-M\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage test power( \\u2191)\\nSample size: 1000\\nFigure 4: The results on two real data. Left: 3DShapes. Right two: MSD Dataset.\\nresults under different settings of D, our method LFHSIC-M performs consistently well and exhibits\\nprogressively better performance as D increases.\\n6.2\\nResults on Real Data\\nSettings of Two Real Data. The first real dataset used is a high-dimensional image dataset 3Dshapes\\nas in [30]. In our experiments, we vectorize image X to a vector with dimension 64\\u00d764\\u00d73 = 12, 288.\\nThe sample size is set as 128. We add standard Gaussian noise N(0, 1) to the angle Y to make the\\nsetting more challenging. The Type I error rate is evaluated by the samples obtained by permutation.\\nBesides, we consider the Million Song Data (MSD) as the second real dataset. The first dimension\\nrepresents the year of release of each song and is referred to as variable Y . The remaining 90-\\ndimensional features (e.g., mean timbre and timbre covariance) constitute variable X. We follow\\nthe recommended setting [44], i.e., disturbing each entry of the X with an independent Gaussian\\nnoise N(0, 1000). For this dataset MSD, in order to fully utilize the data, we randomly select\\nn \\u2208{500, 1000} samples as the training set and other n samples from the remaining data 100 times\\nfor the evaluation and obtain the average result. The above training and testing processes are repeated\\n10 times to evaluate the robustness of the optimization scheme.\\nPerformance. The results of two real data with D = 10 are presented in Fig. 4. For the results on\\n3Dshapes (shown in the left of Fig. 4), all methods except BHSIC and NFSIC control the Type I\\nerror well. The linear-time test has relatively lower power compared to the quadratic-time test except\\nfor LFHSIC-M, proving that its more significant features obtained in high-dimensional scenarios\\nenable it to achieve outstanding performance even in scenarios with high approximation requirements\\n(D = 10). Similar conclusions can drawn from the MSD dataset (shown in the right of Fig. 4).\\nAdditionally, the results for NFSIC and LFHSIC-G\\/M with different sample sizes indicate increased\\nrobustness of the optimization as the sample size increases (reflected in the reduction of variance),\\nand the more flexible design also contributes to this (comparing LFHSIC-M and LFHSIC-G), thus\\ncan be more effectively applied to real-world scenarios.\\n7\"}\n{\"section_content\":\"Performance of Human and Machines\\nF.1\\nError Consistency of Humans and Machines\\nError consistency of Machine and Human\\nCPD\\nHuman\\nLocal Crop\\nGlobal\\nGlobal-local\\nFigure 13: The error consistency differs among decision-makers (machines\\/humans) in STDChallenge.\\nThis represents the degree of behavioral similarity between different decision-makers. For each\\nsequence in the STDChallenge-Turing Benchmark, we calculate the error consistency o each frame\\nfor different decision-makers, and then average these across all sequences to obtain the overall error\\nconsistency among the decision-makers.\\nAs shown in Fig. 13, CPDTrack exhibits a higher degree of consistency with human behavior\\ncompared to other machines, yet it remains closely aligned with machines. Notably, it demonstrates a\\nhigher level of consistency with trackers that utilize ViT as their backbone. This observation under-\\nscores the significant influence that backbone can have on algorithmic performance. Furthermore,\\nCPDTrack shares more similarities with trackers that implement a global strategy, under the same\\nnetwork architecture. We attribute this similarity to the adoption of strategies that more closely mirror\\nhuman cognitive processes.\\nAs shown in Fig. 13,ViT-based trackers display high similarity due to shared components like the\\nbackbone and motion model. Enhanced parameters, increased resolution, and more training data have\\nimproved their alignment with human error consistency. This is believed to result from the labels in\\nsupervised learning being annotated by humans, enhancing the algorithms\\u2019 ability to fit these labels.\\n24\\nAs shown in Fig. 13, compared to ViT-based trackers, those based on CNN exhibit lower similarity\\namong themselves. Nonetheless, certain algorithms that build on others still achieve high consistency\\nin error consistency.\\nAs shown in Fig. 13, the motion model plays a crucial role in this consistency, particularly affecting\\nhow trackers process and interpret video sequences, a difference that becomes particularly pronounced\\nin high-resolution settings.\\nF.2\\nEffect of different modules in pipeline on trackers\\n(a) Impact of the temporal module. \\n0-1:with\\/without temporal module\\n(b) Impact of different architecture. \\n0,1-2-3-4:customized-snn+cf-snn-one_stream\\n(c) Impact of the motion model. \\n0-1-2-3:local crop-local global-global-cpd\\n(d) Impact of Different contextual ratio\\n0-1-2-3-4-5:4(256)-4(384)-5-8-whole-cpd\\n(e) Impact of different parameters  \\n0-1-2-3-4-5:alexnet-resnet_18-resnet_50-\\nvit_base-vit_large\\n(f) Impact of different train data\\nFigure 14: Impact of different settings in tracker pipeline. We highlight the median in each setting.\\nIn (f), \\\"1\\\" means the training set contains GOT-10k, COCO, TrackingNet, and LaSOT, while \\\"2\\\"\\nmeans the training set contains GOT-10k, COCO, TrackingNet, LaSOT, and VideoCube, \\\"3\\\" means\\nthe training set contains GOT-10k, TrackingNet, LaSOT, and VideoCube.\\n(a) As shown in Fig. 14(a). Using temporal information is more effective and also more human-like.\\n(b) As shown in Fig. 14(b). Compared to other network architectures, one-stream is more effective\\nand also more human-like. However, since current one-stream trackers are all based on ViT, it is\\nunclear whether the effects are due to one-stream or ViT.\\n(c) As shown in Fig. 14(c). Undoubtedly, the CPD motion model has better effects and more\\nhuman-like behavior. At the same time, compared to the local-based motion model, the global one is\\nmore human-like, as it treats the entire image as central vision, improving performance at the cost of\\ncomputational efficiency.\\n25\\n(d) As shown in Fig. 14(d). Increasing the context ratio does not seem to change the performance\\nand error consistency, but after changing the resolution from 4(256) to 4(384), there has been an\\nimprovement. How to maintain computational efficiency while increasing resolution is a promising\\ndirection.\\n(e) As shown in Fig. 14(e). Obviously, increasing the number of parameters and switching to a newer\\nbackbone can significantly improve performance and error consistency. We believe this is due to the\\nincreased parameters enhancing the network\\u2019s ability to fit human labels. The error consistency of\\nvit_large has decreased, and we believe this might be because the network has learned some strategies\\nthat surpass human capabilities.\\n(f) As shown in Fig. 14(f). Increasing training data does not necessarily mean an improvement in\\nperformance, but it often enhances error consistency with humans. We believe this is because richer\\nhuman labels enhance the network\\u2019s learning capabilities.\\n26\\nG\"}\n{\"section_content\":\"Published as a conference paper at ICLR 2024\\nD\\nCORRELATION OF METRICS BY INPUTS OF DIFFERENT DIMENSIONS\\nThe correlation between these metrics and the validation accuracies of the networks is measured by\\nSpearman\\u2019s rank correlation coefficient. The full visualisation of results is shown in Fig. 15. The\\nmetric based on standard activation patterns drops dramatically when the input dimension increases.\\nThis aligns with the observation from Table 3. Both SWAP-Score and regularised SWAP-Score show\\na strong and consistent correlation with rising input dimensions. In particular, regularised SWAP-\\nScore outperforms the other two, regardless of the input size. When using the original dimension of\\nCIFAR-10, 32 \\u00d7 32, regularised SWAP-Score shows a strong correlation, 0.93.\\nFigure 15: Spearman\\u2019s coefficient between three metrics and the validation accuracies.\\nE\\nVISUALISATION OF CORRELATION BETWEEN SWAP-SCORES AND\\nNETWORKS\\u2019 GROUND-TRUTH PERFORMANCE\\nFigures 16 and 17 demonstrate the correlation between the SWAP-Score\\/Regularised SWAP-Score\\nand the ground-truth performance of networks across various search spaces and tasks. In these\\nfigures, each dot represents a distinct neural network. The visualisations effectively demonstrate a\\nstrong correlation between the SWAP-Score and the ground-truth performance across the majority of\\nsearch spaces and tasks. Furthermore, the application of the regularisation function results in a more\\nconcentrated distribution of dots, indicating an enhanced correlation with the networks\\u2019 ground-truth\\nperformance.\\n(a)\\n(b)\\n(c)\\n23\\nPublished as a conference paper at ICLR 2024\\n(d)\\n(e)\\n(f)\\n(g)\\n(h)\\n(i)\\n(j)\\n(k)\\n(l)\\n(m)\\nFigure 16: Visualisation of correlation between SWAP-Score and networks\\u2019 ground-truth perfor-\\nmance, for each search space and task combination. Colors indicate the search spaces.\\n24\\nPublished as a conference paper at ICLR 2024\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\n(f)\\n(g)\\n(h)\\n(i)\\n(j)\\n(k)\\n(l)\\n(m)\\nFigure 17: Visualisation of correlation between regularised SWAP-Score and networks\\u2019 ground-\\ntruth performance, for each search space and task combination. Colors indicate the search spaces.\\n25\\nPublished as a conference paper at ICLR 2024\\nF\"}\n{\"section_content\":\"To asses more realistic performance of our system, we study its performance on FAVIQ-A (Park et al., 2022). We use the silver passage supervision from Asai et al. (2022), and feed the model with top20 passages retrieved via DPR system (Karpukhin et al., 2020). We keep all the hyperparameters same as for FEVER, and use dev set only for earlystopping. We compare to evidentiality-guided generator (EGG), a t5-based Fusion-in-Decoder (FiD) (Izacard and Grave, 2021) with two decoders from Asai et al. (2022). We use hyperparameters from FEVER. The results are shown in Table 6. Our DebertaV3-based Claim-Dissector reaches state-ofthe-art results on the dataset. The domain mismatch (measured by difference $\\\\Delta$ ) between development and test set is likely caused by the domain shift of NaturalQuestions test set, from which FAVIQ\\u2019s test set was created (see Appendix B in Min et al. (2020)). However, despite our best efforts, we have not uncovered the cause of massive degradation between dev and test set for roberta-base based Claim-Dissector (the standard deviation on test set was only $\\\\pm0.4$ accuracy points).\"}\n\n... # only display 6 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The retrieved sections still do not provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. Let's try a more targeted SQL query to find the specific performance metrics in the sections.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND sections.section_content LIKE '%GPT-3.5-turbo%' AND sections.section_content LIKE '%GPT4score%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Baselines. We compare our proposed GRAPHCOT with three types of baseline methods: standard LLMs (Base LLMs), text retrieval-augmented LLMs (Text RAG LLMs), and graph retrievalaugmented LLMs (Graph RAG LLMs):\\n\\u2022 Base LLMs: We test if the LLMs can answer the given question with their knowledge without interacting with external data. We adopt the standard prompting, which involves providing simple instructions and letting LLMs generate an answer for the question.\\n\\u2022 Text RAG LLMs (Gao et al., 2023): We treat the external graphs as pure text corpora and utilize a retriever to retrieve relevant text information from them. Subsequently, the retrieved text serves as context to augment the LLM for question answering.\\n\\u2022 Graph RAG LLMs: This is an extension of text RAG, where not only the retrieved text\\/node but also the subgraph associated with it is linearized into a text sequence (Ye et al., 2023) and serves as the context. In the main result, we use 1-hop ego-graphs.\\nFor all categories of baselines, we explore three LLM backbones, including LLaMA-2-13b-chat (Touvron et al., 2023), Mixtral-8x7b-Instruct (Jiang et al., 2024), and GPT-3.5-turbo (Ouyang et al., 2022). R-L\\nEvaluation Metrics. We use both rule-based metrics and model-based metrics to comprehensively evaluate the model results. For the former, we use Rouge-L(R-L), which measures the longest common subsequence of words between the responses and the ground truth answers. For the latter, we call GPT-4 to measure if the model output and ground truth are the same. We calculate the percentage of \\u201ccorrect\\u201d predicted by GPT-4 as GPT4score.\\nImplementation Settings. All experiments are conducted on NVIDIA GeForce RTX A6000 GPUs with Python 3.8 and Huggingface 4.36.2. We use Mpnet-v2 7 as the retriever for all the baselines and our method and implement the indexing with FAISS (Johnson et al., 2019). In GRAPH-COT, we adopt GPT-3.5-turbo-16k (Jan 2024) as the backbone LLM in the main results and set the temperature $t$ to 0 for consistent responses. We provide demonstrations for GRAPH-COT on how to conduct reasoning in Appendix E.\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved section provides the evaluation metrics used, including GPT4score, but it does not directly state the improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. We need to refine the query to focus on the specific results or tables that might contain this information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND sections.section_content LIKE '%Table%' AND sections.section_content LIKE '%GPT-3.5-turbo%' AND sections.section_content LIKE '%GPT4score%';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the specific table or section containing the GPT4score improvement for the GPT-3.5-turbo model might not be directly mentioned in the sections. Let's try a more general query to retrieve all sections that discuss the performance of the GPT-3.5-turbo model and then manually filter the relevant content.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT sections.section_content FROM sections JOIN metadata ON sections.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND sections.section_content LIKE '%GPT-3.5-turbo%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_content\":\"Baselines. We compare our proposed GRAPHCOT with three types of baseline methods: standard LLMs (Base LLMs), text retrieval-augmented LLMs (Text RAG LLMs), and graph retrievalaugmented LLMs (Graph RAG LLMs):\\n\\u2022 Base LLMs: We test if the LLMs can answer the given question with their knowledge without interacting with external data. We adopt the standard prompting, which involves providing simple instructions and letting LLMs generate an answer for the question.\\n\\u2022 Text RAG LLMs (Gao et al., 2023): We treat the external graphs as pure text corpora and utilize a retriever to retrieve relevant text information from them. Subsequently, the retrieved text serves as context to augment the LLM for question answering.\\n\\u2022 Graph RAG LLMs: This is an extension of text RAG, where not only the retrieved text\\/node but also the subgraph associated with it is linearized into a text sequence (Ye et al., 2023) and serves as the context. In the main result, we use 1-hop ego-graphs.\\nFor all categories of baselines, we explore three LLM backbones, including LLaMA-2-13b-chat (Touvron et al., 2023), Mixtral-8x7b-Instruct (Jiang et al., 2024), and GPT-3.5-turbo (Ouyang et al., 2022). R-L\\nEvaluation Metrics. We use both rule-based metrics and model-based metrics to comprehensively evaluate the model results. For the former, we use Rouge-L(R-L), which measures the longest common subsequence of words between the responses and the ground truth answers. For the latter, we call GPT-4 to measure if the model output and ground truth are the same. We calculate the percentage of \\u201ccorrect\\u201d predicted by GPT-4 as GPT4score.\\nImplementation Settings. All experiments are conducted on NVIDIA GeForce RTX A6000 GPUs with Python 3.8 and Huggingface 4.36.2. We use Mpnet-v2 7 as the retriever for all the baselines and our method and implement the indexing with FAISS (Johnson et al., 2019). In GRAPH-COT, we adopt GPT-3.5-turbo-16k (Jan 2024) as the backbone LLM in the main results and set the temperature $t$ to 0 for consistent responses. We provide demonstrations for GRAPH-COT on how to conduct reasoning in Appendix E.\"}\n{\"section_content\":\"How Important are the Demonstrations for GRAPH-COT? To answer this question, we conduct experiments from two aspects: zero-shot study (no demonstrations) and cross-domain study (demonstrations from other domains (Ding et al., 2018)). The results are shown in Figure 3, where the columns and rows correspond to the source domain and target domain respectively. For the zero-shot study, no demonstrations are given (rightest column in Figure 3). We empirically fnd that given no reasoning demonstrations, GRAPH-COT cannot work in all the datasets (nearly 0 performance). This implies that the LLMs suffer if given insuffcient instructions (only graph defnition and interaction function defnitions). For the crossdomain study, we provide demonstrations from the source domain graphs and test on the target domain graphs. From the result (left fve columns in Figure 3), in-domain demonstrations (diagonal) perform quite well and GRAPH-COT is overall robust to demonstration domain-shift. This observation underscores the adaptability and effectiveness of GRAPH-COT in capturing the key steps of graph chain-reasoning through in-context learning, despite the diverse demonstration domains.\\n\\nHow Different LLMs Perform in GRAPH-COT? In the main results, we adopt GPT-3.5-turbo as the LLM backbone for GRAPH-COT. In this section, we explore GRAPH-COT with other LLM backbones including LLaMA-2-13b-chat, Mixtral-8x7bInstruct, GPT-3.5-turbo, and GPT-4. We randomly extract a subset from GRBENCH (one sample for each question template) to experiment and the results are shown in Table 3. From the result, we fnd that the LLM backbone matters. An LLM with more advanced instruction following ability and reasoning ability (i.e., GPT-4) can contribute to better performance in GRAPH-COT.\"}\n\nIn total, 2 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections mention GPT-3.5-turbo and GPT4score, but do not provide the specific improvement in GPT4score after using Graph-CoT. Let's try a similarity search in the vectorstore to find more detailed information.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo GPT4score improvement after using Graph-CoT', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7077,\"pdf_id\":\"84cc7138-cde6-59b4-b672-0d654c6796d5\",\"page_number\":23,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"77aa5558-6e73-5767-94c9-b0747b57116a\",\"text\":\"GPT-4 calls while optimizing performance of weaker models like GPT-3.5-turbo, will cost an estimated total of\\n55 USD. Pairwise analysis of a 100 node graph using GPT-3.5-turbo on the other hand will lead to an estimated\\ncost of 10 USD. Therefore while our proposed triplet pipeline uses more calls, but is able to use GPT-4 more\\noptimally because of inbuilt error correction thus leading to cost optimization while improving performance\\nsignificantly over pairwise orientation using GPT-3.5-turbo.\\nI\"}\n{\"score\":0.6806,\"pdf_id\":\"15c24ac9-8bd0-52e5-9a73-c97fb440404b\",\"page_number\":37,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"f989cd6b-a778-5c0e-b6d5-6b2b758abe42\",\"text\":\"despite navigating the graph, the LLM fails to self-correct inaccuracies due to inherent reasoning\\nlimitations.\\nWe emphasize that the results of GraphSearch with GPT-4-turbo, tend to \\u201cmaintaining\\u201d cases. Under\\nsuch conditions, even a few failures can degrade overall performance, explaining why GraphSearch\\ndoes not consistently enhance performance for GPT-4-turbo.\\n37\"}\n{\"score\":0.6775,\"pdf_id\":\"92a9ad28-3849-5a0e-a3ec-4dedd72d10eb\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"daa5c5a5-80f3-54a6-bc71-01af198b3325\",\"text\":\"GPT-3.5-TURBO are instruction-tuned models, we use the chat style prompt for them while using the\\ncode completion prompt from the CODELLAMA models. We elaborate further on our setup, models,\\nand prompts in Appendix E.\\nContamination. GPT-4 and GPT-3.5-TURBO have a cut-off date of 2021 and are therefore not\\ncontaminated on our benchmark since we curate our problems from repositories created after August\\n2022 (see Section 3.1). Given a problem instance I = {D, R, T } in our benchmark, we need to use\"}\n{\"score\":0.6711,\"pdf_id\":\"b7c43a2c-11c4-5c63-8986-49097ff6e18d\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4b8d71c2-9243-5c21-a761-51b5bfc3ab94\",\"text\":\"27.0% with adjacency.\\nA.6\\nEXPERIMENTING WITH GPT-3.5-TURBO\\nIn this section, we conduct experiments using gpt-3.5-turbo (Brown et al., 2020a) and report the\\nresults in Table 10. This additional evaluation provides further insights into the performance of\\n18\"}\n{\"score\":0.6571,\"pdf_id\":\"47285410-a774-5685-a6de-cb8c5ded7285\",\"page_number\":20,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"6d06a864-a705-578b-a5f0-832a2a5bdd57\",\"text\":\"evidenced by Fig. 13d, we see that CoT is much more effective for GPT-4 output prediction\\n20\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks do not directly provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. Let's try a more targeted similarity search focusing on the specific performance metrics and the GPT4score improvement.\n[Action]:\nClassicRetrieve(query='GPT-3.5-turbo GPT4score improvement Graph-CoT', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.7212,\"pdf_id\":\"84cc7138-cde6-59b4-b672-0d654c6796d5\",\"page_number\":23,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"77aa5558-6e73-5767-94c9-b0747b57116a\",\"text\":\"GPT-4 calls while optimizing performance of weaker models like GPT-3.5-turbo, will cost an estimated total of\\n55 USD. Pairwise analysis of a 100 node graph using GPT-3.5-turbo on the other hand will lead to an estimated\\ncost of 10 USD. Therefore while our proposed triplet pipeline uses more calls, but is able to use GPT-4 more\\noptimally because of inbuilt error correction thus leading to cost optimization while improving performance\\nsignificantly over pairwise orientation using GPT-3.5-turbo.\\nI\"}\n{\"score\":0.7148,\"pdf_id\":\"b7c43a2c-11c4-5c63-8986-49097ff6e18d\",\"page_number\":18,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"4b8d71c2-9243-5c21-a761-51b5bfc3ab94\",\"text\":\"27.0% with adjacency.\\nA.6\\nEXPERIMENTING WITH GPT-3.5-TURBO\\nIn this section, we conduct experiments using gpt-3.5-turbo (Brown et al., 2020a) and report the\\nresults in Table 10. This additional evaluation provides further insights into the performance of\\n18\"}\n{\"score\":0.7031,\"pdf_id\":\"92a9ad28-3849-5a0e-a3ec-4dedd72d10eb\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"daa5c5a5-80f3-54a6-bc71-01af198b3325\",\"text\":\"GPT-3.5-TURBO are instruction-tuned models, we use the chat style prompt for them while using the\\ncode completion prompt from the CODELLAMA models. We elaborate further on our setup, models,\\nand prompts in Appendix E.\\nContamination. GPT-4 and GPT-3.5-TURBO have a cut-off date of 2021 and are therefore not\\ncontaminated on our benchmark since we curate our problems from repositories created after August\\n2022 (see Section 3.1). Given a problem instance I = {D, R, T } in our benchmark, we need to use\"}\n{\"score\":0.7023,\"pdf_id\":\"8f68adfa-9710-5c56-804f-a79376f4e6c4\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"685a8231-0262-580a-9dcb-6278ec993171\",\"text\":\"of several models approaching zero. Consequently, we report only the performance of GPT-4 Turbo\\nusing least-to-most prompting on this dataset, employing DARG across varying levels of complexity.\\nAs illustrated in Figure 8, the performance of GPT-4 Turbo exhibits a decreasing trend as the number\\nof nodes in the input increases. Additionally, as the number of brackets in the label increases, GPT-4\\nTurbo\\u2019s performance also declines, dropping from 22.8 to 15.6. These results are consistent with\"}\n{\"score\":0.6908,\"pdf_id\":\"ab9d93a7-d639-5790-9ac0-b28eeafcb9d8\",\"page_number\":19,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"df4cf116-a586-57e8-89ca-55cfeba5f6af\",\"text\":\"abstract for the provided paper title. and test our methods using gpt-3.5-turbo. We observed a slight\\n19\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks still do not provide the specific improvement in GPT4score for the GPT-3.5-turbo model after using Graph-CoT. Let's try a more targeted SQL query to find the specific results in the tables.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT tables.table_content FROM tables JOIN metadata ON tables.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = 'fb154467-1ce1-5c1f-9d4f-b4f5c76312ee' AND tables.table_content LIKE '%GPT-3.5-turbo%' AND tables.table_content LIKE '%GPT4score%';\")"}
{"role": "user", "content": "[Observation]:\n{\"table_content\":\"<html><body><table><tr><td rowspan=\\\"2\\\"><\\/td><td rowspan=\\\"2\\\">Model<\\/td><td colspan=\\\"2\\\">Academic<\\/td><td colspan=\\\"2\\\">E-commerce<\\/td><td colspan=\\\"2\\\">Literature<\\/td><td colspan=\\\"2\\\">Healthcare<\\/td><td colspan=\\\"2\\\">Legal<\\/td><\\/tr><tr><td>R-L<\\/td><td>GPT4score<\\/td><td>R-L<\\/td><td>GPT4score<\\/td><td>R-L<\\/td><td>GPT4score<\\/td><td>R-L<\\/td><td>GPT4score<\\/td><td>R-L<\\/td><td>GPT4score<\\/td><\\/tr><tr><td rowspan=\\\"3\\\">Base B<\\/td><td>LLaMA-2-13b-chat<\\/td><td>8.13<\\/td><td>8.03<\\/td><td>7.01<\\/td><td>12.00<\\/td><td>5.32<\\/td><td>20.83<\\/td><td>5.25<\\/td><td>13.70<\\/td><td>15.97<\\/td><td>16.11<\\/td><\\/tr><tr><td>Mixtral-8x7b<\\/td><td>9.02<\\/td><td>8.14<\\/td><td>12.54<\\/td><td>18.00<\\/td><td>7.50<\\/td><td>22.50<\\/td><td>3.88<\\/td><td>20.00<\\/td><td>12.74<\\/td><td>16.11<\\/td><\\/tr><tr><td>GPT-3.5-turbo<\\/td><td>6.05<\\/td><td>12.80<\\/td><td>9.18<\\/td><td>23.50<\\/td><td>10.43<\\/td><td>26.67<\\/td><td>5.83<\\/td><td>14.44<\\/td><td>10.51<\\/td><td>20.00<\\/td><\\/tr><tr><td rowspan=\\\"3\\\">Text RA<\\/td><td>LLaMA-2-13b-chat<\\/td><td>8.69<\\/td><td>8.52<\\/td><td>9.23<\\/td><td>12.50<\\/td><td>7.61<\\/td><td>20.00<\\/td><td>1.44<\\/td><td>5.93<\\/td><td>15.37<\\/td><td>16.67<\\/td><\\/tr><tr><td>Mixtral-8x7b<\\/td><td>8.44<\\/td><td>8.02<\\/td><td>23.14<\\/td><td>29.50<\\/td><td>13.35<\\/td><td>27.92<\\/td><td>3.22<\\/td><td>16.67<\\/td><td>19.69<\\/td><td>25.00<\\/td><\\/tr><tr><td>GPT-3.5-turbo<\\/td><td>5.83<\\/td><td>9.91<\\/td><td>14.06<\\/td><td>20.00<\\/td><td>10.04<\\/td><td>20.83<\\/td><td>4.57<\\/td><td>8.52<\\/td><td>18.14<\\/td><td>23.89<\\/td><\\/tr><tr><td rowspan=\\\"3\\\">RAG Grap<\\/td><td>LLaMA-2-13b<\\/td><td>22.01<\\/td><td>22.97<\\/td><td>12.48<\\/td><td>20.00<\\/td><td>9.25<\\/td><td>20.00<\\/td><td>2.97<\\/td><td>4.81<\\/td><td>17.98<\\/td><td>17.22<\\/td><\\/tr><tr><td>Mixtral-8x7b<\\/td><td>27.77<\\/td><td>31.20<\\/td><td>32.87<\\/td><td>37.00<\\/td><td>20.08<\\/td><td>33.33<\\/td><td>8.66<\\/td><td>15.19<\\/td><td>23.48<\\/td><td>25.56<\\/td><\\/tr><tr><td>GPT-3.5-turbo<\\/td><td>18.45<\\/td><td>26.98<\\/td><td>17.52<\\/td><td>28.00<\\/td><td>14.94<\\/td><td>24.17<\\/td><td>8.69<\\/td><td>14.07<\\/td><td>18.66<\\/td><td>22.22<\\/td><\\/tr><tr><td colspan=\\\"2\\\">GRAPH-COT<\\/td><td>31.89<\\/td><td>33.48<\\/td><td>42.40<\\/td><td>44.50<\\/td><td>41.59<\\/td><td>46.25<\\/td><td>22.33<\\/td><td>28.89<\\/td><td>30.52<\\/td><td>28.33<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"table_content\":\"<html><body><table><tr><td>Model GPT4score<\\/td><\\/tr><tr><td>GRAPH-COT<\\/td><\\/tr><tr><td>w.LLaMA-2-13b-chat 16.04<\\/td><\\/tr><tr><td>w.Mixtral-8x7b 36.46<\\/td><\\/tr><tr><td>w.GPT-3.5-turbo 36.63<\\/td><\\/tr><tr><td>w. GPT-4 46.28<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n{\"table_content\":\"<html><body><table><tr><td>Model<\\/td><td>GPT4score<\\/td><\\/tr><tr><td>GPT-3.5-turbo<\\/td><td>19.48<\\/td><\\/tr><tr><td>+noderetrieval<\\/td><td>16.63<\\/td><\\/tr><tr><td>+ 1-hop subgraph retrieval<\\/td><td>23.09<\\/td><\\/tr><tr><td>+ 2-hop subgraph retrieval<\\/td><td>22.12<\\/td><\\/tr><tr><td>+ GRAPH-COT<\\/td><td>36.29<\\/td><\\/tr><\\/table><\\/body><\\/html>\"}\n\nIn total, 3 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved tables contain the GPT4score values for the GPT-3.5-turbo model before and after using Graph-CoT. We can extract the relevant scores and compute the improvement.\n\nFrom the first table:\n- GPT-3.5-turbo (Base) GPT4score: 12.80 (Academic), 23.50 (E-commerce), 26.67 (Literature), 14.44 (Healthcare), 20.00 (Legal)\n- GPT-3.5-turbo (Graph-CoT) GPT4score: 26.98 (Academic), 28.00 (E-commerce), 24.17 (Literature), 14.07 (Healthcare), 22.22 (Legal)\n\nFrom the third table:\n- GPT-3.5-turbo GPT4score: 19.48\n- GPT-3.5-turbo + GRAPH-COT GPT4score: 36.29\n\nWe will compute the improvement using the third table's values.\n[Action]:\nGenerateAnswer(answer=16.81)"}
{"role": "user", "content": "[Observation]: 16.81"}
