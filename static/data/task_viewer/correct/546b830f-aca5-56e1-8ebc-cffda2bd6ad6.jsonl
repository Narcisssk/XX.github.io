{"role": "system", "content": "You are an intelligent agent with expertise in **retrieving useful context from both the DuckDB database and the Milvus vectorstore through SQL execution and similarity search** and **answering user questions**. You will be given a natural language question concerning PDF files, along with the schema of both the database and the vectorstore. Your ultimate goal is to answer the input question with pre-defined answer format. The DuckDB database contains all parsed content of raw PDF files, while the Milvus vectorstore encodes specific column cells from the database as vectors. You can predict executable actions, interact with the hybrid environment (including database and vectorstore) across multiple turns, and retrieve necessary context until you are confident in resolving the question.\n\n--------\n\n## Task Description\nEach task consists of the following parts:\n\n[Question]: User question regarding the PDF, e.g., Is there any ...?\n[Answer Format]: it describes the format of the final answer, e.g., the answer is \"Yes\" or \"No\" without punctuation.\n[Anchor PDF]: Optional, a python str or list, the PDF(s) you must use to answer the question.\n[Reference PDF]: Optional, a python str or list, the PDF(s) you may use to answer the question.\n[Conference]: Optional, a python str or list, the conference(s) and year(s) of the relevant papers for retrieval.\n[Database Schema]: A detailed serialized schema of the DuckDB database for reference when generating SQL queries. It includes 1) tables, 2) columns and their data types, 3) descriptions for these schema items, and 4) primary key and foreign key constraints.\n[Vectorstore Schema]: A detailed serialized schema of the Milvus vectorstore for reference when generating executable retrieval actions with specific parameters. It includes 1) collections, 2) fields, 3) encodable (table, column) pairs in the relational database where the vectorized content originates, and 4) grammar for valid filter rules.\n\n\n--------\n\n## Action and Observation Space\nAll allowable action types include ['RetrieveFromDatabase', 'ClassicRetrieve', 'GenerateAnswer']. Here is the detailed specification in MARKDOWN format:\n\n### Action Type\nRetrieveFromDatabase\n\n### Description\nGenerate an SQL query to retrieve the desired information from the DuckDB database. Please refer to the concrete database schema to produce a valid and executable SQL.\n\n### Observation\nThe observation space is the execution result of the SQL query. You do not need to worry about the actual execution, we will perform it for you. If the SQL failed to execute, we will return the error message. Extremely long SQL output will be truncated.\n\n### Syntax and Parameters (MARKDOWN Format)\nRetrieveFromDatabase(sql: str)\n- sql: str, required. The concrete DuckDB SQL query to execute and retrieve results.\n\n### Use Cases\n\n#### Case 1\nGet the abstract of the paper with paper_id '12345678' from the metadata table in the DuckDB database.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT abstract FROM metadata WHERE paper_id = '12345678';\")\n\n\n#### Case 2\nFind which page in the paper with paper_id '12345678' contains Figure 3.\n\n[Action]: RetrieveFromDatabase(sql=\"SELECT pages.page_number FROM images JOIN pages JOIN metadata ON images.ref_page_id = pages.page_id AND pages.ref_paper_id = metadata.paper_id WHERE metadata.paper_id = '12345678' AND images.image_caption LIKE '%Figure 3%';\")\n\n\n----\n\n### Action Type\nClassicRetrieve\n\n### Description\nGiven a query text, retrieve relevant context from the Milvus vectorstore.\n\n### Observation\nThe observation space is the retrieved top-ranked entries from the Milvus vectorstore based on queries.\n\n### Syntax and Parameters (MARKDOWN Format)\nClassicRetrieve(query: str, limit: int = 5)\n- query: str, required. The query text will be encoded and used to search for relevant context. You can rephrase the original user question to obtain a more clear and structured query requirement.\n- limit: int, optional, default to 5. The number of top-ranked context to retrieve. Please set it to a positive integer to limit the number of returned results. Extremely large limit values may be truncated.\n\n### Use Cases\n\n#### Case 1\nRetrieve top 5 pieces about a certain topic.\n\n[Action]: ClassicRetrieve(query='Is there any work about the topic structured RAG?')\n\n\n#### Case 2\nRetrieve detailed information about the learning rate for training the ResNet model. The top 4 most relevant entries will be returned based on the query.\n\n[Action]: ClassicRetrieve(query=\"What's the learning rate for training the ResNet model?\", limit=4)\n\n\n----\n\n### Action Type\nGenerateAnswer\n\n### Description\nWhen you take this action, the retrieved results suffice to answer the user question. PLEASE STRICTLY ADHERE TO THE ANSWER FORMAT FOR THE CURRENT QUESTION.\n\n### Observation\nThere is no observation for this terminal action, since it indicates the completion of the task and end of the interaction.\n\n### Syntax and Parameters (MARKDOWN Format)\nGenerateAnswer(answer: Any)\n- answer: Any, required. The final answer to the user question. Please adhere to the answer format for the current question.\n\n### Use Cases\n\n#### Case 1\nThe answer to the user question is 42.\n\n[Action]: GenerateAnswer(answer=42)\n\n\n#### Case 2\nThe answer to the user question is a list of strings: ['Results', 'Discussion'].\n\n[Action]: GenerateAnswer(answer=['Results', 'Discussion'])\n\n\n\n--------\n\n## Interaction Framework\n\nThe main interaction procedure proceeds like this:\n\n----\n\n[Thought]: ... reasoning process, why to take this action ...\n[Action]: ... which action to take, please strictly conform to the action specification ...\n[Observation]: ... execution results or error message after taking the action ...\n\n... # more interleaved triplets of ([Thought], [Action], [Observation])\n\n[Thought]: ... reasoning process to produce the final answer ...\n[Action]: ... the terminate action `GenerateAnswer`, there is no further observation ...\n\n----\n\nIn general, the main interaction loop consists of an interleaved of triplets ([Thought], [Action], [Observation]), except the last `GenerateAnswer` action which does not have \"[Observation]:\". You need to predict the \"[Thought]: ...\" followed by the \"[Action]: ...\" for each turn, and we will execute your action in the environment and provide the \"[Observation]: ...\" for the previous action.\n\n--------\n\n## Suggestions or Hints for Agent Interaction\n\n1. Explore multiple retrieval strategies. For example:\n- Experiment with different (table, column) pairs to extract diverse types of information.\n- Query various embedding models (collections) to find the most relevant context.\n\n2. Combine both structured and unstructured data. Concretely:\n- Use SQL queries to retrieve precise facts and structured data. Pay special attention to morphological variations in cell values.\n- Perform similarity searches in the vectorstore to capture semantic relationships and hidden insights.\n\n3. Iterate and refine:\n- If SQL execution result is not satisfactory, try alternative SQL queries to explore the database content carefully.\n- If the vector-based neural retrieval is insufficient, try alternative approaches or parameter settings.\n- Use your findings to validate or enrich the final response.\n\n4. Ensure confidence. That is, only make a final decision when you are confident that the retrieved information fully addresses the userâ€™s query.\n\n\nRemember that, for each question, you only have 20 interaction turns at most. Now, let's start!\n"}
{"role": "user", "content": "[Question]: Except WKM, which method performs the best on WebShop? Whether the two methods' papers use the same evaluation datasets or not?\n[Answer Format]: Your answer should be a Python list of two strings, the first is the abbreviation of the method, the second is either `true` or `false`.\n[Anchor PDF]: '0d7108da-de5c-5e4c-9865-7c4141672767'\n[Database Schema]: The database schema for /home/htc981/workspace/project/HNS-RAG/data/database/ai_research/ai_research.sql is as follows:\n/* database ai_research: This database contains information about AI research papers. Each PDF file is represented or parsed via different views, e.g., pages, sections, figures, tables, and references. We also extract the concrete content inside each concrete element via OCR.\n*/\n/* table metadata: This table stores metadata about each paper, including the number of pages, paper path and paper id.\n*/\nCREATE TABLE IF NOT EXISTS metadata (\n\tpaper_id UUID, -- A unique identifier for this paper.\n\ttitle VARCHAR, -- The title of this paper.\n\tabstract VARCHAR, -- The abstract of this paper.\n\tnum_pages INTEGER, -- The number of pages in this paper.\n\tconference_full VARCHAR, -- The full name of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tconference_abbreviation VARCHAR, -- The abbreviation of the conference where this paper was published. `uncategorized` if the paper has not been published yet.\n\tpub_year INTEGER, -- The year when this paper was published.\n\tvolume VARCHAR, -- The volume of the conference where this paper was published. e.g. findings.\n\tdownload_url VARCHAR, -- The url from which the paper can be downloaded.\n\tbibtex VARCHAR, -- The bibtex of this paper.\n\tauthors VARCHAR[], -- The full names of authors of this paper.\n\tpdf_path VARCHAR, -- The path to the PDF file of this paper.\n\ttldr VARCHAR, -- A brief summary of the paper's main idea or findings generated by LLM based on title and abstract.\n\ttags VARCHAR[], -- Keywords representing the paper's topics, methods, or applications generated by LLM based on title and abstract.,\n\tPRIMARY KEY (paper_id)\n);\n/* table pages: This table stores information of the pages in the papers, including their content, size and their order within the paper. The pagesare extracted using PyMuPDF.\n*/\nCREATE TABLE IF NOT EXISTS pages (\n\tpage_id UUID, -- A unique identifier for each page.\n\tpage_number INTEGER, -- The page number in the current paper, starting from 1.\n\tpage_width INTEGER, -- The pixel width of the current page.\n\tpage_height INTEGER, -- The pixel height of the current page.\n\tpage_content VARCHAR, -- The content of the page.\n\tpage_summary VARCHAR, -- A brief summary of the page content, generated by LLM, focusing on key information and describing the page content.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (page_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table images: This table stores the information about images in each paper. The images are extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS images (\n\timage_id UUID, -- The unique identifier of the image.\n\timage_caption VARCHAR, -- The caption of this image, \"\" if it doesn't have one.\n\timage_summary VARCHAR, -- A brief summary of the image, generated by LLM, focusing on key information and describing the image.\n\tbounding_box INTEGER[4], -- The bounding box of the figure in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height which are used to determine the shape of the rectangle.\n\tordinal INTEGER, -- Each figure is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (image_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table chunks: This table contains the information of each chunk of text (chunk size = 512 tokens with no overlapping) in each page of the paper. A chunk is a sub-text that is extracted from the main text using langchain, such as a sentence or a paragraph.\n*/\nCREATE TABLE IF NOT EXISTS chunks (\n\tchunk_id UUID, -- A unique identifier for each chunk of text.\n\ttext_content VARCHAR, -- The text content of the current chunk.\n\tordinal INTEGER, -- Each chunk is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table.,\n\tPRIMARY KEY (chunk_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table tables: This table stores information about tables extracted from pages using MinerU, including content (in html format), bounding box and summary of each table generated by LLM.\n*/\nCREATE TABLE IF NOT EXISTS tables (\n\ttable_id UUID, -- A unique identifier for each table.\n\ttable_caption VARCHAR, -- Caption of the table, showing key information of the table.\n\ttable_content VARCHAR, -- The content of the table in html format.\n\ttable_summary VARCHAR, -- A brief summary of the table content generated by LLM, focusing on key information and describing the table content.\n\tbounding_box INTEGER[4], -- The bounding box of the table in the format [x0, y0, w, h], where (x0, y0) represents the coordinates of the top-left corner and (w, h) represents the width and height.\n\tordinal INTEGER, -- Each table is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this table is located.,\n\tPRIMARY KEY (table_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table sections: This table contains the text content of each section in the paper, e.g. `Introduction`, `Experiment`, `Conclusion`, `Reference`. The sections are extracted using PyMuPDF or MinerU.\n*/\nCREATE TABLE IF NOT EXISTS sections (\n\tsection_id UUID, -- A unique identifier for each section of text.\n\tsection_title VARCHAR, -- The title of the current section.\n\tsection_content VARCHAR, -- The text content of the current section.\n\tsection_summary VARCHAR, -- A brief summary of the section content generated by LLM, focusing on key information and describing the section content.\n\tordinal INTEGER, -- Each section is labeled with one distinct integer number in the paper, which starts from 0.\n\tpage_numbers INTEGER[], -- A list of page numbers where current section is located in the relevant paper.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.,\n\tPRIMARY KEY (section_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id)\n);\n/* table equations: This table stores information about equations extracted from pages using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS equations (\n\tequation_id UUID, -- A unique identifier for each equation.\n\tequation_content VARCHAR, -- Content of the equation in latex format.\n\tordinal INTEGER, -- Each equation is labeled with one distinct integer number in the current page, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the paper ID in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the page ID in the `pages` table where this equation is located.,\n\tPRIMARY KEY (equation_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n/* table reference: This table stores information about references extracted using MinerU.\n*/\nCREATE TABLE IF NOT EXISTS reference (\n\treference_id UUID, -- A unique identifier for each reference.\n\treference_content VARCHAR, -- Content of the reference.\n\tordinal INTEGER, -- Each reference is labeled with one distinct integer number according to its order in the paper, which starts from 0.\n\tref_paper_id UUID, -- A foreign key linking to the `paper_id` in the `metadata` table.\n\tref_page_id UUID, -- A foreign key linking to the `page_id` in the `pages` table where this reference is located.,\n\tPRIMARY KEY (reference_id),\n\tFOREIGN KEY (ref_paper_id) REFERENCES metadata(paper_id),\n\tFOREIGN KEY (ref_page_id) REFERENCES pages(page_id)\n);\n[Vectorstore Schema]: The vectorstore schema for ai_research is as follows. You can try collections with different encoding models or modalities:\n[\n    {\n        \"collection_name\": \"text_bm25_en\",\n        \"modality\": \"text\",\n        \"embed_type\": \"bm25\",\n        \"embed_model\": \"en\",\n        \"description\": \"This collection is used to store the sparse embeddings generated by the BM25 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric inner-product (IP).\",\n        \"fields\": [\n            {\n                \"name\": \"id\",\n                \"dtype\": \"INT64\",\n                \"is_primary\": true,\n                \"description\": \"The unique id for each encoded text vector in this collection.\"\n            },\n            {\n                \"name\": \"vector\",\n                \"dtype\": \"SPARSE_FLOAT_VECTOR\",\n                \"description\": \"The encoded sparse vector of the text content generated by the BM25 model. Note that, the raw text content can be uniquely identified by the triplet fields below: `table_name`, `column_name`, `primary_key`.\"\n            },\n            {\n                \"name\": \"pdf_id\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The unique id of the PDF file stored in the relational database that the encoded text belongs to. This field can be used to quickly filter the search space.\"\n            },\n            {\n                \"name\": \"page_number\",\n                \"dtype\": \"INT16\",\n                \"description\": \"The page number in the PDF file that the encoded text belongs to (starting from 1). This field can be used to quickly filter the search space. Value -1 means unknown.\"\n            },\n            {\n                \"name\": \"table_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which table in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"column_name\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"Which column under `table_name` in the relational database is the text content from.\"\n            },\n            {\n                \"name\": \"primary_key\",\n                \"dtype\": \"VARCHAR\",\n                \"description\": \"The primary key value of `table_name` (or which row) in the relational database that the text content belongs to. Note that, the triplet (`table_name`, `column_name`, `primary_key`) can be used to construct a one-to-one mapping between the `text` field in the current vectorstore and the cell entry in the relational database.\"\n            }\n        ]\n    },\n    {\n        \"collection_name\": \"text_sentence_transformers_all_minilm_l6_v2\",\n        \"modality\": \"text\",\n        \"embed_type\": \"sentence_transformers\",\n        \"embed_model\": \"all-MiniLM-L6-v2\",\n        \"description\": \"This collection is used to store the embeddings generated by the all-MiniLM-L6-v2 model for all encodable text content in another relational database. The similary search is based on field `vector` with metric COSINE.\",\n        \"fields\": \"The fields of this collection are similar to the `text_bm25_en` collection.\"\n    }\n]\n\nThe following lists all encodable (table_name, column_name, encode_modality) information from the corresponding DuckDB database, where the encoded vector entries are sourced. Note that, for text modality, we directly encode cell values from the `table_name.column_name`; for image modality, we encode the cropped images for `page_number` in `pdf_id` with bounding boxes from `table_name.column_name`. You can leverage them for filter condition during similarity search:\n[\n    [\n        \"metadata\",\n        \"title\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"abstract\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"bibtex\",\n        \"text\"\n    ],\n    [\n        \"metadata\",\n        \"tldr\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_content\",\n        \"text\"\n    ],\n    [\n        \"pages\",\n        \"page_summary\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_caption\",\n        \"text\"\n    ],\n    [\n        \"images\",\n        \"image_summary\",\n        \"text\"\n    ],\n    [\n        \"chunks\",\n        \"text_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_caption\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_content\",\n        \"text\"\n    ],\n    [\n        \"tables\",\n        \"table_summary\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_title\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_content\",\n        \"text\"\n    ],\n    [\n        \"sections\",\n        \"section_summary\",\n        \"text\"\n    ],\n    [\n        \"equations\",\n        \"equation_content\",\n        \"text\"\n    ],\n    [\n        \"reference\",\n        \"reference_content\",\n        \"text\"\n    ]\n]\n\nHere are the operators that you can use in the filtering condition for the vectorstore:\n{\n    \"operators\": {\n        \"logical\": [\n            {\n                \"symbol\": \"not\",\n                \"example\": \"not expr\",\n                \"description\": \"True if expr is false.\"\n            },\n            {\n                \"symbol\": \"and\",\n                \"example\": \"expr1 and expr2\",\n                \"description\": \"True if both expr1 and expr2 are true.\"\n            },\n            {\n                \"symbol\": \"or\",\n                \"example\": \"expr1 or expr2\",\n                \"description\": \"True if either expr1 or expr2 are true.\"\n            }\n        ],\n        \"binary arithmetic\": [\n            {\n                \"symbol\": \"+\",\n                \"example\": \"a + b\",\n                \"description\": \"Add the two operands.\"\n            },\n            {\n                \"symbol\": \"-\",\n                \"example\": \"a - b\",\n                \"description\": \"Subtract the second operand from the first operand.\"\n            },\n            {\n                \"symbol\": \"*\",\n                \"example\": \"a * b\",\n                \"description\": \"Multiply the two operands.\"\n            },\n            {\n                \"symbol\": \"/\",\n                \"example\": \"a / b\",\n                \"description\": \"Divide the first operand by the second operand.\"\n            },\n            {\n                \"symbol\": \"**\",\n                \"example\": \"a ** b\",\n                \"description\": \"Raise the first operand to the power of the second operand.\"\n            },\n            {\n                \"symbol\": \"%\",\n                \"example\": \"a % b\",\n                \"description\": \"Divide the first operand by the second operand and yield the remainder portion.\"\n            }\n        ],\n        \"relational\": [\n            {\n                \"symbol\": \"<\",\n                \"example\": \"a < b\",\n                \"description\": \"True if a is less than b.\"\n            },\n            {\n                \"symbol\": \">\",\n                \"example\": \"a > b\",\n                \"description\": \"True if a is greater than b.\"\n            },\n            {\n                \"symbol\": \"==\",\n                \"example\": \"a == b\",\n                \"description\": \"True if a is equal to b. Please use **two** equal signs for equality comparison.\"\n            },\n            {\n                \"symbol\": \"!=\",\n                \"example\": \"a != b\",\n                \"description\": \"True if a is not equal to b.\"\n            },\n            {\n                \"symbol\": \"<=\",\n                \"example\": \"a <= b\",\n                \"description\": \"True if a is less than or equal to b.\"\n            },\n            {\n                \"symbol\": \">=\",\n                \"example\": \"a >= b\",\n                \"description\": \"True if a is greater than or equal to b.\"\n            },\n            {\n                \"symbol\": \"in\",\n                \"example\": \"a in [b, c, ...]\",\n                \"description\": \"True if a is equal to any of the values in the list.\"\n            }\n        ],\n        \"array operations\": [\n            {\n                \"symbol\": \"[]\",\n                \"example\": \"bbox[0]\",\n                \"description\": \"This expression gets the first element in array bbox. For example, 'bbox[0] < 50' evaluates to true if the value of bbox[0] is less than 50.\"\n            },\n            {\n                \"symbol\": \"array_contains\",\n                \"example\": \"array_contains(bbox, 100)\",\n                \"description\": \"This expression evaluates to true if array bbox contains element 100.\"\n            },\n            {\n                \"symbol\": \"array_length\",\n                \"example\": \"array_length(int_array) == 4\",\n                \"description\": \"This expression evaluates to true if int_array contains exactly 4 elements.\"\n            }\n        ]\n    },\n    \"use cases\": [\n        {\n            \"example\": \"table_name == 'chunks' and column_name == 'text_content'\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and column_name equal to 'text_content'.\"\n        },\n        {\n            \"example\": \"table_name == 'chunks' and primary_key in ['123', '456', '789']\",\n            \"description\": \"Filter the results with table_name equal to 'chunks' and primary_key equal to any of the values in the list ['123', '456', '789'].\"\n        },\n        {\n            \"example\": \"bbox[0] + bbox[2] <= 100 and bbox[1] + bbox[3] <= 300 and pdf_id == '123456' and page_number == 1\",\n            \"description\": \"For image modality, filter results to include only image regions that are entirely within the upper-left quadrant of page 1 in the PDF with ID '123456', assuming the page dimensions are 200 (width) x 600 (height).\"\n        }\n    ]\n}"}
{"role": "assistant", "content": "[Thought]: First, I need to find the paper with the given PDF ID and check its content to identify the method that performs the best on WebShop, excluding WKM. I will start by retrieving the metadata of the paper with the given PDF ID.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM metadata WHERE pdf_path = '0d7108da-de5c-5e4c-9865-7c4141672767';\")"}
{"role": "user", "content": "[Observation]: [Warning]: The SQL execution result is empty, please check the SQL first."}
{"role": "assistant", "content": "[Thought]: The SQL query did not return any results, which suggests that the PDF ID might be incorrect or the paper is not in the database. I will try to retrieve the paper using the `paper_id` instead of `pdf_path`.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM metadata WHERE paper_id = '0d7108da-de5c-5e4c-9865-7c4141672767';\")"}
{"role": "user", "content": "[Observation]:\n{\"paper_id\":\"0d7108da-de5c-5e4c-9865-7c4141672767\",\"title\":\"Agent Planning with World Knowledge Model\",\"abstract\":\"Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the \\\"real\\\" physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three real-world simulated datasets with Mistral-7B, Gemma-7B, and Llama-3-8B demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development.\",\"num_pages\":29,\"conference_full\":\"Conference on Neural Information Processing Systems\",\"conference_abbreviation\":\"NeurIPS\",\"pub_year\":2024,\"volume\":\"NeurIPS 2024 poster\",\"download_url\":\"https:\\/\\/openreview.net\\/pdf\\/f7a356cde46106ef3f622466166ad2984ce70670.pdf\",\"bibtex\":\"@inproceedings{\\nqiao2024agent,\\ntitle={Agent Planning with World Knowledge Model},\\nauthor={Shuofei Qiao and Runnan Fang and Ningyu Zhang and Yuqi Zhu and Xiang Chen and Shumin Deng and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https:\\/\\/openreview.net\\/forum?id=j6kJSS9O6I}\\n}\",\"authors\":\"['Shuofei Qiao' 'Runnan Fang' 'Ningyu Zhang' 'Yuqi Zhu' 'Xiang Chen'\\n 'Shumin Deng' 'Yong Jiang' 'Pengjun Xie' 'Fei Huang' 'Huajun Chen']\",\"pdf_path\":\"data\\/dataset\\/airqa\\/papers\\/neurips2024\\/0d7108da-de5c-5e4c-9865-7c4141672767.pdf\",\"tldr\":\"We introduce parametric World Knowledge Model (WKM) which provides global prior task knowledge and local dynamic state knowledge to facilitate agent planning.\",\"tags\":\"['world knowledge model' 'agent planning' 'large language models']\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: Now that I have the metadata of the paper, I need to find the sections or tables that discuss the performance of methods on the WebShop dataset. I will start by searching the sections and tables for relevant information.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM sections WHERE ref_paper_id = '0d7108da-de5c-5e4c-9865-7c4141672767' AND section_content LIKE '%WebShop%';\")"}
{"role": "user", "content": "[Observation]:\n{\"section_id\":\"60681377-c831-5284-93f5-3a95ce10f4c2\",\"section_title\":\"Introduction\",\"section_content\":\"Introduction\\nThe remarkable advances in Large Language Models (LLMs) have witnessed a rapid development of\\nvarious natural language processing tasks [25, 16, 28, 47, 60, 33]. Recently, multiple attempts that\\n\\u2217\\nEqual Contribution.\\n\\u2020\\nCorresponding Author.\\n3The code is available at https:\\/\\/github.com\\/zjunlp\\/WKM.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\ndirectly exploit LLMs as agent models to address physical world planning tasks have demonstrated\\npromising achievements [54, 57, 56, 34, 38, 64, 44]. However, as most state-of-the-art LLMs\\nare autoregressive models trained with next-token prediction, they lack the ability to essentially\\nunderstand the real world, leading to generating hallucinatory actions and performing brainless\\ntrial-and-error in the environment as shown in Figure 1(a).\\nIn contrast to LLMs, humans possess a mental knowledge model about the physical world [1, 18,\\n17, 30]. When facing a specific task, they will first briefly rehearse the entire process in mind using\\ntheir rich prior knowledge before performing mindless actions. We call this kind of knowledge global\\ntask knowledge (a.k.a. environment\\/task commonsense). In addition, during the task procedure, the\\nmental world knowledge model will constantly maintain a kind of local state knowledge, representing\\nhumans\\u2019 cognition of the current world state. For example, imagine you are in a room and your task\\nis to put a clean egg in microwave. The task knowledge may refer to The egg is most\\nlikely in the fridge ... The workflows are: 1) locate and take the egg; 2)\\nclean the egg using sinkbasin ... The state knowledge possibly refers to My task is to\\n... I have found and taked the egg ... Next I should ... The absence of world knowledge\\ncan lead to blind trial-and-error in the early planning stages when environmental information is\\nlimited. Conversely, in later stages when information is redundant, it can easily result in a confused\\ncognition of the current world state and generate hallucinatory actions.\\nThe process by which humans handle planning tasks reminds us to develop a parametric World\\nKnowledge Model (WKM) to facilitate agent planning. As humans typically acquire knowledge from\\nexpertise and practical experience, we build WKM based on knowledge learned from both expert\\nand explored trajectories. Specifically, we first steer the agent model to synthesize task knowledge\\nfrom the comparison between expert and sampled trajectories. Then we prompt it to summarize state\\nknowledge for each planning step from expert trajectories and combine the previous and next actions\\nto build a state knowledge base. Lastly, we integrate the generated knowledge into expert trajectories\\nand train a WKM. The agent model needs to be retrained to adapt to the task knowledge. Note our\\nagent and knowledge model are both trained with LoRA [12] sharing the same backbone.\\nDuring the planning phase, we use the WKM to provide global prior task knowledge and maintain\\nlocal dynamic state knowledge for the agent model as shown in Figure 1(b). The task knowledge\\nwill be concatenated in natural language form following the specific task to guide the agent model\\u2019s\\ntrial-and-error. At each planning step, to prevent the occurrence of hallucinatory actions, we utilize the\\ngenerated state knowledge as the query to conduct kNN retrieval from the pre-built state knowledge\\nbase. We then use the constraints from the previous action, the probabilities of the retrieved next\\nactions, and the probabilities from the agent model to make a weighted prediction for the next action.\\nWe evaluate our method on three real-world simulated planning tasks: ALFWorld [41], WebShop\\n[53], and ScienceWorld [50] with three state-of-the-art open-source LLMs: Mistral-7B [16], Gemma-\\n7B [24], and Llama-3-8B [25]. Empirical results demonstrate that our method achieves superior\\nperformance compared to various strong baselines on both seen and unseen tasks. Moreover, further\\nanalytical results show that 1) our WKM can effectively reduce blind trial-and-error and hallucinatory\\nactions, 2) our model-generated instance-level knowledge can generalize better to unseen tasks, 3)\\nweak-guide-strong is feasible, 4) multi-task unified WKM possesses strong potential, and 5) explicit\\nstate knowledge will hurt the performance of agent planning.\\n2\",\"section_summary\":\"Recent advances in LLMs show promise in physical world planning tasks but lack real-world understanding. A World Knowledge Model (WKM) is proposed, integrating global task and local state knowledge, improving performance and reducing hallucinatory actions in simulated planning tasks.\",\"ordinal\":0,\"page_numbers\":\"[1 2]\",\"ref_paper_id\":\"0d7108da-de5c-5e4c-9865-7c4141672767\"}\n\n... # only display 1 rows in JSON format, more are truncated due to length constraint based on max_tokens (5000)"}
{"role": "assistant", "content": "[Thought]: The introduction section mentions that the paper evaluates the WKM on three datasets, including WebShop. I need to find the specific section or table that compares the performance of different methods on WebShop. I will search the sections and tables again with a more specific query.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM sections WHERE ref_paper_id = '0d7108da-de5c-5e4c-9865-7c4141672767' AND section_content LIKE '%WebShop%' AND section_title IN ('Results', 'Experiments', 'Evaluation');\")"}
{"role": "user", "content": "[Observation]:\n{\"section_id\":\"c20a59ee-5774-5e06-a482-892ac52c5ea2\",\"section_title\":\"Experiments\",\"section_content\":\"Experiments\\n4.1\\nExperimental Settings\\nDatasets and Metrics.\\nWe evaluate our method on three real-world simulated planning datasets:\\nALFWorld [41], WebShop [53], and ScienceWorld [50]. AlFWorld and ScienceWorld include\\n5\\nTable 1: Main Results. The best results are marked in bold and the second-best results are marked with\\nunderline. All the prompt-based baselines ( u ) are evaluated under one-shot prompting and all the fine-tuning-\\nbased baselines ( v ) are trained through LoRA. Red represents the changes of WKM relative to the optimal\\nresults in the baselines. WKM and agent model are different LoRAs sharing the same backbone.\\nBackbone\\nMethod\\nALFWorld\\nWebShop\\nScienceWorld\\nSeen\\nUnseen\\nSeen\\nUnseen\\nGPT-3.5-Turbo\\nu REACT\\n8.57\\n5.97\\n44.37\\n15.41\\n13.99\\nGPT-4\\n44.29\\n38.05\\n62.76\\n67.32\\n65.09\\nMistral-7B\\nu REACT\\n7.86\\n5.22\\n14.63\\n20.72\\n17.65\\nu Reflexion\\n11.56\\n6.00\\n16.64\\n21.07\\n18.11\\nv NAT\\n64.43\\n68.96\\n61.01\\n57.12\\n50.79\\nv ETO\\n66.84\\n71.43\\n64.09\\n58.17\\n51.85\\nv KNOWAGENT\\n70.44\\n70.72\\n61.28\\n59.32\\n47.24\\nWKM\\n73.57 +3.13\\n76.87 +5.44\\n65.48 +1.39\\n62.12 +2.80\\n53.62 +1.77\\nGemma-7B\\nu REACT\\n6.43\\n2.24\\n5.93\\n3.58\\n3.51\\nu Reflexion\\n7.14\\n2.99\\n7.71\\n4.94\\n3.93\\nv NAT\\n67.86\\n65.88\\n55.82\\n47.63\\n44.98\\nv ETO\\n66.43\\n68.66\\n62.67\\n50.44\\n47.84\\nv KNOWAGENT\\n69.29\\n67.60\\n58.80\\n48.55\\n45.28\\nWKM\\n70.71 +1.42\\n70.40 +1.74\\n63.75 +1.08\\n53.68 +3.24\\n49.24 +1.40\\nLlama-3-8B\\nu REACT\\n2.86\\n3.73\\n19.32\\n24.76\\n22.66\\nu Reflexion\\n4.29\\n4.48\\n22.73\\n27.23\\n25.41\\nv NAT\\n60.71\\n59.70\\n61.60\\n55.24\\n48.76\\nv ETO\\n64.29\\n64.18\\n64.57\\n57.90\\n52.33\\nv KNOWAGENT\\n66.71\\n62.69\\n64.40\\n58.67\\n49.18\\nWKM\\n68.57 +1.86\\n65.93 +1.75\\n66.64 +2.07\\n60.12 +1.55\\n54.75 +2.42\\nunseen tasks to evaluate the agent\\u2019s generalization ability. The reward of ALFWorld is binary 0 or\\n1, indicating whether the agent has completed the task or not. WebShop and ScienceWorld provide\\ndense rewards from 0 to 1 to measure the completion level of the task. For all the datasets, we apply\\naverage reward as the final metrics. Please refer to Appendix B for detailed dataset information.\\nModels and Baselines.\\nWe evaluate on three state-of-the-art open-source models: 1) Mistral-7B\\n[16], the Mistral-7B-Instruct-v0.2 version. 2) Gemma-7B [24], the Gemma-1.1-7B-it version. 3)\\nLlama-3-8B [25], the Meta-Llama-3-8B-Instruct version. We compare our method with two prompt-\\nbased baselines: REACT [54] and Reflexion [40]. Besides, we adopt two strong baselines that\\nintroduce rejected trajectories into the training process to learn from experience: NAT [49], learn\\nfrom rejected trajectories through SFT, and ETO [44], learn from rejected trajectories through DPO\\n[36]. Moreover, we compare with a knowledge-augmented planning method KNOWAGENT. We\\nalso include ChatGPT (gpt-3.5-turbo-0125) [27] and GPT-4 (gpt-4-32K-0613) [28] for comparison.\\nAll the prompt-based baselines are tested under one-shot and all the fine-tuning-based baselines are\\ntrained with LoRA [12]. Please refer to Appendix C for baselines and re-producing details.\\nTraining and Inference Setups.\\nWe fine-tune the proposed approach with LoRA [12] using the\\nLlamaFactory [62] framework. During training, the model is tuned after finishing the entire trajectory\\nrather than each step of action. The learning rate is 1e-4 and the sequence length is 2048 for all\\nthe models. The training epoch is 3 and the batch size is 32. We adopt the AdamW optimizer [22]\\nwith a cosine learning scheduler. During inference, we apply the embedding layer of WKM as the\\nencoder and use the cosine similarity between sentences for retrieval. The number of retrieved\\naction-state-action triplets N is set to 3000 and the Pagent(Au) weight \\u03b3 is set to {0.4, 0.5, 0.7}. All\\nthe training and inference experiments are conducted on 8 NVIDIA V100 32G GPUs within 12 hours.\\nPlease refer to Appendix D for detailed hyperparameters used in our paper.\\n4.2\\nResults\\nMain Results.\\nAs shown in Table 1, for prompt-based baselines on open-source models, both RE-\\nACT and Reflexion exhibit poor performance, far behind our method and fine-tuning-based baselines\\non various datasets. GPT-3.5-Turbo performs ordinarily on two datasets other than WebShop, and it\\neven falls behind Mistral-7B and Llama-3-8B\\u2019s REACT performance on ScienceWorld. However,\\nGPT-4 exhibits strong performance across various datasets. Nevertheless, our approach, through\\n6\\nseen\\nunseen\\n50\\n60\\n70\\n80\\n63.57\\n69.40\\n67.86\\n70.67\\n69.29\\n75.37\\n73.57\\n76.87\\n67.19\\n71.57\\n65.46\\n70.71\\n65.14\\n67.40\\nALFWorld\\nAverage Reward\\ntest\\n50\\n60\\n70\\n80\\n56.98\\n62.44 63.68\\n65.48 63.97 63.70\\n61.03\\nWebShop\\nseen\\nunseen\\n40\\n50\\n60\\n70\\n52.78\\n50.32\\n55.04\\n51.52\\n60.81\\n53.42\\n62.12\\n53.62\\n55.49\\n48.38\\n58.51\\n51.78\\n56.36\\n45.27\\nScienceWorld\\nw\\/o all\\nw\\/ state\\nw\\/ task\\nw\\/ task&state\\nw\\/o rejected\\nmerge\\nprompt\\nFigure 3: Ablation Study on Mistral-7B. w\\/o all means the vanilla experienced agent model training with\\npure expert trajectories. w\\/ state is testing agent model with only state knowledge base constraints. w\\/ task\\nstands for guiding agent model with only task knowledge. w\\/ task&state is our WKM with both task knowledge\\nguidance and state knowledge constraints. w\\/o rejected means synthesizing task knowledge solely through\\nexpert trajectories. merge stands for training WKM and the agent model together with one single model. prompt\\nmeans using few-shot prompts to replace the WKM for providing knowledge.\\nTable 2: Average Steps. The maximum number of steps in\\nALFWorld and WebShop is 40 and 10. In ScienceWorld, the\\nnumber of steps ranges from 10 to 120 depending on the task\\ntype, with an average of around 40.\\nMethod\\nALFWorld\\nWebShop\\nScienceWorld\\nSeen\\nUnseen\\nSeen\\nUnseen\\nNAT\\n23.27\\n23.42\\n4.08\\n20.18\\n21.21\\nETO\\n19.82\\n22.29\\n3.99\\n24.13\\n26.35\\nKNOWAGENT\\n18.51\\n24.56\\n4.01\\n21.06\\n24.74\\nWKM\\n17.66\\n17.92\\n3.97\\n18.74\\n19.59\\nTable 3: Hallucinatory Action Rates on\\nALFWorld. We calculate the proportion of\\ntrajectories containing invalid actions regard-\\nless of their correctness.\\nMethod\\nALFWorld\\nSeen\\nUnseen\\nNAT\\n45.71%\\n50.00%\\nETO\\n34.29%\\n36.57%\\nKNOWAGENT\\n33.57%\\n44.78%\\nWKM\\n32.86%\\n29.85%\\nLoRA training alone, surpasses GPT-4 on ALFWorld (44.29\\u219273.57 on seen, 38.05\\u219276.87 on\\nunseen) and WebShop (62.76\\u219266.64). For fine-tuning-based baselines, both NAT and ETO fall\\nbehind our method, implying that just integrating world knowledge for agent models is worth more\\nthan further fussy SFT or DPO on negative examples. Our method also performs better than KNOWA-\\nGENT which brings human-designed fixed action knowledge and long action paths into trajectories.\\nThis suggests the effectiveness of our WKM which is responsible for generating instance-level task\\nknowledge and maintaining implicit action constraints. Furthermore, KNOWAGENT\\u2019s performance\\non unseen tasks is not as impressive as on seen tasks, while WKM can keep its advantage. This\\nphenomenon also demonstrates the generalization ability of WKM.\\nApproach Ablations.\\nAs shown in Figure 3, taking Mistral-7B as an example, we decompose\\nthe key components of WKM to examine the roles of the task and state knowledge separately. In a\\nmacro view, removing each module results in a clear drop in the agent\\u2019s performance, which validates\\nthe power of our world knowledge. Furthermore, the improvement through task knowledge (w\\/\\ntask) is more pronounced than that through state knowledge (w\\/ state), suggesting the necessity of\\nglobal prior knowledge for agent planning. A more micro observation reveals that the impact of state\\nknowledge is more significant on seen tasks compared to unseen tasks, while the influence of task\\nknowledge is sustainable across seen and unseen tasks. This may be attributed that although our\\nreal-time state knowledge is generated by WKM, the state knowledge base is built on the training\\nset, which may weaken generalization to some extent. Additionally, to validate our motivation of\\nallowing the agent to learn task knowledge from both expert and generated trajectories, we exclude the\\nrejected trajectories during the synthesis of task knowledge, instructing the agent model to synthesize\\nknowledge solely based on the chosen trajectories. The results (w\\/o rejected) demonstrate that\\nlearning from the contrast between chosen and rejected trajectories is more effective than learning\\nfrom chosen examples alone. This procedure is a little similar to DPO, but we achieve it through\\nknowledge augmentation rather than directly converting it into a loss calculation between chosen and\\nrejected trajectories. Additional results can further evident that training a WKM separately performs\\nbetter than training one single model together with the agent model as well as using few-shot prompts\\nto replace WKM for providing knowledge.\\n4.3\\nAnalysis\\nWorld knowledge can mitigate blind trial-and-error and reduce hallucinatory actions.\\nWe\\ncompare the number of planning steps for each dataset between three strong baselines and WKM\\nand calculate the average steps of each method. As depicted in Figure 9 (in Appendix F), WKM\\n7\\ndemonstrates the ability to complete a significant proportion of tasks using the shortest trajectory,\\nindicating that guidance from world knowledge can effectively reduce the agent\\u2019s blind trial-and-error\\nin the environment. Taking a further perspective from an average standpoint in Table 2, it can\\nbe observed that WKM exhibits lower average planning steps compared to other baselines. As\\nALFWorld can respond to invalid actions, in Table 3, we count the percentage of hallucinatory actions\\nthat occurred in trajectories from ALFWorld for each method. The results confirm the effectiveness\\nof our world knowledge model to decrease hallucinatory actions. Furthermore, it is worth noting that\\nmost baselines show a prominent increase in the average number of steps and percentage of invalid\\nactions when transitioning from seen tasks to unseen tasks, but WKM can still maintain a relatively\\nlow level. This reflects laterally that our world knowledge can still effectively guide the agent model\\non unseen tasks, highlighting the knowledge generalization brought by the world knowledge model.\\nTo see how our world knowledge works, please refer to our case study in Appendix H.\\nALFWorld seen\\nALFWorld unseen\\nSciWorld seen\\nSciWorld unseen\\n40\\n50\\n60\\n70\\n80\\nAverage Reward\\nHuman\\nWKM w\\/o state\\nFigure 4: Performance of human-\\ndesigned dataset-level knowledge\\ncompared to WKM generated\\ninstance-level knowledge.\\nOur instance-level knowledge can generalize better to unseen\\ntasks.\\nTo further explore the benefit of using a knowledge model to\\ngenerate instance-level task knowledge, we carefully survey the task\\nknowledge generated by our WKM and abstract it into dataset-level\\nknowledge for each dataset. Then we retrain the agent model to\\nadapt to new dataset-level knowledge5. As illustrated in Figure 4,\\nwe compare the performance of dataset-level knowledge with our\\ninstance-level task knowledge (WKM w\\/o state) on ALFWorld and\\nScienceWorld. It can be observed that our model-generated instance-\\nlevel knowledge not only surpasses human-designed knowledge\\non seen tasks but also exhibits even more remarkable performance\\non unseen tasks, with the improvement in performance on unseen\\ntasks significantly greater than that on seen tasks. This phenomenon\\nstraightly reflects the strong generalization ability of our knowledge\\nmodel compared to rigidly designed knowledge by humans.\\nTable 4: Weak-guide-strong. The knowl-\\nedge model here is based on Mistral-7B.\\nBackbone\\nMethod\\nALFWorld\\nSeen\\nUnseen\\nGPT-3.5-Turbo\\nREACT\\n8.57\\n5.97\\nWKM w\\/o state\\n12.86\\n8.96\\nGPT-4\\nREACT\\n44.29\\n38.05\\nWKM w\\/o state\\n50.71\\n47.01\\nWeak knowledge model guides strong agent model\\nplanning.\\nIn our main experiments, the knowledge\\nmodel and agent model are based on the same backbone.\\nHere, we explore on ALFWorld what will happen if we use\\na weak knowledge model to guide a strong agent model.\\nWe choose Mistral-7B as the backbone of the knowledge\\nmodel and ChatGPT and GPT-4 as the agent model. Since\\nwe cannot get the token distribution from OpenAI API, we\\nonly apply task knowledge to the agent model. As exhibited in Table 4, the results of both ChatGPT\\nand GPT-4 show distinct advances after being guided by the Mistral-7B world knowledge model,\\nindicating the weak world knowledge model also contains knowledge that the strong model may lack.\\nIn the era of LLMs, this inspires us with a new agent learning paradigm: weak-guide-strong. Due\\nto its lightweight nature, the weak knowledge model can flexibly adjust its parameters based on the\\nneeds of the agent model, which can address the difficulty of large agent models in adapting to new\\nenvironments through fine-tuning.\\nScienceWorld\\nseen\\nScienceWorld\\n unseen\\nALFWorld\\nseen\\nALFWorld \\nunseen\\nWebShop\\n0%\\n50%\\n100%\\nETO\\nKnowAgent\\nNAT\\nWKM-single-task\\nWKM-multi-task\\nFigure 5: Relative performance\\nof multi-task WKM compared to\\nvarious baselines.\\nUnified World Knowledge Model Training.\\nWe mix the world\\nknowledge collected from all three datasets and jointly train one\\nsingle world knowledge model to investigate the effect of multi-task\\nworld knowledge learning. Figure 5 illustrates the relative perfor-\\nmance comparison between multi-task WKM and various baselines,\\nfrom which we can observe that multi-task WKM not only does\\nnot lead to performance degradation but also exhibits visible im-\\nprovements compared to single-task WKM, especially on WebShop\\nand ScienceWorld. Similar to [57, 58, 3] which endeavor to train\\na unified agent model and achieve strong generalization ability to\\nheld-out tasks, this observation inspires us with the potential of train-\\ning a unified world knowledge model that can be applied to help\\n5Detailed manually designed dataset-level knowledge prompt can be found in Appendix I.3\\n8\\nvarious held-in agent models and also generalize to guide held-out agent models. A more daring\\nidea is whether a unified agent model combined with a unified world knowledge model is the key to\\nArtificial General Intelligence (AGI).\\nALFWorld seen\\nALFWorld unseen\\n40\\n50\\n60\\n70\\n80\\n73.57\\n76.87\\n69.29\\n75.37\\n65.43\\n66.69\\nAverage Reward\\nWKM\\nWKM w\\/o state\\nExplicit State\\nFigure 6: Performance of ex-\\nplicit state knowledge.\\nExplicit state knowledge will hurt the planning performance.\\nTo demonstrate the rationality of our choice to construct a state\\nknowledge base, we explore the effect of directly incorporating\\nstate knowledge into the context of the agent model (we retrain the\\nagent model to follow both the task and state knowledge), as shown\\nin Figure 6. The performance of explicit state knowledge is far\\ninferior to our approach of retrieving from a state knowledge base\\nand utilizing probabilistic constraints. It even performs worse than\\nwhen we remove state knowledge and only include task knowledge. This clearly indicates that blindly\\nextending prompts with a large amount of explicit natural language feedback is lose-more-than-gain\\nfor agent planning, and implicit knowledge constraints may be sometimes more prudent.\\nCase Study.\\nIn Figure 10 (Appendix H), we list the trajectories of ETO and our WKM within the\\nsame task in ALFWorld to illustrate how world knowledge functions. The rationales before each\\naction have been omitted to guarantee a clear illustration. The task is to clean some soapbar\\nand put it in cabinet. Initially, ETO blindly searches for the soapbar in the countertop\\nand cabinet, introducing a lot of irrelevant information and unnecessary context. In the later stages\\nof planning, ETO experiences the hallucination and executes the put action after close the cabinet,\\ncausing the environment to become unrecognizable and resulting in a collapse. On the contrary,\\nguided by task knowledge, WKM directly identified the possible locations of the soapbar and\\nsuccessfully found it in the first attempt. Subsequently, WKM efficiently completed the task with\\nprecision, adhering to the constraints of state knowledge.\\n5\",\"section_summary\":\"The study evaluates WKM, a world knowledge model, on three planning datasets, showing superior performance over baselines in both seen and unseen tasks, reducing hallucinatory actions and planning steps. WKM's instance-level knowledge generalizes better than human-designed knowledge.\",\"ordinal\":3,\"page_numbers\":\"[5 6 7 8 9]\",\"ref_paper_id\":\"0d7108da-de5c-5e4c-9865-7c4141672767\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The \"Experiments\" section provides detailed results for the WebShop dataset. According to Table 1, the method that performs the best on WebShop, excluding WKM, is ETO. I also need to check if the datasets used in the WKM paper and the ETO paper are the same. I will first retrieve the metadata of the ETO paper.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM metadata WHERE title LIKE '%ETO%';\")"}
{"role": "user", "content": "[Observation]:\n{\"paper_id\":\"2399ae45-0de4-5e32-bbbc-acef0d2f3b0b\",\"title\":\"ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses\",\"abstract\":\"We tackle the efficiency problem of learning local feature matching.Recent advancements have given rise to purely CNN-based and transformer-based approaches, each augmented with deep learning techniques. While CNN-based methods often excel in matching speed, transformer-based methods tend to provide more accurate matches. We propose an efficient transformer-based network architecture for local feature matching.This technique is built on constructing multiple homography hypotheses to approximate the continuous correspondence in the real world and uni-directional cross-attention to accelerate the refinement. On the YFCC100M dataset, our matching accuracy is competitive with LoFTR, a state-of-the-art transformer-based architecture, while the inference speed is boosted to 4 times, even outperforming the CNN-based methods.Comprehensive evaluations on other open datasets such as Megadepth, ScanNet, and HPatches demonstrate our method's efficacy, highlighting its potential to significantly enhance a wide array of downstream applications.\",\"num_pages\":15,\"conference_full\":\"Conference on Neural Information Processing Systems\",\"conference_abbreviation\":\"NeurIPS\",\"pub_year\":2024,\"volume\":\"NeurIPS 2024 poster\",\"download_url\":\"https:\\/\\/openreview.net\\/pdf\\/e1759f7fb7b08ea7843c1ece330b1ff8b3513224.pdf\",\"bibtex\":\"@inproceedings{\\nni2024etoefficient,\\ntitle={{ETO}:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses},\\nauthor={Junjie Ni and Guofeng Zhang and Guanglin Li and Yijin Li and Xinyang Liu and Zhaoyang Huang and Hujun Bao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https:\\/\\/openreview.net\\/forum?id=3xHCaDdYcc}\\n}\",\"authors\":\"['Junjie Ni' 'Guofeng Zhang' 'Guanglin Li' 'Yijin Li' 'Xinyang Liu'\\n 'Zhaoyang Huang' 'Hujun Bao']\",\"pdf_path\":\"data\\/dataset\\/airqa\\/papers\\/neurips2024\\/2399ae45-0de4-5e32-bbbc-acef0d2f3b0b.pdf\",\"tldr\":\"We reduce the time comsuming for local feature matching by reducing the units that participate in transformer, while using more complicated homography hypothese to maintain the accuracy.\",\"tags\":\"['local feature matching' '3d vision' 'pose estimation']\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The metadata of the ETO paper shows that it is a different paper focusing on local feature matching and not related to the WebShop dataset. I need to find the ETO paper that is relevant to the WebShop dataset. I will perform a similarity search in the vectorstore to find the relevant context.\n[Action]:\nClassicRetrieve(query='ETO method on WebShop dataset', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.5573,\"pdf_id\":\"722fbe98-4e3d-5d07-aea6-e4261418a8c8\",\"page_number\":7,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"7ae6588c-c46a-5352-9711-edf6845ee82b\",\"text\":\"iteration numbers in ETO. The results are shown\\nin Figure 4. As depicted, ETO demonstrates the\\nability to enhance the performance of agents in the\\nfirst two iterations on both the WebShop and Sci-\\nenceWorld datasets. However, further increasing\\nthe iterations does not lead to continuous improve-\\nment. Instead, the performance starts to decline\\nafter the third iteration. Regarding the ALFWorld\\ndataset, only the first iteration of ETO shows an\\nimprovement. Surprisingly, the performance on the\"}\n{\"score\":0.5565,\"pdf_id\":\"722fbe98-4e3d-5d07-aea6-e4261418a8c8\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"74ca39ed-5f4a-5d1d-8fd4-8e4de395c852\",\"text\":\"A\\nDatasets\\nWebShop\\nWebShop (Yao et al., 2022a) is an online shopping website environment where agents\\nnavigate the platform to make purchases based on user instructions. Once the agent selects the \\\"buy\\\"\\naction, the environment provides a final reward, which is calculated based on the matching heuristics of\\nthe product\\u2019s attributes and price.\\nScienceWorld\\nScienceWorld (Wang et al., 2022) is a text-based virtual environment centered around\"}\n{\"score\":0.5249,\"pdf_id\":\"72b6ef51-35fe-552e-b0ff-6177229ce19a\",\"page_number\":4,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"6eaeab9d-42d5-5bd1-88ca-e8f84176d8e1\",\"text\":\"datasets in order to assess our approach on vari-\\nous applications and add three public datasets that\\nare predominantly evaluated in recommendation\\ncommunities. The in-house datasets are built from\\nservices of an online booking service (OBS), an\\nonline travel agency (OTA), and e-commerce platm-\\nform (ECOMM). For public datasets, we select two\\ncategories \\u201cIndustrial and Scientific\\u201d (Scientific)\\nand \\u201cPrime Pantry\\u201d (Pantry) from Amazon review\\ndatatsets (Ni et al., 2019) which are two completely\"}\n{\"score\":0.5113,\"pdf_id\":\"648e3d50-375b-5189-b6b0-e0520626716e\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"88247a04-0efd-5ef6-8133-8daefa3a77fc\",\"text\":\"(v) WebShopping contains tasks related to online\\nshopping on E-commerce websites, e.g., searching\\nfor an item, adding an item to the cart, and viewing\\nthe shopping cart.\\nTable 7 presents the data statistics of the AITW\\ndataset. Each subset is split episode-wise into a\\ntraining, validation, and test set (80\\/10\\/10%).\\nDataset\\nEpisodes\\nScreens\\nInstructions\\nGeneral\\n9,476\\n85,413\\n545\\nInstall\\n25,760\\n250,058\\n688\\nGoogleApps\\n625,542\\n4,903,601\\n306\\nSingle\\n26,303\\n85,668\\n15,366\\nWebShopping\\n28,061\\n365,253\\n13,473\"}\n{\"score\":0.4897,\"pdf_id\":\"dba877dd-b1c3-59a4-a31c-dbc2b55e0b98\",\"page_number\":29,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"be58ea45-0bee-5833-aacb-ce1622c1dc96\",\"text\":\"For the baseline methods, we adopt the public source code with the default parameter set-\\ntings and reproduce their results on the four benchmarks.\\nThe source codes of these meth-\\nods are available in Table 12.\\nBesides, for the used benchmarks, following [18], we only\\nkept datasets where all users and items have at least five interactions.\\nBesides, we adopted\\nthe dataset split settings used in [18]. The Sports, Beauty, and Toys datasets [71, 33] are ob-\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks indicate that the ETO method is indeed used on the WebShop dataset. However, the context does not specify the exact evaluation datasets used in the ETO paper. I will retrieve the metadata of the paper with the ID `722fbe98-4e3d-5d07-aea6-e4261418a8c8` to get more details.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM metadata WHERE paper_id = '722fbe98-4e3d-5d07-aea6-e4261418a8c8';\")"}
{"role": "user", "content": "[Observation]:\n{\"paper_id\":\"722fbe98-4e3d-5d07-aea6-e4261418a8c8\",\"title\":\"Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents\",\"abstract\":\"Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.\",\"num_pages\":17,\"conference_full\":\"Annual Meeting of the Association for Computational Linguistics\",\"conference_abbreviation\":\"ACL\",\"pub_year\":2024,\"volume\":\"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\"download_url\":\"https:\\/\\/aclanthology.org\\/2024.acl-long.409.pdf\",\"bibtex\":\"@inproceedings{song-etal-2024-trial,\\n    title = \\\"Trial and Error: Exploration-Based Trajectory Optimization of {LLM} Agents\\\",\\n    author = \\\"Song, Yifan  and\\n      Yin, Da  and\\n      Yue, Xiang  and\\n      Huang, Jie  and\\n      Li, Sujian  and\\n      Lin, Bill Yuchen\\\",\\n    editor = \\\"Ku, Lun-Wei  and\\n      Martins, Andre  and\\n      Srikumar, Vivek\\\",\\n    booktitle = \\\"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\\\",\\n    month = aug,\\n    year = \\\"2024\\\",\\n    address = \\\"Bangkok, Thailand\\\",\\n    publisher = \\\"Association for Computational Linguistics\\\",\\n    url = \\\"https:\\/\\/aclanthology.org\\/2024.acl-long.409\\\",\\n    doi = \\\"10.18653\\/v1\\/2024.acl-long.409\\\",\\n    pages = \\\"7584--7600\\\",\\n    abstract = \\\"Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.\\\",\\n}\\n\",\"authors\":\"['Yifan Song' 'Da Yin' 'Xiang Yue' 'Jie Huang' 'Sujian Li'\\n 'Bill Yuchen Lin']\",\"pdf_path\":\"data\\/dataset\\/airqa\\/papers\\/acl2024\\/722fbe98-4e3d-5d07-aea6-e4261418a8c8.pdf\",\"tldr\":\"ETO optimizes LLM agent performance by learning from exploration failures.\",\"tags\":\"['LLM agents' 'trajectory optimization' 'exploration-based learning'\\n 'contrastive learning' 'autonomous systems']\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The metadata of the ETO paper confirms that it is indeed the same paper that evaluates the ETO method on the WebShop dataset. Now, I need to check if the WebShop dataset used in the ETO paper is the same as the one used in the WKM paper. I will retrieve the relevant sections from both papers to compare the datasets.\n[Action]:\nRetrieveFromDatabase(sql=\"SELECT * FROM sections WHERE ref_paper_id IN ('0d7108da-de5c-5e4c-9865-7c4141672767', '722fbe98-4e3d-5d07-aea6-e4261418a8c8') AND section_content LIKE '%WebShop%' AND section_title IN ('Experiments', 'Datasets');\")"}
{"role": "user", "content": "[Observation]:\n{\"section_id\":\"c20a59ee-5774-5e06-a482-892ac52c5ea2\",\"section_title\":\"Experiments\",\"section_content\":\"Experiments\\n4.1\\nExperimental Settings\\nDatasets and Metrics.\\nWe evaluate our method on three real-world simulated planning datasets:\\nALFWorld [41], WebShop [53], and ScienceWorld [50]. AlFWorld and ScienceWorld include\\n5\\nTable 1: Main Results. The best results are marked in bold and the second-best results are marked with\\nunderline. All the prompt-based baselines ( u ) are evaluated under one-shot prompting and all the fine-tuning-\\nbased baselines ( v ) are trained through LoRA. Red represents the changes of WKM relative to the optimal\\nresults in the baselines. WKM and agent model are different LoRAs sharing the same backbone.\\nBackbone\\nMethod\\nALFWorld\\nWebShop\\nScienceWorld\\nSeen\\nUnseen\\nSeen\\nUnseen\\nGPT-3.5-Turbo\\nu REACT\\n8.57\\n5.97\\n44.37\\n15.41\\n13.99\\nGPT-4\\n44.29\\n38.05\\n62.76\\n67.32\\n65.09\\nMistral-7B\\nu REACT\\n7.86\\n5.22\\n14.63\\n20.72\\n17.65\\nu Reflexion\\n11.56\\n6.00\\n16.64\\n21.07\\n18.11\\nv NAT\\n64.43\\n68.96\\n61.01\\n57.12\\n50.79\\nv ETO\\n66.84\\n71.43\\n64.09\\n58.17\\n51.85\\nv KNOWAGENT\\n70.44\\n70.72\\n61.28\\n59.32\\n47.24\\nWKM\\n73.57 +3.13\\n76.87 +5.44\\n65.48 +1.39\\n62.12 +2.80\\n53.62 +1.77\\nGemma-7B\\nu REACT\\n6.43\\n2.24\\n5.93\\n3.58\\n3.51\\nu Reflexion\\n7.14\\n2.99\\n7.71\\n4.94\\n3.93\\nv NAT\\n67.86\\n65.88\\n55.82\\n47.63\\n44.98\\nv ETO\\n66.43\\n68.66\\n62.67\\n50.44\\n47.84\\nv KNOWAGENT\\n69.29\\n67.60\\n58.80\\n48.55\\n45.28\\nWKM\\n70.71 +1.42\\n70.40 +1.74\\n63.75 +1.08\\n53.68 +3.24\\n49.24 +1.40\\nLlama-3-8B\\nu REACT\\n2.86\\n3.73\\n19.32\\n24.76\\n22.66\\nu Reflexion\\n4.29\\n4.48\\n22.73\\n27.23\\n25.41\\nv NAT\\n60.71\\n59.70\\n61.60\\n55.24\\n48.76\\nv ETO\\n64.29\\n64.18\\n64.57\\n57.90\\n52.33\\nv KNOWAGENT\\n66.71\\n62.69\\n64.40\\n58.67\\n49.18\\nWKM\\n68.57 +1.86\\n65.93 +1.75\\n66.64 +2.07\\n60.12 +1.55\\n54.75 +2.42\\nunseen tasks to evaluate the agent\\u2019s generalization ability. The reward of ALFWorld is binary 0 or\\n1, indicating whether the agent has completed the task or not. WebShop and ScienceWorld provide\\ndense rewards from 0 to 1 to measure the completion level of the task. For all the datasets, we apply\\naverage reward as the final metrics. Please refer to Appendix B for detailed dataset information.\\nModels and Baselines.\\nWe evaluate on three state-of-the-art open-source models: 1) Mistral-7B\\n[16], the Mistral-7B-Instruct-v0.2 version. 2) Gemma-7B [24], the Gemma-1.1-7B-it version. 3)\\nLlama-3-8B [25], the Meta-Llama-3-8B-Instruct version. We compare our method with two prompt-\\nbased baselines: REACT [54] and Reflexion [40]. Besides, we adopt two strong baselines that\\nintroduce rejected trajectories into the training process to learn from experience: NAT [49], learn\\nfrom rejected trajectories through SFT, and ETO [44], learn from rejected trajectories through DPO\\n[36]. Moreover, we compare with a knowledge-augmented planning method KNOWAGENT. We\\nalso include ChatGPT (gpt-3.5-turbo-0125) [27] and GPT-4 (gpt-4-32K-0613) [28] for comparison.\\nAll the prompt-based baselines are tested under one-shot and all the fine-tuning-based baselines are\\ntrained with LoRA [12]. Please refer to Appendix C for baselines and re-producing details.\\nTraining and Inference Setups.\\nWe fine-tune the proposed approach with LoRA [12] using the\\nLlamaFactory [62] framework. During training, the model is tuned after finishing the entire trajectory\\nrather than each step of action. The learning rate is 1e-4 and the sequence length is 2048 for all\\nthe models. The training epoch is 3 and the batch size is 32. We adopt the AdamW optimizer [22]\\nwith a cosine learning scheduler. During inference, we apply the embedding layer of WKM as the\\nencoder and use the cosine similarity between sentences for retrieval. The number of retrieved\\naction-state-action triplets N is set to 3000 and the Pagent(Au) weight \\u03b3 is set to {0.4, 0.5, 0.7}. All\\nthe training and inference experiments are conducted on 8 NVIDIA V100 32G GPUs within 12 hours.\\nPlease refer to Appendix D for detailed hyperparameters used in our paper.\\n4.2\\nResults\\nMain Results.\\nAs shown in Table 1, for prompt-based baselines on open-source models, both RE-\\nACT and Reflexion exhibit poor performance, far behind our method and fine-tuning-based baselines\\non various datasets. GPT-3.5-Turbo performs ordinarily on two datasets other than WebShop, and it\\neven falls behind Mistral-7B and Llama-3-8B\\u2019s REACT performance on ScienceWorld. However,\\nGPT-4 exhibits strong performance across various datasets. Nevertheless, our approach, through\\n6\\nseen\\nunseen\\n50\\n60\\n70\\n80\\n63.57\\n69.40\\n67.86\\n70.67\\n69.29\\n75.37\\n73.57\\n76.87\\n67.19\\n71.57\\n65.46\\n70.71\\n65.14\\n67.40\\nALFWorld\\nAverage Reward\\ntest\\n50\\n60\\n70\\n80\\n56.98\\n62.44 63.68\\n65.48 63.97 63.70\\n61.03\\nWebShop\\nseen\\nunseen\\n40\\n50\\n60\\n70\\n52.78\\n50.32\\n55.04\\n51.52\\n60.81\\n53.42\\n62.12\\n53.62\\n55.49\\n48.38\\n58.51\\n51.78\\n56.36\\n45.27\\nScienceWorld\\nw\\/o all\\nw\\/ state\\nw\\/ task\\nw\\/ task&state\\nw\\/o rejected\\nmerge\\nprompt\\nFigure 3: Ablation Study on Mistral-7B. w\\/o all means the vanilla experienced agent model training with\\npure expert trajectories. w\\/ state is testing agent model with only state knowledge base constraints. w\\/ task\\nstands for guiding agent model with only task knowledge. w\\/ task&state is our WKM with both task knowledge\\nguidance and state knowledge constraints. w\\/o rejected means synthesizing task knowledge solely through\\nexpert trajectories. merge stands for training WKM and the agent model together with one single model. prompt\\nmeans using few-shot prompts to replace the WKM for providing knowledge.\\nTable 2: Average Steps. The maximum number of steps in\\nALFWorld and WebShop is 40 and 10. In ScienceWorld, the\\nnumber of steps ranges from 10 to 120 depending on the task\\ntype, with an average of around 40.\\nMethod\\nALFWorld\\nWebShop\\nScienceWorld\\nSeen\\nUnseen\\nSeen\\nUnseen\\nNAT\\n23.27\\n23.42\\n4.08\\n20.18\\n21.21\\nETO\\n19.82\\n22.29\\n3.99\\n24.13\\n26.35\\nKNOWAGENT\\n18.51\\n24.56\\n4.01\\n21.06\\n24.74\\nWKM\\n17.66\\n17.92\\n3.97\\n18.74\\n19.59\\nTable 3: Hallucinatory Action Rates on\\nALFWorld. We calculate the proportion of\\ntrajectories containing invalid actions regard-\\nless of their correctness.\\nMethod\\nALFWorld\\nSeen\\nUnseen\\nNAT\\n45.71%\\n50.00%\\nETO\\n34.29%\\n36.57%\\nKNOWAGENT\\n33.57%\\n44.78%\\nWKM\\n32.86%\\n29.85%\\nLoRA training alone, surpasses GPT-4 on ALFWorld (44.29\\u219273.57 on seen, 38.05\\u219276.87 on\\nunseen) and WebShop (62.76\\u219266.64). For fine-tuning-based baselines, both NAT and ETO fall\\nbehind our method, implying that just integrating world knowledge for agent models is worth more\\nthan further fussy SFT or DPO on negative examples. Our method also performs better than KNOWA-\\nGENT which brings human-designed fixed action knowledge and long action paths into trajectories.\\nThis suggests the effectiveness of our WKM which is responsible for generating instance-level task\\nknowledge and maintaining implicit action constraints. Furthermore, KNOWAGENT\\u2019s performance\\non unseen tasks is not as impressive as on seen tasks, while WKM can keep its advantage. This\\nphenomenon also demonstrates the generalization ability of WKM.\\nApproach Ablations.\\nAs shown in Figure 3, taking Mistral-7B as an example, we decompose\\nthe key components of WKM to examine the roles of the task and state knowledge separately. In a\\nmacro view, removing each module results in a clear drop in the agent\\u2019s performance, which validates\\nthe power of our world knowledge. Furthermore, the improvement through task knowledge (w\\/\\ntask) is more pronounced than that through state knowledge (w\\/ state), suggesting the necessity of\\nglobal prior knowledge for agent planning. A more micro observation reveals that the impact of state\\nknowledge is more significant on seen tasks compared to unseen tasks, while the influence of task\\nknowledge is sustainable across seen and unseen tasks. This may be attributed that although our\\nreal-time state knowledge is generated by WKM, the state knowledge base is built on the training\\nset, which may weaken generalization to some extent. Additionally, to validate our motivation of\\nallowing the agent to learn task knowledge from both expert and generated trajectories, we exclude the\\nrejected trajectories during the synthesis of task knowledge, instructing the agent model to synthesize\\nknowledge solely based on the chosen trajectories. The results (w\\/o rejected) demonstrate that\\nlearning from the contrast between chosen and rejected trajectories is more effective than learning\\nfrom chosen examples alone. This procedure is a little similar to DPO, but we achieve it through\\nknowledge augmentation rather than directly converting it into a loss calculation between chosen and\\nrejected trajectories. Additional results can further evident that training a WKM separately performs\\nbetter than training one single model together with the agent model as well as using few-shot prompts\\nto replace WKM for providing knowledge.\\n4.3\\nAnalysis\\nWorld knowledge can mitigate blind trial-and-error and reduce hallucinatory actions.\\nWe\\ncompare the number of planning steps for each dataset between three strong baselines and WKM\\nand calculate the average steps of each method. As depicted in Figure 9 (in Appendix F), WKM\\n7\\ndemonstrates the ability to complete a significant proportion of tasks using the shortest trajectory,\\nindicating that guidance from world knowledge can effectively reduce the agent\\u2019s blind trial-and-error\\nin the environment. Taking a further perspective from an average standpoint in Table 2, it can\\nbe observed that WKM exhibits lower average planning steps compared to other baselines. As\\nALFWorld can respond to invalid actions, in Table 3, we count the percentage of hallucinatory actions\\nthat occurred in trajectories from ALFWorld for each method. The results confirm the effectiveness\\nof our world knowledge model to decrease hallucinatory actions. Furthermore, it is worth noting that\\nmost baselines show a prominent increase in the average number of steps and percentage of invalid\\nactions when transitioning from seen tasks to unseen tasks, but WKM can still maintain a relatively\\nlow level. This reflects laterally that our world knowledge can still effectively guide the agent model\\non unseen tasks, highlighting the knowledge generalization brought by the world knowledge model.\\nTo see how our world knowledge works, please refer to our case study in Appendix H.\\nALFWorld seen\\nALFWorld unseen\\nSciWorld seen\\nSciWorld unseen\\n40\\n50\\n60\\n70\\n80\\nAverage Reward\\nHuman\\nWKM w\\/o state\\nFigure 4: Performance of human-\\ndesigned dataset-level knowledge\\ncompared to WKM generated\\ninstance-level knowledge.\\nOur instance-level knowledge can generalize better to unseen\\ntasks.\\nTo further explore the benefit of using a knowledge model to\\ngenerate instance-level task knowledge, we carefully survey the task\\nknowledge generated by our WKM and abstract it into dataset-level\\nknowledge for each dataset. Then we retrain the agent model to\\nadapt to new dataset-level knowledge5. As illustrated in Figure 4,\\nwe compare the performance of dataset-level knowledge with our\\ninstance-level task knowledge (WKM w\\/o state) on ALFWorld and\\nScienceWorld. It can be observed that our model-generated instance-\\nlevel knowledge not only surpasses human-designed knowledge\\non seen tasks but also exhibits even more remarkable performance\\non unseen tasks, with the improvement in performance on unseen\\ntasks significantly greater than that on seen tasks. This phenomenon\\nstraightly reflects the strong generalization ability of our knowledge\\nmodel compared to rigidly designed knowledge by humans.\\nTable 4: Weak-guide-strong. The knowl-\\nedge model here is based on Mistral-7B.\\nBackbone\\nMethod\\nALFWorld\\nSeen\\nUnseen\\nGPT-3.5-Turbo\\nREACT\\n8.57\\n5.97\\nWKM w\\/o state\\n12.86\\n8.96\\nGPT-4\\nREACT\\n44.29\\n38.05\\nWKM w\\/o state\\n50.71\\n47.01\\nWeak knowledge model guides strong agent model\\nplanning.\\nIn our main experiments, the knowledge\\nmodel and agent model are based on the same backbone.\\nHere, we explore on ALFWorld what will happen if we use\\na weak knowledge model to guide a strong agent model.\\nWe choose Mistral-7B as the backbone of the knowledge\\nmodel and ChatGPT and GPT-4 as the agent model. Since\\nwe cannot get the token distribution from OpenAI API, we\\nonly apply task knowledge to the agent model. As exhibited in Table 4, the results of both ChatGPT\\nand GPT-4 show distinct advances after being guided by the Mistral-7B world knowledge model,\\nindicating the weak world knowledge model also contains knowledge that the strong model may lack.\\nIn the era of LLMs, this inspires us with a new agent learning paradigm: weak-guide-strong. Due\\nto its lightweight nature, the weak knowledge model can flexibly adjust its parameters based on the\\nneeds of the agent model, which can address the difficulty of large agent models in adapting to new\\nenvironments through fine-tuning.\\nScienceWorld\\nseen\\nScienceWorld\\n unseen\\nALFWorld\\nseen\\nALFWorld \\nunseen\\nWebShop\\n0%\\n50%\\n100%\\nETO\\nKnowAgent\\nNAT\\nWKM-single-task\\nWKM-multi-task\\nFigure 5: Relative performance\\nof multi-task WKM compared to\\nvarious baselines.\\nUnified World Knowledge Model Training.\\nWe mix the world\\nknowledge collected from all three datasets and jointly train one\\nsingle world knowledge model to investigate the effect of multi-task\\nworld knowledge learning. Figure 5 illustrates the relative perfor-\\nmance comparison between multi-task WKM and various baselines,\\nfrom which we can observe that multi-task WKM not only does\\nnot lead to performance degradation but also exhibits visible im-\\nprovements compared to single-task WKM, especially on WebShop\\nand ScienceWorld. Similar to [57, 58, 3] which endeavor to train\\na unified agent model and achieve strong generalization ability to\\nheld-out tasks, this observation inspires us with the potential of train-\\ning a unified world knowledge model that can be applied to help\\n5Detailed manually designed dataset-level knowledge prompt can be found in Appendix I.3\\n8\\nvarious held-in agent models and also generalize to guide held-out agent models. A more daring\\nidea is whether a unified agent model combined with a unified world knowledge model is the key to\\nArtificial General Intelligence (AGI).\\nALFWorld seen\\nALFWorld unseen\\n40\\n50\\n60\\n70\\n80\\n73.57\\n76.87\\n69.29\\n75.37\\n65.43\\n66.69\\nAverage Reward\\nWKM\\nWKM w\\/o state\\nExplicit State\\nFigure 6: Performance of ex-\\nplicit state knowledge.\\nExplicit state knowledge will hurt the planning performance.\\nTo demonstrate the rationality of our choice to construct a state\\nknowledge base, we explore the effect of directly incorporating\\nstate knowledge into the context of the agent model (we retrain the\\nagent model to follow both the task and state knowledge), as shown\\nin Figure 6. The performance of explicit state knowledge is far\\ninferior to our approach of retrieving from a state knowledge base\\nand utilizing probabilistic constraints. It even performs worse than\\nwhen we remove state knowledge and only include task knowledge. This clearly indicates that blindly\\nextending prompts with a large amount of explicit natural language feedback is lose-more-than-gain\\nfor agent planning, and implicit knowledge constraints may be sometimes more prudent.\\nCase Study.\\nIn Figure 10 (Appendix H), we list the trajectories of ETO and our WKM within the\\nsame task in ALFWorld to illustrate how world knowledge functions. The rationales before each\\naction have been omitted to guarantee a clear illustration. The task is to clean some soapbar\\nand put it in cabinet. Initially, ETO blindly searches for the soapbar in the countertop\\nand cabinet, introducing a lot of irrelevant information and unnecessary context. In the later stages\\nof planning, ETO experiences the hallucination and executes the put action after close the cabinet,\\ncausing the environment to become unrecognizable and resulting in a collapse. On the contrary,\\nguided by task knowledge, WKM directly identified the possible locations of the soapbar and\\nsuccessfully found it in the first attempt. Subsequently, WKM efficiently completed the task with\\nprecision, adhering to the constraints of state knowledge.\\n5\",\"section_summary\":\"The study evaluates WKM, a world knowledge model, on three planning datasets, showing superior performance over baselines in both seen and unseen tasks, reducing hallucinatory actions and planning steps. WKM's instance-level knowledge generalizes better than human-designed knowledge.\",\"ordinal\":3,\"page_numbers\":\"[5 6 7 8 9]\",\"ref_paper_id\":\"0d7108da-de5c-5e4c-9865-7c4141672767\"}\n\nIn total, 1 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved sections from the WKM paper and the ETO paper both mention the WebShop dataset. However, the details about the dataset in the ETO paper are not explicitly provided. I will perform a similarity search to find the specific details about the WebShop dataset in the ETO paper.\n[Action]:\nClassicRetrieve(query='WebShop dataset details in ETO paper', limit=5)"}
{"role": "user", "content": "[Observation]:\n{\"score\":0.6593,\"pdf_id\":\"722fbe98-4e3d-5d07-aea6-e4261418a8c8\",\"page_number\":12,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"74ca39ed-5f4a-5d1d-8fd4-8e4de395c852\",\"text\":\"A\\nDatasets\\nWebShop\\nWebShop (Yao et al., 2022a) is an online shopping website environment where agents\\nnavigate the platform to make purchases based on user instructions. Once the agent selects the \\\"buy\\\"\\naction, the environment provides a final reward, which is calculated based on the matching heuristics of\\nthe product\\u2019s attributes and price.\\nScienceWorld\\nScienceWorld (Wang et al., 2022) is a text-based virtual environment centered around\"}\n{\"score\":0.6106,\"pdf_id\":\"648e3d50-375b-5189-b6b0-e0520626716e\",\"page_number\":13,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"88247a04-0efd-5ef6-8133-8daefa3a77fc\",\"text\":\"(v) WebShopping contains tasks related to online\\nshopping on E-commerce websites, e.g., searching\\nfor an item, adding an item to the cart, and viewing\\nthe shopping cart.\\nTable 7 presents the data statistics of the AITW\\ndataset. Each subset is split episode-wise into a\\ntraining, validation, and test set (80\\/10\\/10%).\\nDataset\\nEpisodes\\nScreens\\nInstructions\\nGeneral\\n9,476\\n85,413\\n545\\nInstall\\n25,760\\n250,058\\n688\\nGoogleApps\\n625,542\\n4,903,601\\n306\\nSingle\\n26,303\\n85,668\\n15,366\\nWebShopping\\n28,061\\n365,253\\n13,473\"}\n{\"score\":0.6099,\"pdf_id\":\"d13060ef-ed2a-5ed3-9cd9-016c6aa24006\",\"page_number\":30,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"fe44bc95-dcf9-55ba-a7f1-b8762b36a4fa\",\"text\":\"\\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n\\u2022 If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com\\/datasets\\n30\"}\n{\"score\":0.5977,\"pdf_id\":\"1c214ef8-daa0-596c-88cf-16066bfef488\",\"page_number\":28,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"791ed7b3-a5cf-554b-a7dc-1a67948f2a3a\",\"text\":\"Figure 9: The schema of the webkp dataset.\\n28\"}\n{\"score\":0.5812,\"pdf_id\":\"c17c9644-91c5-5727-b42a-b4a83f7af036\",\"page_number\":6,\"table_name\":\"chunks\",\"column_name\":\"text_content\",\"primary_key\":\"d89f588b-3ebb-5497-bec0-2a51bc429df6\",\"text\":\"Dataset Characteristics\\nThe dataset characteristics for each retailer are presented in Table 2.\\n6\"}\n\nIn total, 5 rows are displayed in JSON format."}
{"role": "assistant", "content": "[Thought]: The retrieved chunks from the ETO paper provide a description of the WebShop dataset, which matches the description in the WKM paper. This indicates that the WebShop dataset used in both papers is the same.\n\nNow, I have enough information to answer the question.\n[Action]:\nGenerateAnswer(answer=['ETO', 'true'])"}
{"role": "user", "content": "[Observation]: ['ETO', 'true']"}
